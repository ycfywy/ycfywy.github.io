<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>260225</title>
    <link href="/2026/02/25/260225/"/>
    <url>/2026/02/25/260225/</url>
    
    <content type="html"><![CDATA[<h2 id="leetcode">Leetcode</h2><p><a href="https://leetcode.cn/problems/sort-integers-by-the-number-of-1-bits/?envType=daily-question&amp;envId=2026-02-25">1356</a></p><p>给你一个整数数组 arr 。请你将数组中的元素按照其二进制表示中数字 1 的数目升序排序。</p><p>如果存在多个数字二进制中 1 的数目相同，则必须将它们按照数值大小升序排列。</p><p>请你返回排序后的数组。</p><p>示例 1：</p><p>输入：arr = [0,1,2,3,4,5,6,7,8] 输出：[0,1,2,4,8,3,5,6,7] 解释：[0] 是唯一一个有 0 个 1 的数。 [1,2,4,8] 都有 1 个 1 。 [3,5,6] 有 2 个 1 。 [7] 有 3 个 1 。 按照 1 的个数排序得到的结果数组为 [0,1,2,4,8,3,5,6,7]</p><h2 id="codeforces">Codeforces</h2><h2 id="ai-learning">AI Learning</h2><h2 id="english">English</h2><h2 id="实习刷帖">实习刷帖</h2><h2 id="ds杂谈">DS杂谈</h2>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>260224</title>
    <link href="/2026/02/24/260224/"/>
    <url>/2026/02/24/260224/</url>
    
    <content type="html"><![CDATA[<h2 id="leetcode">Leetcode</h2><p><a href="https://leetcode.cn/problems/sum-of-root-to-leaf-binary-numbers/description/?envType=daily-question&amp;envId=2026-02-24">1022</a></p><p>1022 从根到叶的二进制数之和</p><p>给出一棵二叉树，其上每个结点的值都是 0 或 1 。每一条从根到叶的路径都代表一个从最高有效位开始的二进制数。</p><p>例如，如果路径为 0 -&gt; 1 -&gt; 1 -&gt; 0 -&gt; 1，那么它表示二进制数 01101，也就是 13 。 对树上的每一片叶子，我们都要找出从根到该叶子的路径所表示的数字。</p><p>返回这些数字之和。题目数据保证答案是一个 32 位 整数。</p><p><img src="/2026/02/24/260224/2026-02-24-15-46-16.png" /> 输入：root = [1,0,1,0,1,0,1] 输出：22 解释：(100) + (101) + (110) + (111) = 4 + 5 + 6 + 7 = 22</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * struct TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode *left;</span><br><span class="hljs-comment"> *     TreeNode *right;</span><br><span class="hljs-comment"> *     TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125;</span><br><span class="hljs-comment"> * &#125;;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">sumRootToLeaf</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;<br>        <br>        <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br><br>        function&lt;<span class="hljs-type">int</span>(TreeNode*, <span class="hljs-type">int</span>)&gt; dfs = [&amp;](TreeNode* root, <span class="hljs-type">int</span> val)&#123;<br>            <br>            <span class="hljs-keyword">if</span>(root == <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>   <br>            val += root-&gt;val;<br><br>            <span class="hljs-keyword">if</span>(root-&gt;left == <span class="hljs-literal">nullptr</span> &amp;&amp; root-&gt;right == <span class="hljs-literal">nullptr</span>)&#123;<br>                res += val ;<br>                <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>            &#125;<br><br>            <span class="hljs-keyword">if</span>(root-&gt;left != <span class="hljs-literal">nullptr</span>)&#123;<br>                <span class="hljs-built_in">dfs</span>(root-&gt;left, val &lt;&lt; <span class="hljs-number">1</span>);<br>            &#125;<br>            <span class="hljs-keyword">if</span>(root-&gt;right != <span class="hljs-literal">nullptr</span>)&#123;<br>                <span class="hljs-built_in">dfs</span>(root-&gt;right, val &lt;&lt; <span class="hljs-number">1</span>);<br>            &#125;<br><br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>        &#125;;<br><br><br>        <span class="hljs-built_in">dfs</span>(root, <span class="hljs-number">0</span>);<br>        <span class="hljs-keyword">return</span> res;<br>    &#125;<br>&#125;;<br><br></code></pre></td></tr></table></figure><p>刚写的时候想了好久。。 一会想着递归 一会想着xxx 其实本质就是dfs同时要维护depth信息，到达leaf node就是一种方案加到答案里。不过这里的depth不需要记录，只需要用一次。</p><h2 id="codeforces">Codeforces</h2><h2 id="ai-learning">AI Learning</h2><h2 id="english">English</h2><h2 id="实习刷帖">实习刷帖</h2><h2 id="ds杂谈">DS杂谈</h2><p><a href="https://www.youtube.com/watch?v=2BmWf6Yn3WU">（26054）毛大帅爱“吃”小菇子！他说：“小菇子有特殊记忆，我还是吃小的”！罗瑞卿的“炫酷”！</a></p><p>权力场中的“嫡系”：罗瑞卿的性格侧写与政治命运</p><h3 id="资历底蕴与监军本色">资历底蕴与“监军”本色</h3><p>在开国大将的序列中，罗瑞卿的资历处于相对末位。相比于粟裕、陈赓等在战场上统率千军万马、建立尺寸军功的战将，罗瑞卿的晋升路径更多地依赖于对核心权力的忠诚与职能的特殊性。</p><p>在解放战争时期的“杨罗耿”兵团中，其角色定位极具象征意义：司令员杨得志负责实战指挥，参谋长耿彪负责制定方案，而作为政委的罗瑞卿，其实际职能更偏向于<strong>“监督”</strong>。在战火纷飞的前线，他曾手持驳壳枪督战，严惩擅自撤退者。这种“监军”色彩决定了他的威慑力并非来自战功积累，而是源于对纪律的极端贯彻和对领袖意图的绝对代理。</p><h3 id="井冈山上的政治显白">井冈山上的政治显白</h3><p>1965年6月，罗瑞卿紧随毛泽东的足迹重登井冈山。在井冈山革命博物馆，他曾公开感叹自己自1959年以来一直想上山，却因忙于“反右倾机会主义”斗争而无暇抽身。</p><p>这段独白在当时的语境下，是一场高层政治中的“炫酷”。表面上是感叹身兼数职、公务繁忙，深层意图则是向官场宣示自己在庐山会议期间的政治站位——正是他在1959年7月23日晚间的观察与汇报，为彭德怀等人定性为“军事俱乐部”接上了关键引线。这种作为“嫡系”的优越感，使其在政治表达上呈现出一种带有“撒娇”意味的豪横。</p><h3 id="小菇子情结与权力的温情">“小菇子”情结与权力的温情</h3><p>1965年6月中旬的杭州汪庄会议上，毛泽东曾对周恩来、罗瑞卿等人谈起一段关于食欲与历史的往事。他回忆起1927年井冈山时期的艰苦，战士们因群众纪律只能吃个头矮小的野蘑菇（小菇子）。</p><p>即便到了1961年物资相对丰富时，毛泽东依然拒绝厨师提供的大个蘑菇，坚持表示：“小菇子有特殊记忆，我还是吃小的。” 这段关于“小菇子”的论述，不仅是对艰苦岁月的怀念，也是一种对特定历史纽带的强调。在场的罗瑞卿作为当年的亲历者，自认深处这层“特殊记忆”的核心圈层。</p><h3 id="嫡系的宿命与落幕">嫡系的宿命与落幕</h3><p>罗瑞卿的政治轨迹揭示了权力结构中的一种悖论：最受信任的“嫡系”往往在风暴来临时最先被抛弃。由于长期处于领袖身边，且表现出极高的政治敏感度与优越感，罗瑞卿在权力平衡中逐渐显得过于“炫酷”与张扬。</p><p>尽管他在多次政治斗争中立下“投名状”，但在1965年12月举行的上海特别会议上，他成为了文革前夜第一个被正式打倒的高层人物。这种从巅峰到谷底的坠落，标志着这种依附于领袖个人信任的政治地位，在绝对权力面前呈现出极度的脆弱性。</p>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>260223</title>
    <link href="/2026/02/23/260223/"/>
    <url>/2026/02/23/260223/</url>
    
    <content type="html"><![CDATA[<h2 id="leetcode">Leetcode</h2><p><a href="https://leetcode.cn/problems/check-if-a-string-contains-all-binary-codes-of-size-k/?envType=daily-question&amp;envId=2026-02-23">1461</a></p><p>给你一个二进制字符串 s 和一个整数 k 。如果所有长度为 k 的二进制字符串都是 s 的子串，请返回 true ，否则请返回 false 。</p><p>示例 1：</p><p>输入：s = “00110110”, k = 2 输出：true 解释：长度为 2 的二进制串包括 “00”，“01”，“10” 和 “11”。它们分别是 s 中下标为 0，1，3，2 开始的长度为 2 的子串。</p><p><img src="/2026/02/23/260223/2026-02-23-19-25-26.png" /></p><p>直接暴力也不会超时 $5 * 10^{5} $， 所以这里我们直接遍历。由于k比较小 可以使用字符串转int存储在一个set中来判重和计数(要有 1&lt;&lt; k 个数字)。</p><p>这里如果把字符串转成10进制会爆int，所以转成二进制，具体写法就是</p><p>int value = std::stoi(binaryStr, nullptr, 2);</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">hasAllCodes</span><span class="hljs-params">(string s, <span class="hljs-type">int</span> k)</span> </span>&#123;<br>       unordered_set&lt;<span class="hljs-type">int</span>&gt; set;<br>        <span class="hljs-type">int</span> cnt = <span class="hljs-number">1</span> &lt;&lt; k, n = s.<span class="hljs-built_in">size</span>();<br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i + k - <span class="hljs-number">1</span> &lt; n;  i ++)&#123;<br>            string substr = s.<span class="hljs-built_in">substr</span>(i, k);<br>            <span class="hljs-type">int</span> t = <span class="hljs-built_in">stoi</span>(substr, <span class="hljs-literal">nullptr</span>, <span class="hljs-number">2</span>);<br>            set.<span class="hljs-built_in">insert</span>(t);<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> cnt - set.<span class="hljs-built_in">size</span>() == <span class="hljs-number">0</span>;<br><br>    &#125;<br>&#125;;<br><br></code></pre></td></tr></table></figure><h2 id="codeforces">Codeforces</h2><p>E. Idiot First Search https://codeforces.com/contest/2195/problem/E</p><p>有点复杂 先放一放</p><h2 id="ai-learning">AI Learning</h2><ol type="1"><li><p>介绍下bm25的计算方式</p></li><li><p>mcp function call的区别</p></li></ol><p>function call: LLM 推理的能力，能够根据user prompt中的api写出一个json request。然后你自己执行它。所以每有一个function 都要有一种交互格式的定义。 mcp: 把交互变成一套标准，一次编写可以通用。比如你提供service的api放在一个MCP Server上，定义好之后，大家都可以调用你，模型通过通用的标准进行function call就是mcp.</p><ol start="3" type="1"><li><p>介绍下grpo算法原理</p></li><li><p>transformer架构和特点</p></li></ol><p>编码器 (Encoder) 负责理解输入文本。它将输入的单词序列转换为高维的向量表示（Context Vector）。</p><p>多头自注意力 (Multi-Head Self-Attention)： 让模型在处理一个词时，能“看见”句子中其他所有词，并理解它们之间的关系。</p><p>前馈神经网络 (Feed-Forward Network)： 对注意力层提取的特征进行非线性变换。</p><p>解码器 (Decoder) 负责生成输出。它不仅看编码器传来的信息，还要看已经生成的上文。</p><p>掩码自注意力 (Masked Self-Attention)： 确保在生成当前词时，模型只能看到之前的词，不能“偷看”未来的答案。</p><p>自注意力机制 (Self-Attention) 这是 Transformer 的“灵魂”。它通过计算 Query (查询)、Key (键) 和 Value (值) 之间的匹配度，给句子中的每个词分配权重。</p><p>例子： 在“动物没过马路，因为它太累了”中，自注意力能让模型精准捕捉到“它”指的是“动物”而不是“马路”。</p><p>位置编码 (Positional Encoding) 由于 Transformer 是并行处理所有词的（不像 RNN 那样一个接一个），它本身并不知道词的先后顺序。为了解决这个问题，它在输入向量中注入了位置编码，通过正弦和余弦函数赋予模型“位置感”。</p><p>多头机制 (Multi-Head) 模型不只用一套注意力，而是并行运行多套（头）。</p><p>有的“头”可能关注语法结构。</p><p>有的“头”可能关注语义指代。</p><p>最后将这些信息汇总，使模型对语言的理解更立体</p><ol start="5" type="1"><li>经典强化学习算法有哪些</li></ol><h2 id="english">English</h2><p>transformers is the pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, …), inference engines (vLLM, SGLang, TGI, …), and adjacent modeling libraries (llama.cpp, mlx, …) which leverage the model definition from transformers.</p><p>compatible 兼容的 leverage plej 杠杆作用 Use a metal bar to increase the leverage | use (something) to maximum advantage 充分利用 adjacent əˈjās(ə)nt 邻近的 pledge 保证</p><p>Transformers库的功能： define model for other framework</p><p>主要的方法：</p><ul><li>pipeline</li><li>Trainer</li><li>generate</li></ul><p>To use Transformers in an offline or firewalled environment requires the downloaded and cached files ahead of time. Download a model repository from the Hub with the snapshot_download method.</p><p>from huggingface_hub import snapshot_download</p><p>snapshot_download(repo_id=“meta-llama/Llama-2-7b-hf”, repo_type=“model”)</p><p>However, if you need to download files to a specific folder, you can pass a <strong>local_dir</strong> parameter to the download function. This is useful to get a workflow closer to what the git command offers. The downloaded files will maintain their original file structure within the specified folder. For example, if filename=“data/train.csv” and local_dir=“path/to/folder”, the resulting filepath will be “path/to/folder/data/train.csv”.</p><h2 id="实习刷帖">实习刷帖</h2><p>见AI Learning</p><h2 id="ds杂谈">DS杂谈</h2><p><a href="https://www.youtube.com/watch?v=wjIAKSFR1Lo&amp;t=968s">（26052）“太祖爷”在整人前夕只给对手一次机会！叶帅总能在第一时间领取到“重磅私享信息”的原因是什么？叶帅发明的“冷板凳哲学”值得思考！</a></p><p>叶剑英元帅在中共高层政治斗争中展现出的独特智慧，即所谓的 <strong>“冷板凳哲学”</strong> 。</p><p>一、 引子：高层会议中的反常细节 通过周恩来卫士高振普的回忆录引出话题。高振普在1965年12月8日上海会议（处理罗瑞清的会议）中观察到一个奇怪的现象：</p><p>叶群和林立衡：当时叶群既不是政治局成员也不是书记处成员，而林立衡只是个高干子女，却在如此重大的会议现场“跑前跑后”，表现得非常张扬。</p><p>对比叶帅：温相指出，真正的政治高手如叶剑英，不仅早已掌握核心机密，而且在现场极其稳重，绝不随意外露。这种“沉得住气”是潜层次和高层次政治人物的分水岭 。</p><p>二、 案例分析：罗瑞清出事前的“信息差”</p><p>叶帅总能在第一时间获得“重磅私享信息”，并将其有选择地通报给值得信任的人： 提前通气（1965年11月4日）：在林彪正式告状和毛泽东表态（12月2日）之前，叶帅就从杭州密电罗瑞清的一位老部下，将其接到空军疗养院，开门见山地说：“家门不幸，罗瑞清出问题了” 。</p><p>精准定性：叶帅当时就总结了罗瑞清的四大问题：想当国防部长、对林彪封锁消息、折磨林彪、对林彪搞突然袭击 。</p><p>政治暗示：当罗的部下问该怎么办时，叶帅只给了一句金口：“该怎么办就怎么办” 。</p><p>刘志坚的回忆：1965年12月8日凌晨，叶帅亲自给刘志坚打电话，要求他不惊动家属立即赶往机场。在飞往上海的飞机上，叶帅始终一言不发，保持高度机密，直到降落后刘才知晓是要打倒罗瑞清。</p><p>三、 核心理论：“冷板凳哲学”</p><p>温相认为，叶帅多次在历史转折点（如打倒罗瑞清、林彪、四人帮、华国锋等）立于不败之地，归功于他发明的<strong>冷板凳哲学</strong>：</p><p>内涵：一个人要勇于、善于、甘于坐冷板凳。</p><p>意义：这不仅是韬光养晦，更是在边缘化时期保持耐心，暗中观察局势，等待反击机会。</p><p>四、 诗词中的政治信号（“诗言志”）</p><p>视频重点解读了叶帅在1963年至1965年间写的三首诗，认为这是叶帅向毛泽东发送的“政治暗号”： 《题二号楼》（1963年）：在被冷落时期，叶帅写下“留下一年秋”，表达了在冷板凳上的隐忍与耐心 。</p><p>送给演员的诗（1963年底）：引用“移宫换羽”的典故。温相分析，这不仅是谈音乐变调，更是谈政治变调。叶帅通过诗句向毛泽东喊话，暗示自己是能听懂政治风向的“知音” 。</p><p>大连棒棰岛作诗（1965年9月4日）：写下“一篇持久重新读，眼底吴钩看不休”。</p><p>解读：表面上是纪念抗战，实际上是向毛泽东表态支持“持久战、总体战”。</p><p>关键词“吴钩”：暗示要动刀子、整人了。毛泽东读后非常欣赏，认为叶帅看懂了他的心思。</p><p>五、 结论：权谋的默契</p><p>视频最后总结，叶帅之所以能多次提前获得消息，是因为他深刻理解毛泽东的整人习惯：</p><p>“太祖”的慈悲与残酷：毛泽东在收拾对手的最后关头，往往会给对手留出唯一一次机会（例如暗示或试探）。如果对手抓不住，下场就会很惨 。</p><p>总结语：叶帅的智慧在于他能在政治寒冬中稳坐冷板凳，通过细腻的诗词与最高领导人达成心理契约，并在第一时间捕捉到最危险也最宝贵的政治信息。</p>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>AI算法面经汇总</title>
    <link href="/2026/02/23/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F%E6%B1%87%E6%80%BB/"/>
    <url>/2026/02/23/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/wdndev/llm_interview_note">resource1</a></p><h1 id="llm-算法工程师面试考点通关清单-2025-2026版">🚀 LLM 算法工程师面试考点通关清单 (2025-2026版)</h1><table><thead><tr class="header"><th style="text-align: left;">主题分类</th><th style="text-align: left;">详细面试考点 / 核心问题</th><th style="text-align: left;">复习进度</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><strong>1. 模型架构 (Architecture)</strong></td><td style="text-align: left;"><strong>Attention 机制</strong>: MHA/MQA/GQA 的设计动机与显存收益；<span class="math inline">$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</span> 中 <span class="math inline">$\sqrt{d_k}$</span> 的缩放意义。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>位置编码</strong>: RoPE (旋转位置编码) 的数学推导；为什么 RoPE 具备长文本外推性；ALiBi 与相对位置编码。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>归一化与激活</strong>: RMSNorm 与 LayerNorm 的计算差异；Pre-Norm 与 Post-Norm 的收敛稳定性对比；SwiGLU 激活函数的公式与优势。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>长文本处理</strong>: FlashAttention 1/2/3 硬件加速原理（Tiling &amp; Recomputation）；Ring Attention 并行逻辑；KV Cache 占用计算。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>新型架构</strong>: MoE (混合专家模型) 的 Router 负载均衡策略与 Top-K 选通；SSM (Mamba) 对比 Transformer 的线性复杂度优势。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"><strong>2. 数学与算法基础</strong></td><td style="text-align: left;"><strong>优化器</strong>: AdamW 的权重衰减逻辑；Warmup 策略的必要性；一阶（SGD/Adam）与二阶优化器的优劣对比。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>损失函数与指标</strong>: Cross-Entropy 计算细节；KL 散度在 PPO 和 DPO 中的数学约束作用；困惑度 (Perplexity) 与 BPC 的换算。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>理论基石</strong>: Scaling Laws (Chinchilla 计算法则)；如何预估模型能力、训练数据量与算力 (FLOPs) 的关系。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"><strong>3. 预训练与微调 (Pre-train &amp; SFT)</strong></td><td style="text-align: left;"><strong>数据工程</strong>: 网页数据清洗流水线；数据去重 (MinHash/LSH) 算法；Tokenization (BPE/WordPiece) 流程与 OOV 处理。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>高效微调 (PEFT)</strong>: LoRA (<span class="math inline"><em>B</em><em>A</em><em>x</em></span>) 秩 <span class="math inline"><em>r</em></span> 与 <span class="math inline"><em>α</em></span> 的选择；QLoRA 原理（4-bit NormalFloat）；Prefix Tuning 与 Prompt Tuning 区别。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>分布式训练</strong>: DP/DDP 原理；TP (张量并行) 与 PP (流水线并行) 的通信开销；ZeRO-1/2/3 (显存分片) 优化细节。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>训练稳定性</strong>: 梯度裁剪 (Clipping)；混合精度训练 (FP16/BF16/FP8) 的溢出问题与 Loss Scale 机制。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"><strong>4. 强化学习与对齐 (RLHF/DPO)</strong></td><td style="text-align: left;"><strong>RLHF 三阶段</strong>: SFT -&gt; Reward Model (RM) -&gt; PPO；PPO 算法中的四个模型 (Actor/Critic/Ref/Reward) 协作流程。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>直接偏好优化 (DPO)</strong>: DPO 损失函数推导；为什么 DPO 放弃了奖励模型；DPO 的局限性（如对负样本的过度惩罚）。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>在线对齐</strong>: 拒绝采样 (Rejection Sampling)；KTO (Kahneman-Tversky Optimization) 及其对非配对数据的支持。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"><strong>5. AI Infra 与推理优化</strong></td><td style="text-align: left;"><strong>KV Cache 优化</strong>: PageAttention (vLLM) 如何解决显存碎片化；Copy-on-Write 机制在多轮对话中的应用。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>量化技术</strong>: GPTQ vs AWQ 权重采样原理；FP8 训练与推理趋势；SmoothQuant 平滑激活值抑制离群点。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>推理加速</strong>: 连续批处理 (Continuous Batching)；投机采样 (Speculative Decoding) 提速比计算；Chunked Prefill 优化。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>显存分析</strong>: 估算 <span class="math inline"><em>N</em></span> 参数模型在不同精度下的显存占用（权重+梯度+优化器状态）；推理时的上下文显存估算。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"><strong>6. 应用工程 (RAG &amp; Agent)</strong></td><td style="text-align: left;"><strong>RAG 链路</strong>: 向量检索 (HNSW/IVF)；Rerank 二阶段重排模型；处理长上下文下的 “Lost in the Middle” 现象。</td><td style="text-align: left;">[ ]</td></tr><tr class="odd"><td style="text-align: left;"></td><td style="text-align: left;"><strong>Agent 架构</strong>: ReAct (Reason + Act) 循环；Tool Use (Function Calling) 的实现链路；多 Agent 框架 (AutoGPT/MetaGPT)。</td><td style="text-align: left;">[ ]</td></tr><tr class="even"><td style="text-align: left;"></td><td style="text-align: left;"><strong>评估体系</strong>: LLM-as-a-Judge 原理与偏见规避；代码/数学/逻辑能力的常用 Benchmark (HumanEval, GSM8K)。</td><td style="text-align: left;">[ ]</td></tr></tbody></table>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>test</title>
    <link href="/2026/02/22/test/"/>
    <url>/2026/02/22/test/</url>
    
    <content type="html"><![CDATA[<h2 id="阅读总结"><a href="#阅读总结" class="headerlink" title="阅读总结"></a>阅读总结</h2><p>&emsp;&emsp;</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>&emsp;&emsp;</p><p><img src="/2026/02/22/test/2026-02-22-22-23-26.png"><br><img src="/2026/02/22/test/2026-02-22-23-40-25.png"></p><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><p>&emsp;&emsp;</p><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><p>&emsp;&emsp;</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Claude Code 入门指南</title>
    <link href="/2026/02/04/claude-code-tutorial/"/>
    <url>/2026/02/04/claude-code-tutorial/</url>
    
    <content type="html"><![CDATA[<h2 id="Claude-Code-简介"><a href="#Claude-Code-简介" class="headerlink" title="Claude Code 简介"></a>Claude Code 简介</h2><p>&emsp;&emsp;Claude Code 是 Anthropic 官方推出的 AI 编程助手 CLI 工具，它可以帮助开发者完成代码编写、调试、重构、测试等各类软件开发任务。与传统的代码编辑器插件不同，Claude Code 是一个独立的命令行工具，可以在任何目录下运行，通过自然语言与开发者交互。</p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h3><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td><code>claude</code></td><td>启动 Claude Code 交互会话</td></tr><tr><td><code>claude &quot;任务描述&quot;</code></td><td>直接执行单次任务</td></tr><tr><td><code>claude -p &quot;任务描述&quot;</code></td><td>使用提示模式执行任务</td></tr><tr><td><code>claude --help</code></td><td>查看帮助信息</td></tr><tr><td><code>exit</code> &#x2F; <code>Ctrl+C</code></td><td>退出 Claude Code</td></tr></tbody></table><h3 id="任务执行相关"><a href="#任务执行相关" class="headerlink" title="任务执行相关"></a>任务执行相关</h3><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td><code>Task /task_name</code></td><td>执行已命名的任务</td></tr><tr><td><code>/plan</code></td><td>创建并保存执行计划</td></tr><tr><td><code>/approve</code></td><td>批准执行计划</td></tr><tr><td><code>/compact</code></td><td>压缩对话历史，减少上下文消耗</td></tr><tr><td><code>/complete</code></td><td>标记任务完成</td></tr></tbody></table><h3 id="Git-集成命令"><a href="#Git-集成命令" class="headerlink" title="Git 集成命令"></a>Git 集成命令</h3><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td><code>Git: Commit</code></td><td>创建 git 提交</td></tr><tr><td><code>Git: Push</code></td><td>推送到远程仓库</td></tr><tr><td><code>Git: PR</code></td><td>创建 Pull Request</td></tr></tbody></table><h2 id="文件夹与项目操作"><a href="#文件夹与项目操作" class="headerlink" title="文件夹与项目操作"></a>文件夹与项目操作</h2><h3 id="工作目录相关"><a href="#工作目录相关" class="headerlink" title="工作目录相关"></a>工作目录相关</h3><p>&emsp;&emsp;Claude Code 可以分析任意项目结构，理解你的代码库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 进入项目目录后直接使用</span><br><span class="hljs-built_in">cd</span> /path/to/your/project<br>claude<br></code></pre></td></tr></table></figure><h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><p>&emsp;&emsp;在 Claude Code 对话中，你可以直接要求它：</p><table><thead><tr><th>操作</th><th>示例指令</th></tr></thead><tbody><tr><td>查看文件</td><td>“Read the file src&#x2F;utils.ts”</td></tr><tr><td>创建文件</td><td>“Write a new component at src&#x2F;components&#x2F;Button.tsx”</td></tr><tr><td>编辑文件</td><td>“Edit the config file to add new setting”</td></tr><tr><td>搜索文件</td><td>“Find all files matching *.test.ts”</td></tr><tr><td>搜索内容</td><td>“Search for all occurrences of ‘getUser’”</td></tr></tbody></table><h3 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h3><p>&emsp;&emsp;Claude Code 支持使用 Glob 模式查找文件：</p><ul><li><code>**/*.js</code> - 查找所有 JS 文件</li><li><code>src/**/*.ts</code> - 查找 src 目录下所有 TS 文件</li><li><code>**/test*.py</code> - 查找所有测试 Python 文件</li></ul><h2 id="Skills（技能）使用"><a href="#Skills（技能）使用" class="headerlink" title="Skills（技能）使用"></a>Skills（技能）使用</h2><h3 id="什么是-Skills"><a href="#什么是-Skills" class="headerlink" title="什么是 Skills"></a>什么是 Skills</h3><p>&emsp;&emsp;Skills 是 Claude Code 的扩展功能模块，每个 Skill 都是一个专门化的工具或功能集，可以帮助完成特定类型的任务。</p><h3 id="常用内置-Skills"><a href="#常用内置-Skills" class="headerlink" title="常用内置 Skills"></a>常用内置 Skills</h3><table><thead><tr><th>Skill 名称</th><th>功能描述</th></tr></thead><tbody><tr><td><code>bash</code></td><td>执行 Shell 命令</td></tr><tr><td><code>glob</code></td><td>文件模式匹配搜索</td></tr><tr><td><code>grep</code></td><td>在文件中搜索文本</td></tr><tr><td><code>read</code></td><td>读取文件内容</td></tr><tr><td><code>write</code></td><td>创建或覆盖文件</td></tr><tr><td><code>edit</code></td><td>修改文件内容</td></tr><tr><td><code>Task</code></td><td>创建和管理任务</td></tr><tr><td><code>WebFetch</code></td><td>获取网页内容</td></tr><tr><td><code>WebSearch</code></td><td>网络搜索</td></tr></tbody></table><h3 id="自定义-Skills"><a href="#自定义-Skills" class="headerlink" title="自定义 Skills"></a>自定义 Skills</h3><p>&emsp;&emsp;你可以创建自己的 Skills 来扩展 Claude Code 的功能。Skills 通常以以下方式使用：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">skill:</span>skill_name <span class="hljs-string">&quot;参数&quot;</span><br></code></pre></td></tr></table></figure><h3 id="Task-工具的高级用法"><a href="#Task-工具的高级用法" class="headerlink" title="Task 工具的高级用法"></a>Task 工具的高级用法</h3><p>&emsp;&emsp;<code>Task</code> 工具是 Claude Code 最强大的功能之一：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 执行特定类型的任务</span><br>Task test-runner <span class="hljs-string">&quot;运行所有测试&quot;</span><br>Task code-review <span class="hljs-string">&quot;检查代码质量问题&quot;</span><br>Task docs-generator <span class="hljs-string">&quot;生成 API 文档&quot;</span><br></code></pre></td></tr></table></figure><h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><h3 id="1-明确任务目标"><a href="#1-明确任务目标" class="headerlink" title="1. 明确任务目标"></a>1. 明确任务目标</h3><p>&emsp;&emsp;在描述任务时，越具体越好：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ✅ 好的示例</span><br>claude <span class="hljs-string">&quot;Refactor the user authentication module to use JWT tokens&quot;</span><br><br><span class="hljs-comment"># ❌ 模糊的示例</span><br>claude <span class="hljs-string">&quot;fix the login&quot;</span><br></code></pre></td></tr></table></figure><h3 id="2-使用-plan-模式处理复杂任务"><a href="#2-使用-plan-模式处理复杂任务" class="headerlink" title="2. 使用 &#x2F;plan 模式处理复杂任务"></a>2. 使用 &#x2F;plan 模式处理复杂任务</h3><p>&emsp;&emsp;对于涉及多个步骤的复杂任务，先使用 <code>/plan</code> 让 Claude 制定执行计划，确认无误后再批准执行。</p><h3 id="3-合理使用-compact"><a href="#3-合理使用-compact" class="headerlink" title="3. 合理使用 &#x2F;compact"></a>3. 合理使用 &#x2F;compact</h3><p>&emsp;&emsp;当对话历史较长时，使用 <code>/compact</code> 可以压缩上下文，节省 token 消耗同时保留重要信息。</p><h3 id="4-利用版本控制"><a href="#4-利用版本控制" class="headerlink" title="4. 利用版本控制"></a>4. 利用版本控制</h3><p>&emsp;&emsp;在进行大规模代码修改前，先提交当前更改，或让 Claude 创建新的分支进行实验。</p><h3 id="5-结合-IDE-使用"><a href="#5-结合-IDE-使用" class="headerlink" title="5. 结合 IDE 使用"></a>5. 结合 IDE 使用</h3><p>&emsp;&emsp;Claude Code 可以与 VS Code、Neovim 等编辑器配合使用，实现无缝的开发体验。</p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p><strong>Q: Claude Code 会修改我的代码吗？</strong></p><p>&emsp;&emsp;默认情况下，Claude Code 在执行修改前会请求你的确认。只有获得批准后才会实际修改文件。</p><p><strong>Q: 如何控制 Claude Code 的行为？</strong></p><p>&emsp;&emsp;可以通过系统提示词、对话中的具体指令来控制其行为，也可以在配置文件中设置偏好。</p><p><strong>Q: 支持哪些编程语言？</strong></p><p>&emsp;&emsp;Claude Code 本身是语言无关的，可以处理任何编程语言的代码任务。</p><p><strong>Q: 如何保证代码安全？</strong></p><p>&emsp;&emsp;Claude Code 在执行危险操作前会发出警告，某些敏感操作需要明确授权。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;Claude Code 作为一款 AI 编程助手，通过自然语言交互大大提升了开发效率。掌握其常用命令和工作模式，可以让你在日常开发中事半功倍。建议从简单的任务开始尝试，逐步探索其更多高级功能。</p>]]></content>
    
    
    <categories>
      
      <category>工具教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Claude Code</tag>
      
      <tag>AI编程助手</tag>
      
      <tag>CLI工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jointly Reinforcing Diversity and Quality in Language Model Generations</title>
    <link href="/2026/01/03/Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations/"/>
    <url>/2026/01/03/Jointly-Reinforcing-Diversity-and-Quality-in-Language-Model-Generations/</url>
    
    <content type="html"><![CDATA[<h2 id="阅读总结"><a href="#阅读总结" class="headerlink" title="阅读总结"></a>阅读总结</h2><p>&emsp;&emsp;该文章承接之前关于LLM Diversity的研究。我们之前已经阅读的文章主要是通过采样、数据合成的方式提高LLM Diversity。今天阅读的文章是通过在RL阶段引入Diversity reward，从Quality + Diversity的角度约束RL阶段的参数更新，从而提高LLM Diversity。</p><p>&emsp;&emsp;本文的写作逻辑非常清晰。算法的formula讨论，模型训练的数据集、框架，测试使用的benchmarks，消融实验的设计都非常棒，很有学习意义。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;先来谈一谈Reinforce Learning。现有的大部分LLM做强化学习首选的方案就是GRPO。通俗地将，GRPO的原理就是：给定一个prompt，让LLM输出n个回答。Reward model对这些回答进行打分。将n个回答视为一组，我们希望鼓励LLM输出组内Reward在平均值以上的回答，拒绝组内Reward在平均值以下的回答。具体的数学原理参见<a href="https://zhuanlan.zhihu.com/p/23066650797">link</a>，强化学习台湾大学李宏毅老师视频<a href="https://www.bilibili.com/video/BV15hw9euExZ/?spm_id_from=333.337.search-card.all.click&vd_source=f849fb0704d7dc059fbab19eff482313">link</a>。</p><p>&emsp;&emsp;现有的Reward model在打分时，只针对单个回答的质量，而没有考虑组内的样本的多样性。当我们组内的多个回答质量相当时，我们应该给组内比较Diversity的回答额外的Reward，鼓励更加多样的回答。为了能够提高LLM Diversity，本文提出在RL阶段引入Diversity Reward的方法，鼓励模型输出更加多样性的内容。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>&emsp;&emsp;如何设计Diversity Reward以及将该Reward融入到现有的RL算法中，是本文需要解决的问题。</p><h3 id="Diversity-Reward的设计"><a href="#Diversity-Reward的设计" class="headerlink" title="Diversity Reward的设计"></a>Diversity Reward的设计</h3><p>&emsp;&emsp;考虑给定一个prompt后LLM的n个回答。我们可以从语义的角度对n个回答做聚类。位于相同聚类的回答语义近似，不同聚类的回答语义不同。如图所示，LLM的回答可以划分出3个聚类，其中绿色回答的Quality Reward与蓝色回答一样，但因为是更加Diversity的回答，Reward被放大。<br><img src="./2026-01-03-16-31-57.png"></p><p>&emsp;&emsp;为了从语义的角度衡量两个回答是否相似，本文训练了一个classifier。(<strong>这里比较奇怪：作者并没有详细讨论classifier的设计与实现 而是放在Appendix里面 可能是技术含量比较低吧。这里为了更全面的了解文章，我们做一下讲解</strong>)</p><ul><li>Base model: <a href="https://huggingface.co/answerdotai/ModernBERT-base">ModernBERT-base</a>(一个预训练transformer模型 参数量1GB) </li><li>Traning dataset: 1000 NoveltyBench annotations (2000 responses) 拼接annotations和response，模型需要预测response1 和 response2是否相似。<br><img src="./2026-01-03-16-39-42.png"></li><li>testing: 预留100个data，和现有的classifier以及LLM Judge(GPT-4o, o1-mini, o1-mini-cot)做性能对比，优于LLM Judge。(选用这些模型也是研究惯例吧， 一开始的LLM judge大家用的都是OpenAI的model)<br><img src="./2026-01-03-16-47-37.png"></li></ul><p>&emsp;&emsp;一旦我们有了一个classifier，就可以逐个pair判断回答是否相似。使用如下的公式计算单个Diversity Reward。举个例子：对于4个answer，以蓝色answer为例，有1个回答和他类似，2个回答和他不一样，计算出的Diversity Reward就是 2 &#x2F; (4 - 1) &#x3D; 2&#x2F;3。<br><img src="./2026-01-03-17-12-09.png"><br><img src="./2026-01-03-17-12-17.png"><br><img src="./2026-01-03-17-12-26.png"></p><h3 id="将Diversity-Reward融入RL"><a href="#将Diversity-Reward融入RL" class="headerlink" title="将Diversity Reward融入RL"></a>将Diversity Reward融入RL</h3><p>&emsp;&emsp;前面已经讲过GRPO算法的基本原理，这里给出公式。为了将Diversity Reward引入RL过程，需要对$r(x, y)$进行改写。<br><img src="./2026-01-03-17-15-06.png"></p><p>&emsp;&emsp;如下，作者对GRPO做了3处修改：(1)将Diversity Reward作为一个0-1的放缩因子乘到原理的Reward function。(2)将图中的$\Sigma$部分修改为token level：计算损失时，直接除以整个 Batch 中所有 token 的总数。而不是除以句子的总数。之所以这样处理，是因为前人的研究发现，除以句子的总数放大了短句的reward。(3)去除了Advantage function的normalization。作者认为标准化的操作会放大噪声，后续也会有消融实验。<br><img src="./2026-01-03-17-23-56.png"><br><img src="./2026-01-03-17-24-03.png"></p><p>&emsp;&emsp;至此，讲述完了整篇文章的Method部分，方法比较常规，但是是LLM Diversity领域的重要尝试。下面是实验部分，证明该方法的表现。</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>&emsp;&emsp;本文将Non-verifiable Tasks和Verifiable Tasks分开进行实验。具体来说，就是将开放式问题和数学问题分开进行实验，因为一个是有具体答案的，另一个是没有的。</p><h3 id="Non-verifiable-Tasks"><a href="#Non-verifiable-Tasks" class="headerlink" title="Non-verifiable Tasks"></a>Non-verifiable Tasks</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><ul><li>baseline: 未修改的GRPO | DivPO(强行把“最具有多样性”的作为 Chosen，把“最缺乏多样性”的作为 Rejected。) | GRPO-Unlikeliness(给那些概率较低（低似然）的回答更高的奖励，强迫模型去探索“冷门”路径。) | Our method</li><li>model: Llama-3.1-8B-Instruct | Llama-3.3-70B-Instruct</li><li>data: subset of 10k prompts in WildChat<br><img src="./2026-01-03-17-41-21.png"></li><li>benchmarks: <strong>AlpacaEval 2.0</strong>(评估模型通用指令遵循，对输出长度进行控制的能力，使用LCWR length control win rate指标)&emsp; <strong>ArenaHard v2.0</strong>(复杂逻辑与创意写作，模型需要根据指令进行特定格式、长度的输出，使用WR win rate指标)&emsp; <strong>EQ-Bench</strong>(创意写作&#x2F;情感智能任务，使用LLM judge的评分指标) &emsp; &emsp;(开放式问题，使用Distinct-n计算n-gram的多样性来 评估模型输出的多样性)</li></ul><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>&emsp;&emsp;Darling 在所有基准测试中均实现了质量与多样性的双重最佳。 具体而言，在所有基准模型中，Darling 取得了最高的质量评分（AlpacaEval 和 ArenaHard ），同时在语义层面（Distinct）和词汇层面（Distinct-4）也都达到了最高的多样性。此外，尽管本文并未显式地在创意写作提示词上进行训练，但与所有基准模型相比，Darling 在 EQ-Bench（创意写作）中获得了最高的 ELO 分数，证明了提升多样性对创意类任务的显著效果。<br><img src="./2026-01-03-17-53-56.png"></p><p>&emsp;&emsp;进一步的，本文绘制Quality-Diversity曲线，发现本文提出的方法扩展了Quality-Diversity的边界(将图像的右上角做了延申)。<br><img src="./2026-01-03-17-57-16.png"></p><h3 id="Verifiable-Tasks"><a href="#Verifiable-Tasks" class="headerlink" title="Verifiable Tasks"></a>Verifiable Tasks</h3><h4 id="实验设置-1"><a href="#实验设置-1" class="headerlink" title="实验设置"></a>实验设置</h4><ul><li>baseline: 与前面设置一样</li><li>model: Qwen3-4B-Base | Qwen3-14B-Base</li><li>data: subset of 10k data in DeepscaleR dataset<br><img src="./2026-01-03-18-01-07.png"></li><li>benchmarks: <strong>AIME25</strong> (极其强调深度推理，通常需要模型进行多步复杂的逻辑推导。) <strong>HMMT &amp; Brumo 2025</strong> (2025 年最新的竞赛题目，题型新颖，能够有效解决数据污染的问题。)</li></ul><h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h4><p>&emsp;&emsp;根据实验结果，在不同的pass@k下，本文提出的训练方法有效地提高了模型在Verifiable Tasks上的表现。<br><img src="./2026-01-03-18-03-56.png"><br><img src="./2026-01-03-18-03-41.png"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>&emsp;&emsp;本文主要做了3个消融实验：</p><ol><li>对比融合Diversity Reward的加法方式与乘法方式(本文的方法)</li><li>对比语义多样性与Classifier(本文的方法)</li><li>GRPO Normalization的消融</li></ol><h3 id="Diversity-Reward应该加法融合还是乘法融合？"><a href="#Diversity-Reward应该加法融合还是乘法融合？" class="headerlink" title="Diversity Reward应该加法融合还是乘法融合？"></a>Diversity Reward应该加法融合还是乘法融合？</h3><p>&emsp;&emsp;实验结果：乘法聚合（Darling 采用的方案）在 AlpacaEval 2.0 上的表现优于加法聚合，而在 ArenaHard v2.0 和 NoveltyBench 上的表现则与之持平。本文之所以选择乘法聚合，是因为它具有简洁性：它不需要额外处理不同奖励尺度不匹配的问题，也不需要对各项独立奖励的混合权重进行繁琐的超参数调优。<br><img src="./2026-01-03-18-13-46.png"></p><h3 id="Classifier有没有用？"><a href="#Classifier有没有用？" class="headerlink" title="Classifier有没有用？"></a>Classifier有没有用？</h3><p>&emsp;&emsp;将训练的classifier替换成词汇多样性指标，使用4-gram来评估某个回答的多样性。实验结果：虽然将 4-gram 多样性与质量相结合在AlpacaEval 2.0, ArenaHard v2.0）能够达到与Darling 相当的水平，但在NoveltyBench中，其表现显著逊于 Darling。此外，在数学题目中，使用词汇多样性作为奖励的效果在 pass@1 性能上甚至不如原生的 GRPO 基准。<br><img src="./2026-01-03-18-16-25.png"><br><img src="./2026-01-03-18-15-36.png"></p><h3 id="去掉Normalization有没有影响"><a href="#去掉Normalization有没有影响" class="headerlink" title="去掉Normalization有没有影响"></a>去掉Normalization有没有影响</h3><p>&emsp;&emsp;从结果上看，RL中有没有Normalization影响不大。反而是性能有微小提升。<br><img src="./2026-01-03-18-19-34.png"><br><img src="./2026-01-03-18-19-56.png"> </p><iframe src="//player.bilibili.com/player.html?isOutside=true&aid=115960878996056&bvid=BV171zaBJEp3&cid=35624452519&p=1"     scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"     width="100%" height="500"> </iframe>]]></content>
    
    
    <categories>
      
      <category>ICLR2026</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>视频/图片生成AI的一些抽象事情</title>
    <link href="/2026/01/02/%E8%A7%86%E9%A2%91-%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90AI%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%BD%E8%B1%A1%E4%BA%8B%E6%83%85/"/>
    <url>/2026/01/02/%E8%A7%86%E9%A2%91-%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90AI%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%BD%E8%B1%A1%E4%BA%8B%E6%83%85/</url>
    
    <content type="html"><![CDATA[<h2 id="视频生成好贵"><a href="#视频生成好贵" class="headerlink" title="视频生成好贵"></a>视频生成好贵</h2><p>Veo3.1 ? 我没看错吧<br><img src="/2026/01/02/%E8%A7%86%E9%A2%91-%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90AI%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%BD%E8%B1%A1%E4%BA%8B%E6%83%85/2026-01-02-00-09-04.png"></p><h2 id="视频生成的一些槽点"><a href="#视频生成的一些槽点" class="headerlink" title="视频生成的一些槽点"></a>视频生成的一些槽点</h2><p>为什么人物会被拉伸</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>对LLM输出多样性的更全面的调研</title>
    <link href="/2025/12/29/%E5%AF%B9LLM%E8%BE%93%E5%87%BA%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%9A%84%E6%9B%B4%E5%85%A8%E9%9D%A2%E7%9A%84%E8%B0%83%E7%A0%94/"/>
    <url>/2025/12/29/%E5%AF%B9LLM%E8%BE%93%E5%87%BA%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%9A%84%E6%9B%B4%E5%85%A8%E9%9D%A2%E7%9A%84%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<h2 id="阅读总结"><a href="#阅读总结" class="headerlink" title="阅读总结"></a>阅读总结</h2><p>&emsp;&emsp;这是一篇来自NeurIPS2025的Best paper。该文章系统地讨论了LLM输出多样性的问题，提出Artificial Hivemind现象：在开放式问题中，同类模型的回答具有相似性；不同模型之间也有相似性。这种LLM同质化的现象值得我们警惕和研究。</p><p>&emsp;&emsp;更重要的是，作者在oral talk中对LLM diversity这个领域做了很全面的综述，包括 具有多样性的数据、多样性的解码策略、在训练过程中引入多样性、更加严谨的评估模型输出多样性的指标。<br><img src="./xxx.png"></p><p>值得注意的是，其中关于采样策略的几篇文章我们都有读过。虽然是自己寻找的论文，但能够不偏离主流研究，说明我们读的文章还是有价值的。</p><table><thead><tr><th>分类</th><th>论文名称</th><th>核心研究方向</th><th>arXiv 链接</th></tr></thead><tbody><tr><td><strong>具有多样性的数据</strong></td><td><strong>SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms</strong></td><td>利用质量-多样性算法生成高难度、非重复的合成推理数据。</td><td><a href="https://arxiv.org/abs/2506.06499">2506.06499</a></td></tr><tr><td></td><td><strong>Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning</strong></td><td>提出 G-Vendi 指标，利用模型梯度熵来量化并提升训练数据的多样性。</td><td><a href="https://arxiv.org/abs/2505.20161">2505.20161</a></td></tr><tr><td></td><td><strong>Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity</strong></td><td>解决多样性筛选中易偏向“短文本”的偏见，通过长度控制提升表达力。</td><td><a href="https://arxiv.org/abs/2505.16245">2505.16245</a></td></tr><tr><td><strong>多样性的解码策略</strong></td><td><strong>TURNING UP THE HEAT: MIN- SAMPLING FOR CREATIVE AND COHERENT LLM OUTPUTS</strong></td><td>提出 Min-p 采样，通过动态阈值在保持连贯性的同时大幅提升创意多样性。</td><td><a href="https://arxiv.org/abs/2407.01082">2407.01082</a></td></tr><tr><td></td><td><strong>VERBALIZED SAMPLING: HOW TO MITIGATE MODE COLLAPSE AND UNLOCK LLM DIVERSITY</strong></td><td>通过“口语化”提示词在推理阶段解锁模型原本被对齐算法压缩的多样性。</td><td><a href="https://arxiv.org/abs/2510.01171">2510.01171</a></td></tr><tr><td></td><td><strong>Negative Token Merging: Image-based Adversarial Feature Guidance</strong></td><td>虽然偏向图像生成，但其对抗性特征引导思路为减少生成内容的同质化提供了新策略。</td><td><a href="https://arxiv.org/abs/2412.01339">2412.01339</a></td></tr><tr><td><strong>在训练过程中引入多样性</strong></td><td><strong>Creative Preference Optimization (CrPO)</strong></td><td>将多个维度的创意指标直接注入偏好优化目标中，超越单一的质量指标。</td><td><a href="https://arxiv.org/abs/2505.14442">2505.14442</a></td></tr><tr><td></td><td><strong>SPECTRUM TUNING: POST-TRAINING FOR DISTRIBUTIONAL COVERAGE AND STEERABILITY</strong></td><td>针对指令微调导致的多样性坍缩，通过频谱调优恢复分布覆盖面。</td><td><a href="https://arxiv.org/abs/2510.06084">2510.06084</a></td></tr><tr><td></td><td><strong>Modifying LLM Post-Training for Diverse Creative Writing</strong></td><td>修改 DPO&#x2F;ORPO 算法逻辑，在提升生成质量的同时减少输出的重复感。</td><td><a href="https://arxiv.org/abs/2503.17126">2503.17126</a></td></tr><tr><td></td><td><strong>Diverse Preference Optimization (DivPO)</strong></td><td>在偏好对选择中加入多样性准则，鼓励模型学习更稀有的高质量表达。</td><td><a href="https://arxiv.org/abs/2501.18101">2501.18101</a></td></tr><tr><td><strong>更加严谨的评估多样性</strong></td><td><strong>NOVELTYBENCH: Evaluating Language Models for Human-like Diversity</strong></td><td>构建全新基准，评估模型在主观&#x2F;开放问题上是否能像人类一样提供多种解法。</td><td><a href="https://arxiv.org/abs/2504.05228">2504.05228</a></td></tr><tr><td></td><td><strong>EVALUATING THE DIVERSITY AND QUALITY OF LLM GENERATED CONTENT</strong></td><td>提出“有效语义多样性”概念，剔除那些多样但低质（乱码）的输出干扰。</td><td><a href="https://arxiv.org/abs/2504.12522">2504.12522</a></td></tr><tr><td></td><td><strong>QUDSIM: Quantifying Discourse Similarities in LLM-Generated Text</strong></td><td>深入话语结构层面，量化模型在逻辑推进方式上的相似度（而非仅字面重复）。</td><td><a href="https://arxiv.org/abs/2504.09373">2504.09373</a></td></tr><tr><td></td><td><strong>CreativityPrism: A Holistic Benchmark for LLM Creativity</strong></td><td>借鉴三棱镜原理，从质量、新颖性、多样性三个维度拆解评估创造力。</td><td><a href="https://arxiv.org/abs/2510.20091">2510.20091</a></td></tr><tr><td></td><td><strong>DEATH OF THE NOVEL(TY): BEYOND -GRAM NOVELTY AS A METRIC</strong></td><td>指出传统的 -gram 评估法已过时，强调需结合“实用性”来衡量真正的创新。</td><td><a href="https://arxiv.org/abs/2509.22641">2509.22641</a></td></tr></tbody></table><hr><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;对于单个LLM，在面对开放式问题时的输出具有同质化的现象。作者从Infinity-Chat100数据集中采样50个case，设置temperature&#x3D;1.0，Top-p&#x3D;0.9，让每个LLM生成50个回答。随后采用余弦相似度统计回答之间的相似性。如下图，对于单个LLM，余弦相似度&gt;&#x3D;0.8的case占比很高。<br><img src="./2025-12-30-18-34-00.png"></p><p>&emsp;&emsp;对于不同的LLM，他们的回答也是高度相似的。如图DeepSeek-V3和gpt-4o的回答的相似度很高。<br><img src="./2025-12-30-18-41-12.png"></p><p>&emsp;&emsp;作者又做了一些实验，如：在“用一句话描述时间” 这个开放式问题中，用25个模型每个模型50个回答，绘制Embedding的图像。如下图，模型的回答有2个明显的聚类中心，左边的聚类基本把时间比喻成河流，右边的聚类基本把时间比喻成纺织工。<br><img src="./2025-12-30-18-43-53.png"></p><p>&emsp;&emsp;我们已经知道了LLM在输出的时候有高度同质化的现象。作者的逻辑是：生成内容的同质化，很可能是因为评价标准的僵化导致的。这种同质化背后隐含的是，在开放式问题中，LLM对同样质量的回答可能会有不同的偏好，导致他们只能回答特定的几种范式。现在的模型是通过 RLHF（基于人类反馈的强化学习）训练的，而 RLHF 极度依赖Reward model或LM Judge给出的分数。如果这些 AI 裁判在面对两个“同样好但风格不同”的回答时，无法像人类那样识别出细微的价值，或者在人类有分歧时只会盲目给某一个高分，那么模型在训练过程中就会被诱导去生成那种“保险、平庸、符合 AI 审美”的同质化内容。本文希望能够深入地分析LM Judge对回答的评分是否合理，是否与人类评估保持一致。</p><p>&emsp;&emsp;本文的主要贡献在于：</p><ul><li>构建了INFINITY-CHAT，这是一个包含 26,000 条 真实世界开放式问答的大规模数据集。这些问答源自 WildChat，涵盖了自然发生的、多样化的用户提示词。这些开放式问题，为LLM多样性的研究奠定了基础。</li><li>发现了模型内重复，模型间同质化的现象。</li><li>在研究LM Judge与人类是否一致之前，本文贡献了一个大规模的人类偏好标注数据：包含人类对开放问题回答的绝对值评分和相对评分(which is better)。</li><li>发现了模型与人类偏好的失调，在答案质量相当或者人工标注分歧较大的情况下，LLM Judge往往无法准确反映人类评分。</li></ul><h2 id="INFINITY-CHAT的构建"><a href="#INFINITY-CHAT的构建" class="headerlink" title="INFINITY-CHAT的构建"></a>INFINITY-CHAT的构建</h2><ol><li><p>原始数据挖掘与初步筛选：<br>研究者首先从 WildChat（一个真实用户与大模型互动的对话数据集）中获取原始输入。从海量对话中筛选出 37,426 条 高质量的单轮 GPT-4 查询。语言：仅限英语。内容：无毒。长度：字符数限制在 15–200 之间，确保查询既有意义又不至于过于冗长。</p></li><li><p>自动化分类与精炼：<br>由于 WildChat 包含各种类型的互动，研究者利用 GPT-4o 对上述查询进行细化处理：判断查询是寻求实质性信息，还是简单的问候或对模型的询问。区分该查询是只允许单一正确答案，还是允许多个有效答案。对于表述模糊的查询，利用模型进行重写以确保意图清晰。最终提炼出 26,070 条开放式查询 和 8,817 条封闭式查询。</p></li><li><p>构建开放式查询分类体系<br>为了系统化理解这些开放式问题，研究者开发了一套细粒度的分类框架：<br>人工标注约 100 条查询，赋予初步标签。利用 GPT-4o 对全量 26K 条开放式查询进行自动标注，并由模型检测是否存在种子类别之外的“新类型”（最终发现了 314 个新颖的子类，如文化分析、伦理等）。通过不断的分组和提炼，形成层级结构。最终确定了 6 个高层级类别（如创意内容生成、脑暴与构思等）和 17 个细粒度子类。</p></li></ol><p>具体的分类如下：<br><img src="./2025-12-30-19-05-52.png"></p><h2 id="人工偏好数据的收集"><a href="#人工偏好数据的收集" class="headerlink" title="人工偏好数据的收集"></a>人工偏好数据的收集</h2><p>&emsp;&emsp;为了能够研究LM Judge对开放式问题的评估与人类是否一致，本文首先通过大规模的数据标注，捕捉人类偏好的分布。具体来说，本文收集了绝对评分（采用 1–5 分制衡量回复质量）和成对比对偏好评分（在同一查询的两个回复之间选择“强&#x2F;弱偏好”）。</p><ul><li>绝对评分: 针对来自 INFINITY-CHAT100 的 50 个提示词，每个提示词采样 15 个回复，并对每个“查询-回复”对收集 25 份评分，总计获得 $25 \times 15 \times 50 &#x3D; 18,750$ 条标签。</li><li>成对比对偏好评分: 为每个提示词采样 10 组回复对，并对每个“查询-回复1-回复2”三元组收集 25 份标注，总计获得 $25 \times 10 \times 50 &#x3D; 12,500$ 条标签。</li></ul><p>&emsp;&emsp;对每个case，会有25位标注者打分。这样我们就得到了一个打分的数据分布，然后计算该分布的香农熵。统计不同Entropy下的数据比例。第一张图是绝对评分的Entropy分布，第二张图是相对评分的Entropy分布。可以看到，人工标注的评分Entropy很高，说明了人类偏好的多样性。<br><img src="./2025-12-30-16-56-48.png"><br><img src="./2025-12-30-16-59-12.png"></p><h2 id="在开放式问题中，-LM-Judge与人类是否相似"><a href="#在开放式问题中，-LM-Judge与人类是否相似" class="headerlink" title="在开放式问题中， LM Judge与人类是否相似"></a>在开放式问题中， LM Judge与人类是否相似</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>作者将三类由模型生成的评分与人工标注进行对比：</p><ul><li>LM 评分 (LM scores)： 根据给定查询下回复的困惑度（Perplexity）推导得出。</li><li>奖励模型评分 (Reward model scores)： 基于标准化的标量奖励输出。</li><li>LM 裁判评分 (LM judge ratings)： 遵循标准提示词协议，采用两类评价准则：综合质量评分和 HHH 准则（有用性 Helpfulness、无害性 Harmlessness、诚实性 Honesty）。</li></ul><h3 id="实验一：质量相近的回答，LLM-Judge与人类评估的一致性差"><a href="#实验一：质量相近的回答，LLM-Judge与人类评估的一致性差" class="headerlink" title="实验一：质量相近的回答，LLM Judge与人类评估的一致性差"></a>实验一：质量相近的回答，LLM Judge与人类评估的一致性差</h3><p>该实验表明：对人类来说质量比较相近的案例，LLM Judge会给出不同的评分。</p><h4 id="绝对评分的一致性分析"><a href="#绝对评分的一致性分析" class="headerlink" title="绝对评分的一致性分析"></a>绝对评分的一致性分析</h4><p>&emsp;&emsp;作者对750个case进行Perplexity, Reward model scores, LM judge ratings的打分，并计算和人类打分的相关性。这地方比较绕，举个例子：LM judge会对750个case输出一个score list: [3, 7, 8, 9, …..]，25个人工打分也会对case计算一个average score list: [3, 7, 1, 3, …] 我们可以计算这两个score list的相关性，来分析LM Judges与人类是否相似。</p><p>&emsp;&emsp;进一步的，作者对750个model output做了不同层次的过滤：使用Tukey’s fences 过滤法，作者将系数 $k$ 从 0.5 逐渐调整到 3.0。$k$ 越小（如 0.5）： 过滤越“激进”，只留下那些分数高度集中、质量极其接近的样本。$k$ 越大（如 3.0）： 过滤越“保守”，包含的样本差异稍微大一些。举个例子：我们可以计算出750个cases的average score。然后使用该score和k过滤掉cases中分数太高和太低的case，这样就可以过滤掉一些离群样本。这样的话样本的分数就更加集中。</p><p>&emsp;&emsp;图中随着样本的绝对评分越来越集中，LM Judge的评分和人类评分相关性越来越低，说明LM Judge不能区分质量比较接近的回答。<br><img src="./2025-12-30-17-35-17.png"></p><h4 id="相对评分的一致性分析"><a href="#相对评分的一致性分析" class="headerlink" title="相对评分的一致性分析"></a>相对评分的一致性分析</h4><p>&emsp;&emsp;相对评分的数据一共是50个问题 * 10个Pair(每个pair是 model1 output和 model2 output) &#x3D; 500条，每一条都有25个标注者标注1和2了两个回答哪个好。在数据分析中，作者根据25名标注员中，有多少人选择了“这两个回复质量”相似，构建子集。作者选取人工标注相似度最高的前 60%–95% 的示例构成子集，并进行实验。</p><p>&emsp;&emsp;如下图，随着人工标注越来越相似，LM Judges的方法打出的分数和人类打出的分数相关性越来越低。换句话说：人类觉得差不多的回答，LM Judges打出的分数有高有低。<br><img src="./2025-12-30-17-36-45.png"></p><h3 id="实验二：-LLM-Judge-在标注员存在分歧时对齐度较低"><a href="#实验二：-LLM-Judge-在标注员存在分歧时对齐度较低" class="headerlink" title="实验二： LLM Judge 在标注员存在分歧时对齐度较低"></a>实验二： LLM Judge 在标注员存在分歧时对齐度较低</h3><p>该实验表明：对人类来说分歧比较大的案例，LLM Judge和人类评估的相关性降低。</p><h4 id="绝对评分的情况"><a href="#绝对评分的情况" class="headerlink" title="绝对评分的情况"></a>绝对评分的情况</h4><p>&emsp;&emsp;通过计算 25 份人类标签的香农熵，对绝对评分数据进行排序。随后，选取熵值最高的前 2%、4%、6%、8%、10%、12%、14% 和 16% 的示例作为分歧子集。计算模型评分与人类评分之间的 Pearson 相关系数。结果发现，人工标注的分歧越大，LLM Judge和人工标注分数的相关性越低。<br><img src="./2025-12-30-18-01-02.png"></p><h4 id="成对偏好评分的情况："><a href="#成对偏好评分的情况：" class="headerlink" title="成对偏好评分的情况："></a>成对偏好评分的情况：</h4><p>&emsp;&emsp;作者利用“分歧百分比”（$P_{disagree}$）来量化每个“查询-回复1-回复2”三元组的分歧程度：$$P_{disagree} &#x3D; 1 - \frac{\max(C_{prefer 1}, C_{prefer 2}) + 0.5 \cdot C_{tie}}{C_{total}}$$其中 $C$ 代表每种偏好类型的标注数量。如果选回复 1 的人数和选回复 2 的人数旗鼓相当，这个数值就会变大。</p><p>&emsp;&emsp;作者保留了分歧程度最高的前 60% 到 95% 的示例，并在他们上面计算作者保留了分歧程度最高的前 60% 到 95% 的示例，并在他们上面计算LLM Judge和人工标注分数的Pearson 相关系数。结果发现，人工标注的分歧越大，LLM Judge和人工标注分数的相关性越低。<br><img src="./2025-12-30-18-06-59.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;这篇论文警告我们，AI 正在形成一种“集体思想”，这种开放式的同质化可能会像回声壁一样，在未来削弱人类社会的创造力和思想深度。未来的训练不应只追求迎合“平均人类偏好”，而应学会模拟和尊重人类偏好的分布，允许模型生成更具差异化的内容。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>An Empirical Study of Reasoning Steps in Thinking Code LLMs</title>
    <link href="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/"/>
    <url>/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/</url>
    
    <content type="html"><![CDATA[<h2 id="文章概述"><a href="#文章概述" class="headerlink" title="文章概述"></a>文章概述</h2><p>&emsp;&emsp;这篇论文题目为《An Empirical Study of Reasoning Steps in Thinking Code LLMs》，从steps count, Efficiency, Logic Consistency, Completeness的角度分析了Reasoning model在解决代码问题时的Reasoning steps的具体内容。</p><p>&emsp;&emsp;本文主要的贡献是：</p><ul><li>建立了对Reasoning过程分析的标准，提出从steps count, Efficiency, Logic Consistency, Completeness入手进行分析。</li><li>通过对实验数据的分析，得出了一些结论：Reasoning steps好像没有什么规律，有的模型每一步思考的少但思考多步，有的模型思考的多但思考少步；在代码问题中，模型错误的原因有44.5%都是Completeness Issues（忽略边界、忽略题目中某个约束条件、只生成逻辑没有验证的代码、推理中断）</li></ul><p>但我看下来，论文数据标注和分析有很多不严谨的地方，涉及到人工标注、数据分析，可能都是如此，不像刷benchmarks那样简单直接。另一方面，文章篇幅太长了，足足36页还不加附录，可能是要投期刊所以这么长。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;传统的非思考型（non-thinking）LLMs在处理复杂编程任务时存在透明度不足的问题 。即使模型生成了正确的代码，其内部的推理过程（如假设、决策点）仍然是潜在的，难以进行独立验证和审计 。近期出现的“Thinking LLMs”（如 OpenAI-o1&#x2F;o3、DeepSeek-R1、Claude 3.7 Sonnet-thinking 等）通过显式生成中间推理链（Reasoning Traces）来模拟人类的认知过程，有望提升代码生成的透明度和准确性 。</p><p>尽管这些模型表现优异，但其推理链的质量（如推理步骤的长度、逻辑严密性）尚未得到系统的评估 。</p><p>本文希望对LLMs在解决代码问题时的推理链进行分析，深入研究以下内容：</p><ul><li><p>RQ1：推理过程分析：研究推理链的结构特征（步骤数、词数）及其与成功率的关系 。</p></li><li><p>RQ2：推理质量分析：通过 21 名参与者进行的人工评估，从三个维度打分 ：<br>效率（Efficiency）：推理是否精炼且无冗余。<br>逻辑一致性（Logic Consistency）：步骤之间是否逻辑连贯。<br>完整性（Completeness）：是否覆盖了所有需求和边缘情况。</p></li></ul><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="RQ1-推理Steps分析"><a href="#RQ1-推理Steps分析" class="headerlink" title="RQ1 : 推理Steps分析"></a>RQ1 : 推理Steps分析</h3><p>针对RQ1，作者提出了几个子问题：</p><ul><li>Thinking model对每个task需要思考几步，成功的cases和失败的cases思考的次数差别大吗？</li><li>如果人为地增加或者缩短thinking steps，会影响成功率吗？</li></ul><p>这里直接给出结果：</p><ul><li>模型的thinking steps见Table2；思考的次数差别因模型而异，没什么规律。</li><li>人为增加thinking steps对简单任务的影响不大，对困难任务可能提高也可能降低；认为减少thinking steps对简单和困难任务均有影响，但是对困难任务影响大一些(只减少10%的thinking steps就会导致模型崩溃)</li></ul><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h4><p>&emsp;&emsp;本文在如下的Reasoning model上收集数据。具体来说，本文从BigCodeBench这个代码测试的数据集中选取100个tasks，其中Hard tasks和Full tasks与原数据集的分布一致(分别为14% 和 86%)。随后，这些题目被送入6个测试模型中，提示词要求模型按照 &lt;step1&gt;&lt;step1&gt; …. &lt;stepn&gt;&lt;stepn&gt;的格式输出。最终收集了共600个回答，并统计了每个模型的Total Reasoning Steps和 Average Steps per Task， 如下图所示。<br><img src="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/2025-12-29-11-09-11.png"><br><img src="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/2025-12-29-11-10-03.png"></p><h4 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h4><p>&emsp;&emsp;本文对Reasoning model在困难数据集和一般数据集上的Reasoning steps做了统计，统计结果如下。对这些数据进行分析，发现在一般数据集上(对应 Full)，模型的成功率和Reasoning steps相关性趋近于0；在困难数据集上，推理的深度才发挥作用。<br><img src="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/2025-12-29-11-14-46.png"></p><p>&emsp;&emsp;考虑人为地增加或者缩短模型的thinking steps，比如通过提示词规定，模型必须在k步内解决问题。本文对Reasong model成功解决的问题集进行了thinking steps reduce，解决失败的问题进行了thinking steps increase。根据reduce&#x2F;increase的比例，每10%绘制一个散点，绘制出散点图如下： 随着thinking steps的增加，Full任务上模型表现基本不变，Hard任务上模型表现比较混乱，没什么规律。随着thinking steps的减少，Full&#x2F;Hard任务上模型表现均有下降，Hard任务下降的程度更高。（说实话都没什么规律，但是作者还是强行分析了一下）<br><img src="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/2025-12-29-11-24-56.png"><br><img src="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/2025-12-29-11-25-15.png"></p><h3 id="RQ2-推理质量分析"><a href="#RQ2-推理质量分析" class="headerlink" title="RQ2:推理质量分析"></a>RQ2:推理质量分析</h3><p>针对RQ2, 作者希望从3个角度分析Reasoning Steps： 效率(推理是否精炼且无冗余) 逻辑一致性(步骤之间是否逻辑连贯) 完整性(是否覆盖了所有需求和边缘情况)。为了达到这个目的，作者构建了对错误数据的Reasoning Steps的分类方法，并邀请了 21 名参与者进行数据标注。具体的实验方法和结果在下面讨论。</p><h4 id="人工数据标记的方法"><a href="#人工数据标记的方法" class="headerlink" title="人工数据标记的方法"></a>人工数据标记的方法</h4><ul><li><p>人员背景：21 名标注者(14 名软件工程方向的研究生和 7 名具有 3-8 年工作经验的专业软件工程师。)所有人精通 Python，并对 LLM 推理（Chain-of-Thought）有实际使用经验。</p></li><li><p>标记方法：每个维度都采用 3 分制（1-Poor, 2-Fair, 3-Good）：</p><ul><li>效率（Efficiency）：模型是否绕弯路？是否有大量重复的废话？高分标准：推理过程精炼，每一步都直接导向最终解决方案。</li><li>逻辑一致性（Logic Consistency）：推理步骤之间是否连贯？前面的假设和后面的操作是否矛盾？高分标准：推理链条无逻辑断裂，思维流转自然且正确。</li><li>完整性（Completeness）：逻辑：模型是否考虑了所有题目约束？是否分析了边缘情况？高分标准：所有的输入要求都被逐一分析，且包含了对潜在错误（如空输入、越界等）的预判。</li></ul></li><li><p>其他的标注标准</p><ul><li><p>盲审：标注者不知道当前阅读的推理链是由哪个模型生成的，以消除品牌偏见。</p></li><li><p>基准培训：在正式标记前，所有人先共同标记几个样本，对“什么是 1 分，什么是 3 分”达成共识。</p></li><li><p>独立标注 ：每个推理链至少由 2 名标注者 独立打分。</p></li><li><p>一致性检查：使用 Cohen’s Kappa 系数计算两人打分的一致性。如果系数低（&lt;0.6），说明评分标准模糊，需重新培训。本文的 Kappa 值在 0.72-0.85 之间，属于“高度一致”。</p></li></ul></li></ul><h4 id="标注结果分析"><a href="#标注结果分析" class="headerlink" title="标注结果分析"></a>标注结果分析</h4><p><img src="/2025/12/28/An-Empirical-Study-of-Reasoning-Steps-in-Thinking-Code-LLMs/2025-12-29-16-17-01.png"></p><ol><li><p>完整性缺陷—— 占比最高 (44.5%)</p><ul><li>推理中断 ：模型在没得出最终结论或代码计划前，推理链突然停止（可能受限于 Token 长度）。</li><li>缺失要求覆盖：忽略了题目中的某个特定约束或功能点。</li><li>忽视边界情况 ：未能考虑极端输入（如空列表、极大值、类型错误等）。</li><li>验证缺失 ：推理中只有生成逻辑，没有自我检查或预演代码运行结果的步骤。</li></ul></li><li><p>效率缺陷—— 占比 33.5%</p><ul><li>冗余思考：模型不断重复已知信息，或者在不同的措辞下循环同一逻辑。</li><li>过度解释细节：在一些极其简单的逻辑（如变量赋值）上花费大量篇幅，导致推理链虚胖。</li><li>死胡同搜索 ：模型尝试了一种错误的思路，在意识到错误后又花了大量篇幅解释为什么错了，而不是迅速转向。</li></ul></li><li><p>逻辑与正确性缺陷—— 占比 22%</p><ul><li>逻辑矛盾：推理的前后步骤相互矛盾（例如开头说用 A 算法，结尾却在写 B 算法的逻辑）。</li><li>事实错误 ：对编程语言语法、标准库函数的功能理解有误。</li><li>错误的转换：从推理计划转换为最终代码时，丢失了推理中原本正确的逻辑。</li></ul></li></ol><h2 id="我的收获"><a href="#我的收获" class="headerlink" title="我的收获"></a>我的收获</h2><p>&emsp;&emsp;该研究为我们理解“Thinking LLMs”提供了一些启示 ：推理长度不是唯一指标：步数多不代表质量高。完整性是核心瓶颈：目前模型在代码生成中最弱的环节是未能完全分析所有约束和边缘情况（特别是难题） 。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>FOCUSED CHAIN OF THOUGHT EFFICIENT LLM REA SONING VIA STRUCTURED INPUT INFORMATION</title>
    <link href="/2025/12/25/FOCUSED-CHAIN-OF-THOUGHT-EFFICIENT-LLM-REA-SONING-VIA-STRUCTURED-INPUT-INFORMATION/"/>
    <url>/2025/12/25/FOCUSED-CHAIN-OF-THOUGHT-EFFICIENT-LLM-REA-SONING-VIA-STRUCTURED-INPUT-INFORMATION/</url>
    
    <content type="html"><![CDATA[<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;现有的LLM在解答问题时，通过产生chain-of-thought 思维链，显著提升了能力。但是我们观察到一个现象：LLM在解题时消耗的token数越来越大了，有时候为了解决一个很简单的乘法问题，都要输出几百上千的tokens。可以说，COT在提升模型性能的同时，带来了极大的token消耗。<br>&emsp;&emsp;本文作者通过观察现有的LLM在解数学题时的COT数据集，发现了token消耗的原因：LLM在解题的时候，输出的很大一部分不是解题的计算步骤，而是对题目的分析过程。进一步的，作者把LLM Output分成3类：Extraction （从题干中抽取相关信息）、Reasoning（计算和分析的步骤）、Filter（如”Wait, let me re-read the problem” 之类的无关短语）。<br><img src="./2025-12-25-17-27-32.png"><br><img src="./2025-12-25-17-28-06.png"></p><p>&emsp;&emsp;如何让LLM用更少的token达到同样的表现，更大比例的token用于真正的思考，而不是输出无关或者非解题的内容，是当前需要解决的问题。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>&emsp;&emsp;本文借鉴了人类解题时候的思路: 先从题目题干中抽取所有已知的信息，以及需要求解的问题，再执行真正的解题步骤。而不是像现在的LLM一样，边分析别解题。具体来说，LLM在解答题目之前，需要先从Question中抽取出关键信息和问题，在进行COT。如图，作者把一个具体的问题建模成结构化的形式。<br><img src="./2025-12-25-17-35-29.png"><br><img src="./2025-12-25-17-35-17.png"></p><p>&emsp;&emsp;在实验过程中，作者发现：对参数量比较小的模型(0.6B 1B)，将问题修改为结构化的形式会比较困难。因此，实际的解决策略包含两种：</p><ul><li>Pre-formatting：使用GPT5这类能力比较强的LLM进行结构化，再把结构化后的问题传给测试模型。</li><li>Two-step prompting: 让测试模型自己先输出结构化问题，再进行回答。</li></ul><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul><li>models: Qwen-3 (0.6B, 4B, 14B, 32B)作为测试的模型</li><li>benchmarks: SVAMP, GSM-Hard, MATH-500 (都是数学问题，该场景更能体现模型的reasoning能力)</li><li>metrics: Pass@5 metric(衡量模型解题的准确率)  #Tokens(衡量模型消耗的token数)</li></ul><h3 id="COT和F-COT-本文提出的方法-的对比"><a href="#COT和F-COT-本文提出的方法-的对比" class="headerlink" title="COT和F-COT(本文提出的方法)的对比"></a>COT和F-COT(本文提出的方法)的对比</h3><p>&emsp;&emsp;图中的100%虚线是传统COT的accuracy和token count的baseline。蓝色色柱为accuracy，黄色色柱为token count。可以明显的看到，F-COT解题时算上结构化的题目和推理的过程，在性能与baseline相当的情况下，token消耗平均只有50%不到。<br><img src="./2025-12-25-17-44-05.png"></p><p>更具体的，在保持差不多表现的情况下，这些模型解题时消耗的token都有大幅的下降。<br><img src="./2025-12-25-18-04-59.png"></p><h3 id="Overthinking-score的计算"><a href="#Overthinking-score的计算" class="headerlink" title="Overthinking score的计算"></a>Overthinking score的计算</h3><p>&emsp;&emsp;<a href="https://arxiv.org/abs/2502.08235">Cuadron et al. (2025)</a>在论文中提到了Overthinking score这个指标：它反映了模型倾向于优先进行抽象的反思、推测或多分支规划，而非执行能直接推进问题解决的具体数学步骤。该评分范围为 0 到 10，用于衡量模型在内部思考与具体解题之间的侧重程度。低分表示模型进行的是专注、循序渐进的推理，且极少进行推测性讨论；而高分则反映出模型存在大量的Meta-reasoning、频繁切换方法，或是在未推导中间结果的情况下便过早得出结论。</p><p>&emsp;&emsp; 具体的实验过程就是：使用LLM-as-judge，让大的LLM使用特定的Prompt对模型在COT和F-COT场景下的回答打分，并计算出一个平均的overthinking score。经过实验，作者发现LLM的overthinking score从2.35 ± 1.5 降低至1.74±1.4。<br><img src="./2025-12-25-17-54-04.png"><br><img src="./2025-12-25-17-53-33.png"><br><img src="./2025-12-25-17-53-50.png"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><ol><li><p>context format会影响结果吗？现在的结构化问题描述是使用&lt;contex&gt;标签包裹的结构化数据。如果是没有这些标签，使用有序列表、无序列表、直接拼接的情况下，会影响结果吗？<br>如图Listing15 16 17分别对应上述的3种结构化的格式，可以看到，不同的格式基本不影响模型的表现，且都节省了大量的token输出。<br><img src="./2025-12-25-18-00-44.png"></p></li><li><p>让模型自己生成结构化的问题描述，会影响模型表现吗？本文测试了不同size的LLM生成context的情况下的表现，除了Qwen3.0-0.6B由于模型能力不足，不能产生完整的context，导致accuracy显著下降外，其余的情况下模型表现都很好。<br><img src="./2025-12-25-18-06-28.png"></p></li></ol><h2 id="阅读总结"><a href="#阅读总结" class="headerlink" title="阅读总结"></a>阅读总结</h2><p>&emsp;&emsp;本文聚焦LLM COT，发现LLM输出COT的内容中有相当一部分是无关内容。如果模型一直在输出”Wait, let me re-read the problem”这样的无关内容，消耗了大量的token且对实际表现没有提升。针对该问题，本文提出先extract再reasoning的方法，显著降低了reasoning的token消耗。此外，本文在分析COT内容时，使用的”Extraction Reasoning Filter”分类的方法也很有借鉴意义。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Through the Valley: Path to Effective Long CoT Training for Small Language Models</title>
    <link href="/2025/12/23/Through-the-Valley-Path-to-Effective-Long-CoT-Training-forSmall-Language-Models/"/>
    <url>/2025/12/23/Through-the-Valley-Path-to-Effective-Long-CoT-Training-forSmall-Language-Models/</url>
    
    <content type="html"><![CDATA[<h2 id="阅读总结"><a href="#阅读总结" class="headerlink" title="阅读总结"></a>阅读总结</h2><p>&emsp;&emsp;</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>&emsp;&emsp;</p><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><p>&emsp;&emsp;</p><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><p>&emsp;&emsp;</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>TURNING UP THE HEAT: MIN-p SAMPLING FOR CREATIVE AND COHERENT LLM OUTPUTS</title>
    <link href="/2025/12/21/TURNING-UP-THE-HEAT-MIN-p-SAMPLING-FOR-CREATIVE-AND-COHERENT-LLM-OUTPUTS/"/>
    <url>/2025/12/21/TURNING-UP-THE-HEAT-MIN-p-SAMPLING-FOR-CREATIVE-AND-COHERENT-LLM-OUTPUTS/</url>
    
    <content type="html"><![CDATA[<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>&emsp;&emsp;在 LLM 的文本生成过程中，核心挑战在于如何在 “创造性 ” 和 “连贯性 ” 之间取得平衡。(其实就是之前提到的 Diversity 和 Quality 只是换了个说法)</p><p>现有的采样方法：</p><ul><li>Temperature (温度系数)： 提高温度（例如 $T &gt; 1.0$）可以增加多样性，但会使概率分布变平坦；降低温度可以增加确定性，但会使概率分布变尖锐。Temperature的本质就是把原来的softmax()增加一个T的缩放因子。<br>$$P_i &#x3D; \frac{\exp(Logit_i &#x2F; T)}{\sum \exp(Logit_j &#x2F; T)}$$</li></ul><table><thead><tr><th>场景</th><th>温度设置 ()</th><th>候选词 A (高概率词)</th><th>候选词 B (中概率词)</th><th>候选词 C (低概率词)</th></tr></thead><tbody><tr><td><strong>原始得分 (Logits)</strong></td><td>(未缩放)</td><td><strong>2.0</strong></td><td><strong>1.0</strong></td><td><strong>-1.0</strong></td></tr><tr><td><strong>低温 (保守)</strong></td><td>T &#x3D; 0.1</td><td><strong>99.99%</strong></td><td><strong>0.01%</strong></td><td><strong>0.00%</strong></td></tr><tr><td><strong>标准 (默认)</strong></td><td>T &#x3D; 1.0</td><td><strong>70.53%</strong></td><td><strong>25.95%</strong></td><td><strong>3.52%</strong></td></tr><tr><td><strong>高温 (创意)</strong></td><td>T &#x3D; 2.0</td><td><strong>44.91%</strong></td><td><strong>27.25%</strong></td><td><strong>27.84%</strong></td></tr></tbody></table><ul><li><p>Top-p (Nucleus Sampling)： 先把所有的output概率从大到小排序，再选取累积概率达到 $p$ 的前几个词。在高温度下，由于长尾分布变平，Top-p 仍然会把许多低概率、不连贯的词纳入候选池，导致生成质量崩塌。Top-p的本质就是设置了一个累计的概率阈值：<br><img src="./2025-12-21-15-15-49.png"></p></li><li><p>Top-k： 强制只选前 $k$ 个词。这是一种静态截断，无法根据模型对当前上下文的“信心”动态调整，缺乏灵活性 。</p></li></ul><p>核心痛点： 现有的方法在高温度设置下，很难在保持逻辑连贯的同时提供高质量的创造性输出，且做不到动态调整。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>&emsp;&emsp;为了解决这些痛点，本文提出了Min-p 采样。该采样方法解决了以下问题：</p><ul><li>即使在极高温度（如 $T&#x3D;3.0$）下，Min-p 也能过滤掉那些相对于“最佳候选词”概率过低的词，从而防止模型胡言乱语。</li><li>解决了 Top-k 和 Top-p 对“模型确定性”不敏感的问题。Min-p 能根据模型是“非常确定”（高置信度）还是“犹豫不决”（低置信度）来自动收缩或放宽候选词的选择范围。</li><li>计算复杂度与易用性： 相比于 Mirostat 或 $\eta$-sampling 等基于熵的复杂采样方法，Min-p 非常简单，几乎不增加推理计算成本，且易于集成 。</li></ul><p>&emsp;&emsp;Min-p 的核心思想是基于模型对首选词的“信心”来进行动态截断。算法逻辑在每个解码步骤 $t$，假设模型预测的词表概率分布为 $P(x)$：找到最大概率： 找出当前最可能的那个词的概率值，记为 $p_{max}$ 。<br>$$p_{max} &#x3D; \max_{v \in V} P(v|x_{1:t-1})$$</p><p>&emsp;&emsp;计算截断阈值： 设定一个基础超参数 $p_{base}$（通常为 0.05 或 0.1），实际的过滤阈值 $p_{scaled}$ 是两者的乘积。<br>$$p_{scaled} &#x3D; p_{base} \times p_{max}$$</p><p>&emsp;&emsp;构建候选池： 只有概率值大于等于 $p_{scaled}$ 的词才会被保留，其他的全部剔除。<br>$$\mathcal{V}<em>{min} &#x3D; {v \in \mathcal{V} : P(v|x</em>{1:t-1}) \ge p_{scaled}}$$</p><p>&emsp;&emsp;重归一化与采样： 在剩下的候选词中进行归一化并采样 。</p><p>&emsp;&emsp;如果模型觉得下一个词是“Apple”的概率是 90% ($p_{max}&#x3D;0.9$)，且 $p_{base}&#x3D;0.1$，那么阈值就是 $0.09$。所有概率低于 9% 的词都会被丢弃。这迫使模型专注于高概率词，保持连贯。如果模型觉得下一个词概率都很低，最高的也才 10% ($p_{max}&#x3D;0.1$)，那么阈值就变成了 $0.01$。这样更多低概率的词就有机会入选，增加了创造性和多样性。</p><p>&emsp;&emsp;使用一张图片解释这个采样方法的优势：如图(a, c)，在高确定性的场景下，top-p方法会引入很多小概率的token，导致low quality，但是min-p方法可以过滤掉这些小概率tokne。如图(b, d)，在低确定性的场景下，top-k方法会忽略掉很多高概率的token，导致low diversity，但是min-p方法不会忽略他们。<br><img src="./2025-12-21-15-25-48.png"></p><h2 id="实验论证"><a href="#实验论证" class="headerlink" title="实验论证"></a>实验论证</h2><p>实验设置：</p><ul><li>测试模型： Mistral 7B, Llama 3 (8B&#x2F;70B), Mistral Large (123B) </li><li>对比基线： Top-p (p&#x3D;0.9&#x2F;0.95), Top-k, $\eta$-sampling, Mirostat, 单纯 Temperature 采样 </li><li>超参数： Min-p 设置为 $p_{base} \in {0.05, 0.1}$；Top-p 设置为 $0.9$ 或 $0.95$。</li><li>温度范围： 测试了从 $0.7$ 到 $3.0$ 甚至更高的温度范围</li></ul><h3 id="Mistral-7B在GPQA-GSM8K上的测试"><a href="#Mistral-7B在GPQA-GSM8K上的测试" class="headerlink" title="Mistral 7B在GPQA GSM8K上的测试"></a>Mistral 7B在GPQA GSM8K上的测试</h3><p>&emsp;&emsp;Mistral 7B 在不同温度下的准确率：在常规温度（$T \le 1.0$）下，Min-p 与 Top-p 表现相当。在高温度（$T&#x3D;1.5, 2.0, 3.0$）下，Top-p 的性能迅速崩溃，而 Min-p 仍然能保持较高的准确率。例如在 GSM8K CoT 任务中，当 $T&#x3D;3.0$ 时，Top-p 得分为 0.00%，而 Min-p 仍有 6.21% 的得分，甚至在 $T&#x3D;1.5$ 时 Min-p 还能达到 30% 以上的准确率，远超其他方法。<br><img src="./2025-12-21-15-06-36.png"></p><p>&emsp;&emsp;绘制GSM8K CoT 任务中“准确率 (Accuracy)”与“多样性 (Diversity&#x2F;Entropy)”的对比。图中绿色曲线（Min-p）始终位于灰色曲线（Top-p）的右上方。这说明在相同的准确率下，Min-p 能提供更高的多样性；或者在相同的多样性下，Min-p 能保持更高的准确率。<br><img src="./2025-12-21-15-07-39.png"></p><h3 id="Llama3-70B在故事生成任务上的测试-人工评估"><a href="#Llama3-70B在故事生成任务上的测试-人工评估" class="headerlink" title="Llama3 70B在故事生成任务上的测试 (人工评估)"></a>Llama3 70B在故事生成任务上的测试 (人工评估)</h3><p>&emsp;&emsp;使用 Llama 3 70B 生成故事，人类评价者根据“质量”和“多样性”打分（1-10分）。结果：在 $T&#x3D;3.0$ 的极端设置下，Top-p 的质量得分跌至 ~1.2&#x2F;10（完全不可读），而 Min-p 依然保持在 ~5.8&#x2F;10，且多样性得分也更高 24。人类评估者在盲测中更倾向于认为 Min-p 生成的文本既有创意又逻辑通顺。<br><img src="./2025-12-21-15-09-11.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ICLR2025</title>
    <link href="/2025/12/20/ICLR2025/"/>
    <url>/2025/12/20/ICLR2025/</url>
    
    <content type="html"><![CDATA[<h3 id="ICLR-2025-论文阅读总结"><a href="#ICLR-2025-论文阅读总结" class="headerlink" title="ICLR 2025 论文阅读总结"></a>ICLR 2025 论文阅读总结</h3><table><thead><tr><th>标题</th><th>URL</th><th>摘要</th><th>status</th></tr></thead><tbody><tr><td><strong>Reasoning Elicitation in Language Models via Counterfactual Feedback</strong></td><td><a href="https://papers.cool/venue/VVixJ9QavY@OpenReview">link</a></td><td>提出反事实反馈机制，通过平衡事实与反事实问题的指标来增强 LLM 的因果推理能力。</td><td></td></tr><tr><td><strong>ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement</strong></td><td><a href="https://papers.cool/venue/YUYJsHOf3c@OpenReview">link</a></td><td>提出 ReGenesis 框架，通过从抽象到具体的自我合成推理路径，使模型在无监督下提升泛化推理能力。</td><td></td></tr><tr><td><strong>Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</strong></td><td><a href="https://papers.cool/venue/UHPnqSTBPO@OpenReview">link</a></td><td>提出选择性评估框架，通过置信度估计确保 LLM 裁判与人类评价的一致性，不确定时升级到更强模型。</td><td></td></tr><tr><td><strong>Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs</strong></td><td><a href="https://papers.cool/venue/QWunLKbBGF@OpenReview">link</a></td><td>推出 PrefEval 基准测试，评估模型在长对话中推理、记忆和遵循用户个性化偏好的能力。</td><td></td></tr><tr><td><strong>MAP: Multi-Human-Value Alignment Palette</strong></td><td><a href="https://papers.cool/venue/NN6QHwgRrQ@OpenReview">link</a></td><td>提出多人类价值对齐调色板（MAP），利用约束优化技术解决多样化且动态的人类价值对齐问题。</td><td></td></tr><tr><td><strong>Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs</strong></td><td><a href="https://papers.cool/venue/FBkpCyujtS@OpenReview">link</a></td><td>提出 min-p 采样方法，根据模型置信度动态调整阈值，在提升生成创造性的同时保持连贯性。</td><td></td></tr><tr><td><strong>Backtracking Improves Generation Safety</strong></td><td><a href="https://papers.cool/venue/Bo62NeU6VF@OpenReview">link</a></td><td>引入 [RESET] 令牌实现回溯技术，允许模型撤销并纠正已生成的有害文本，显著提升安全性。</td><td></td></tr><tr><td><strong>From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</strong></td><td><a href="https://papers.cool/venue/QKBu1BOAwd@OpenReview">link</a></td><td>提出 DRAFT 框架，通过 LLM 与工具的自我驱动交互（试错与反馈）来动态优化工具文档。</td><td></td></tr><tr><td><strong>Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models</strong></td><td><a href="https://papers.cool/venue/mtJSMcF3ek@OpenReview">link</a></td><td>研究 LLM 自我改进的数学机理，发现“生成-验证差距”与预训练算力规模之间存在单调比例关系。</td><td></td></tr><tr><td><strong>Linear Representations of Political Perspective Emerge in Large Language Models</strong></td><td><a href="https://papers.cool/venue/rwqShzb9li@OpenReview">link</a></td><td>揭示 LLM 内部存在政治立场的线性表征，并展示了如何通过干预特定注意力头来控制输出倾向。</td><td></td></tr><tr><td><strong>Self-Improvement in Language Models: The Sharpening Mechanism</strong></td><td><a href="https://papers.cool/venue/WJaUkwci9o@OpenReview">link</a></td><td>提出“锐化”理论视角，利用模型的验证能力在后训练阶段优化生成分布，降低推理成本。</td><td></td></tr><tr><td><strong>A Decade’s Battle on Dataset Bias: Are We There Yet?</strong></td><td><a href="https://papers.cool/venue/SctfBCLmWo@OpenReview">link</a></td><td>重新审视数据集偏差，发现现代模型仍能轻易识别图像所属数据集，暗示数据偏差问题依然严峻。</td><td></td></tr><tr><td><strong>Limits to scalable evaluation at the frontier: LLM as judge won’t beat twice the data</strong></td><td><a href="https://papers.cool/venue/NO6Tv6QcDs@OpenReview">link</a></td><td>论证了“LLM 作为裁判”的理论极限，指出当裁判能力不足时，无法通过算法手段显著减少对人工标签的需求。</td><td></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>ICLR2025</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>科研想法汇总</title>
    <link href="/2025/12/20/%E7%A7%91%E7%A0%94%E6%83%B3%E6%B3%95%E6%B1%87%E6%80%BB/"/>
    <url>/2025/12/20/%E7%A7%91%E7%A0%94%E6%83%B3%E6%B3%95%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LLM 主流测试标准调研</title>
    <link href="/2025/12/19/LLM-%E4%B8%BB%E6%B5%81%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86%E8%B0%83%E7%A0%94/"/>
    <url>/2025/12/19/LLM-%E4%B8%BB%E6%B5%81%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;本文介绍了目前主流的测试LLM性能的benchmarks</p><h3 id="增强版大模型评估基准对照表-2025版"><a href="#增强版大模型评估基准对照表-2025版" class="headerlink" title="增强版大模型评估基准对照表 (2025版)"></a>增强版大模型评估基准对照表 (2025版)</h3><table><thead><tr><th>数据集名称</th><th>测试主题</th><th>核心测试方法</th><th>选型建议 &#x2F; 适用场景</th><th>资源链接</th></tr></thead><tbody><tr><td><strong>GPQA</strong></td><td>专家级科学</td><td>448道博士水平物理&#x2F;生物&#x2F;化学题。</td><td><strong>必选。</strong> 用于测试模型在垂直领域硬科学推理的“天花板”水平。</td><td><a href="https://huggingface.co/datasets/Idavidrein/gpqa">Dataset</a></td></tr><tr><td><strong>AIME 2025</strong></td><td>竞赛数学</td><td>深度数学推理，非选择题。</td><td><strong>进阶。</strong> 评估模型是否具备类似 o1&#x2F;o3 的深度思维（Reasoning）能力。</td><td><a href="http://huggingface.co/datasets/opencompass/AIME2025">Dataset</a></td></tr><tr><td><strong>C-Eval</strong></td><td><strong>中文综合能力</strong></td><td>13948道涵盖52个学科的选择题。</td><td><strong>中文必选。</strong> 国内大模型最基础的门槛测试，评估中文综合知识储备。</td><td><a href="https://huggingface.co/datasets/ceval/ceval-exam">Dataset</a></td></tr><tr><td><strong>CMMLU</strong></td><td><strong>中文综合知识</strong></td><td>涵盖67个学科，专门针对中文语境优化。</td><td><strong>中文必选。</strong> 补充 C-Eval，更侧重中文文化背景下的多任务知识评估。</td><td><a href="https://huggingface.co/datasets/haonan-li/cmmlu">Dataset</a></td></tr><tr><td><strong>SWE-bench</strong></td><td>软件工程</td><td>修改真实代码仓库并修复 Bug。</td><td><strong>Agent必选。</strong> 评估模型作为“AI程序员”在复杂环境下的工程落地能力。</td><td><a href="https://huggingface.co/datasets/SWE-bench/SWE-bench">Dataset</a></td></tr><tr><td><strong>AlignBench</strong></td><td><strong>中文指令对齐</strong></td><td>涵盖数学、逻辑、中文理解等8大维度。</td><td><strong>对齐评估。</strong> 衡量模型是否听话、回答是否符合中文用户的语言习惯。</td><td><a href="https://arxiv.org/abs/2311.16452">Paper</a></td></tr><tr><td><strong>MMLU</strong></td><td>通用世界知识</td><td>57个学科，1.16万道题。</td><td><strong>全球基准。</strong> 模型预训练阶段效果的最直观参考，用于横向对比模型量级。</td><td><a href="https://huggingface.co/datasets/cais/mmlu">Dataset</a></td></tr><tr><td><strong>HLE</strong></td><td>前沿专家知识</td><td>跨学科多模态，高难度。</td><td><strong>前沿探索。</strong> 当 MMLU 达到 90%+ 产生饱和时，用此项测试拉开差距。</td><td><a href="https://huggingface.co/datasets/cais/hle">Dataset</a></td></tr><tr><td><strong>Math-500</strong></td><td>高难度数学</td><td>从 MATH 中精选的 500 道难题。</td><td><strong>推理基准。</strong> 快速评估模型逻辑链条（CoT）是否严密，适合小样本快评。</td><td><a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500">Dataset</a></td></tr><tr><td><strong>C-Math</strong></td><td><strong>中文数学能力</strong></td><td>专门针对中文表述的数学问题。</td><td><strong>场景优化。</strong> 评估模型能否理解中文语境下的数学逻辑和数量关系。</td><td><a href="https://www.google.com/search?q=https://huggingface.co/datasets/Libert-S/C-Math">Dataset</a></td></tr><tr><td><strong>IFBench</strong></td><td>指令遵循</td><td>自动化评估格式&#x2F;字数约束。</td><td><strong>指令遵循。</strong> 适合评估模型在 RAG、长文本总结等对格式要求严苛的任务表现。</td><td><a href="https://huggingface.co/datasets/allenai/IF_multi_constraints_upto5">Dataset</a></td></tr><tr><td><strong>GSM8K</strong></td><td>小学数学应用</td><td>8500道多步算术推理。</td><td><strong>基础基准。</strong> 用于评估中轻量级模型（如 7B&#x2F;14B）的基本逻辑能力。</td><td><a href="https://huggingface.co/datasets/openai/gsm8k">Dataset</a></td></tr><tr><td><strong>HumanEval</strong></td><td>代码生成</td><td>164个 Python 编程问题。</td><td><strong>代码必选。</strong> 评估模型生成算法代码的纯净度和逻辑性，最经典的代码集。</td><td><a href="https://huggingface.co/datasets/openai/openai_humaneval">Dataset</a></td></tr><tr><td><strong>SuperCLUE</strong></td><td><strong>中文大模型总榜</strong></td><td>分级评估（L1-L5）的多维度测试。</td><td><strong>行业对比。</strong> 适合用于横向对比国内主流商用模型的地位。</td><td><a href="https://www.superclueai.com/">Link</a></td></tr></tbody></table><hr><hr><h3 id="GPQA"><a href="#GPQA" class="headerlink" title="GPQA"></a>GPQA</h3><p>paper: <a href="https://arxiv.org/abs/2311.12022">https://arxiv.org/abs/2311.12022</a><br>dataset:<a href="https://huggingface.co/datasets/Idavidrein/gpqa">https://huggingface.co/datasets/Idavidrein/gpqa</a></p><p>&emsp;&emsp;GPQA，这是一个由生物学、物理和化学领域专家编写的 448 道选择题的复杂数据集。拥有或正在攻读相关领域博士学位的专家准确率为 65%（扣除专家事后识别的明显错误后为 74%），而高技能的非专家验证者即使平均在无限制访问网络上花费超过 30 分钟。</p><ul><li>case<br>&emsp;&emsp;Two quantum states with energies E1 and E2 have a lifetime of 10^-9 sec and 10^-8 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they be clearly resolved?<br>&emsp;&emsp;能量分别为 E1 和 E2 的两个量子态的寿命分别为 10^-9 秒和 10^-8 秒。我们希望能够清晰地区分这两个能级。下列哪个选项可能是它们之间的能量差，以便能够清晰地区分它们？<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-33-02.png"></li></ul><hr><h3 id="AIME2025"><a href="#AIME2025" class="headerlink" title="AIME2025"></a>AIME2025</h3><p>dataset: <a href="http://huggingface.co/datasets/opencompass/AIME2025">http://huggingface.co/datasets/opencompass/AIME2025</a></p><p>&emsp;&emsp;该数据集包含了美国邀请数学考试（AIME）2025-I 和 II 的题目。主要测试LLM的math能力。</p><ul><li>case:<br>&emsp;&emsp;On $\triangle ABC$ points $A,D,E$, and $B$ lie that order on side $\overline{AB}$ with $AD&#x3D;4, DE&#x3D;16$, and $EB&#x3D;8$. Points $A,F,G$, and $C$ lie in that order on side $\overline{AC}$ with $AF&#x3D;13, FG&#x3D;52$, and $GC&#x3D;26$. Let $M$ be the reflection of $D$ through $F$, and let $N$ be the reflection of $G$ through $E$. Quadrilateral $DEGF$ has area 288. Find the area of heptagon $AFNBCEM$.<br>&emsp;&emsp;在三角形 ABC 上，点 A、D、E、B 按顺序位于边 AB 上，且 AD&#x3D;4，DE&#x3D;16，EB&#x3D;8。点 A、F、G、C 按顺序位于边 AC 上，且 AF&#x3D;13，FG&#x3D;52，GC&#x3D;26。设 M 为点 D 关于点 F 的反射，N 为点 G 关于点 E 的反射。四边形 DEGF 的面积为 288。求七边形 AFNBCEM 的面积。588<br><img src="/2025/12/19/LLM-%E4%B8%BB%E6%B5%81%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86%E8%B0%83%E7%A0%94/2025-12-19-23-37-18.png"></li></ul><h3 id="SWE-bench"><a href="#SWE-bench" class="headerlink" title="SWE-bench"></a>SWE-bench</h3><p>dataset: <a href="https://huggingface.co/datasets/SWE-bench/SWE-bench">https://huggingface.co/datasets/SWE-bench/SWE-bench</a></p><p>&emsp;&emsp;该数据集来自于12 个主流 Python 仓库爬取拉取请求和问题，收集了 2,294 个任务实例。每个实例基于一个拉取请求，（1）与某个问题相关，（2）修改的 1+测试相关文件。每个实例，构建一个执行环境（Docker Image），仓库在拉取请求所基于的提交处成功安装。没有拉取请求的更改，许多测试失败。拉取请求合并后，同样的测试集通过。这些“失败通过”测试是评估的主要信号。</p><p>&emsp;&emsp;SWE-bench 评估的工作原理如下。每个任务实例，给 AI 系统一个问题文本。随后 AI 系统应修改代码库以解决描述的问题。当 AI 系统完成后，我们运行上述失败-通过测试，以检查问题是否成功解决。<br><img src="/2025/12/19/LLM-%E4%B8%BB%E6%B5%81%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86%E8%B0%83%E7%A0%94/2025-12-19-23-39-37.png"></p><h3 id="Humanity’s-Last-Exam"><a href="#Humanity’s-Last-Exam" class="headerlink" title="Humanity’s Last Exam"></a>Humanity’s Last Exam</h3><p>dataset: <a href="https://huggingface.co/datasets/cais/hle">https://huggingface.co/datasets/cais/hle</a></p><p>&emsp;&emsp;人类最后考试（HLE）是一个位于人类知识前沿的多模态基准，旨在成为同类中最后一个封闭式学术基准，涵盖广泛学科。人类最后考试包含 2500 道题目，涵盖数十个学科，包括数学、人文学科和自然科学。HLE 由全球各学科专家开发，包含适合自动评分的选择题和简答题。他的题目分布如下：<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-41-04.png"></p><ul><li>case<br><img src="/2025/12/19/LLM-%E4%B8%BB%E6%B5%81%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86%E8%B0%83%E7%A0%94/2025-12-19-23-42-48.png"></li></ul><h3 id="MMLU"><a href="#MMLU" class="headerlink" title="MMLU"></a>MMLU</h3><p>dataset: <a href="https://huggingface.co/datasets/cais/mmlu">https://huggingface.co/datasets/cais/mmlu</a></p><p>&emsp;&emsp;这是一个庞大的多任务测试，包含来自不同知识领域的多项选择题。该考试涵盖人文学科、社会科学、理科以及其他对某些人来说重要的领域。课程涵盖57项任务，包括基础数学、美国历史、计算机科学、法律等, 一共116K道题目。为了在该测试中获得高准确性，模型必须具备广泛的世界知识和解决问题的能力。</p><ul><li>case:<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-43-59.png"></li></ul><h3 id="Math500"><a href="#Math500" class="headerlink" title="Math500"></a>Math500</h3><p>dataset: <a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500">https://huggingface.co/datasets/HuggingFaceH4/MATH-500</a></p><p>&emsp;&emsp;MATH-500基准测试由OpenAI于2023年推出，作为评估其最新模型（如GPT-4o）数学能力的工具。该基准测试包含500道高难度的数学竞赛题目，旨在挑战模型的极限，评估其在复杂数学问题上的推理和解题能力。</p><ul><li>case:<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-44-25.png"></li></ul><h3 id="IFBench"><a href="#IFBench" class="headerlink" title="IFBench"></a>IFBench</h3><p>dataset: <a href="https://huggingface.co/datasets/allenai/IF_multi_constraints_upto5">https://huggingface.co/datasets/allenai/IF_multi_constraints_upto5</a></p><p>&emsp;&emsp;大语言模型的“指令跟随”能力，是评价模型是否能够按照人类需求输出的重要指标。与一般的问答不同，指令往往包含多种约束，例如输出字数、格式、语言、是否禁止某些词、是否必须包含特定结构等。IFBench就是通过这些约束，自动化评估LLM指令遵循能力的Benchmark。</p><ul><li>case<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-46-46.png"></li></ul><h3 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h3><p>dataset: <a href="https://huggingface.co/datasets/nyu-mll/glue">https://huggingface.co/datasets/nyu-mll/glue</a></p><p>&emsp;&emsp;GLUE, the General Language Understanding Evaluation benchmark，是一套用于训练、评估和分析自然语言理解系统的资源集合。</p><ul><li>case<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-52-40.png"></li></ul><h3 id="GSM8K"><a href="#GSM8K" class="headerlink" title="GSM8K"></a>GSM8K</h3><p>dataset: <a href="https://huggingface.co/datasets/openai/gsm8k">https://huggingface.co/datasets/openai/gsm8k</a></p><p>&emsp;&emsp;GSM8K（Grade School Math 8K）是一个包含 8500 道高质量、语言多样化的小学数学应用题的数据集。该数据集旨在支持对需要多步骤推理的基础数学问题进行问答的任务。</p><p>&emsp;&emsp;这些问题需要 2 到 8 个步骤才能解决。解决方案主要涉及使用基本算术运算（+ − ×÷）进行一系列初等计算，以得出最终答案。一个聪明的中学生应该能够解决所有问题：试卷中写道，“这些问题不需要超出初级代数水平的概念，而且绝大多数问题无需明确定义变量即可解决。”解决方案以自然语言而非纯数学表达式的形式提供。论文中写道：“我们认为这是最通用的数据格式，并期望它能揭示大型语言模型内部独白的特性。”</p><ul><li>case<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-55-08.png"></li></ul><h3 id="HumanEval"><a href="#HumanEval" class="headerlink" title="HumanEval"></a>HumanEval</h3><p>dataset: <a href="https://huggingface.co/datasets/openai/openai_humaneval">https://huggingface.co/datasets/openai/openai_humaneval</a></p><p>&emsp;&emsp;OpenAI 发布的 HumanEval 数据集包含 164 个编程问题，每个问题都包含函数签名、特性、文档字符串、函数体以及若干单元测试。LLM需要根据问题编写代码，并通过单元测试。</p><ul><li>case<br><img src="D:\MyDeskTop\合订本材料\2025-12-19-23-56-32.png"></li></ul>]]></content>
    
    
    <categories>
      
      <category>benchmark</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
    <link href="/2025/12/17/Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model/"/>
    <url>/2025/12/17/Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model/</url>
    
    <content type="html"><![CDATA[<h2 id="阅读总结"><a href="#阅读总结" class="headerlink" title="阅读总结"></a>阅读总结</h2><p>&emsp;&emsp;本文深入分析了RLVR(LLM训练中使用的RL算法)对模型表现的真实作用，指出<strong>RL虽然提升了模型的表现，但是没有提高模型的upper bound</strong>。本文重点贡献在于做了非常详实的实验证明这一论点。实验包括：</p><ul><li>统计LLM在pass@k的情况下的表现，用数据论证</li><li>使用不同的强化学习算法训练base model对比性能</li></ul><h2 id="把avg-k-变成pass-k"><a href="#把avg-k-变成pass-k" class="headerlink" title="把avg@k 变成pass@k"></a>把avg@k 变成pass@k</h2><p>&emsp;&emsp;过去的测试LLM performance的方式：让LLM跑1000道题目，输出正确就记1分，计算正确率。本文为了测试LLM能力的边界，采用pass@k的测试方法：模型做K次，只要有1次做对就算对吗，这代表了模型能力的边界。<br>&emsp;&emsp;把测试改成pass@k之后发现，LLM在k值比较大的时候，表现反而不如base model。进一步在各个模型(Qwen LLaMa)和各个测试的benchmark(AIME24 MATH500..)上实验，也发现类似的现象。<br><img src="./2025-12-17-22-42-47.png"><br><img src="./2025-12-17-22-43-34.png"><br>&emsp;&emsp;对base model和经过rl的model回答问题的情况做分类，分为4类：base和rl都能做对&#x2F;base能做对rl做不对&#x2F;base做不对rl做对&#x2F;base rl都做不对。发现base能做对的rl做不对，强化学习反而降低了模型的表现。<br><img src="./2025-12-17-22-45-36.png"></p><h2 id="使用不同的强化学习算法训练"><a href="#使用不同的强化学习算法训练" class="headerlink" title="使用不同的强化学习算法训练"></a>使用不同的强化学习算法训练</h2><p>&emsp;&emsp;发现RL算法只能提高k比较小的情况下的表现，随着k的增大，LLM的表现有一个upper bound，而且这个upper bound和base model是一样的。用人话说就是：RL的过程没有提高LLM的upper bound，只是让LLM能够尽可能少的K次尝试，就能回答正确。(这个发现说明：RL确实能够提升模型的表现，只是提升不了模型的upper bound)。<br><img src="./2025-12-17-22-59-17.png"></p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>&emsp;&emsp;在RL领域中，AlphaGO非常出名。作为一个使用纯强化学习训练的围棋AI，AlphaGO能够下出来人类棋手意料之外的位置。这表明AlphaGO通过强化学习拓宽了现有围棋知识的边界。<br>&emsp;&emsp;为什么RLVR不能够拓宽LLM能力的边界呢？这是因为RLVR并不等于RL，相比于RL他有一些缺陷：</p><ul><li>搜索空间很大: 相比于围棋的搜索空间，语言的搜索空间很大，模型随机采样是大概率得不到正确答案，也没有reward。这就是为什么现有的RL都依赖于一个比较强大的Pre-Train model。因为需要base model来引导，才能在广大的空间里面，输出一个正确的答案。</li><li>sparse reward: LLM只有输出正确答案的时候才有reward，其他情况都是0。假设模型在RL过程中通过探索，生成了一个answer。这个answer大概率是没有reward的，这种探索边界的行为大概率会被抑制。<br><img src="./2025-12-17-23-06-43.png"><br><img src="./2025-12-17-23-06-59.png"></li></ul>]]></content>
    
    
    <categories>
      
      <category>NeurIPS2025</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>一些工作感想</title>
    <link href="/2025/12/16/%E4%B8%80%E4%BA%9B%E5%B7%A5%E4%BD%9C%E6%84%9F%E6%83%B3/"/>
    <url>/2025/12/16/%E4%B8%80%E4%BA%9B%E5%B7%A5%E4%BD%9C%E6%84%9F%E6%83%B3/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>混合分布的概述</title>
    <link href="/2025/12/16/%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%83%E7%9A%84%E6%A6%82%E8%BF%B0/"/>
    <url>/2025/12/16/%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%83%E7%9A%84%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="我的评价"><a href="#我的评价" class="headerlink" title="我的评价"></a>我的评价</h2><p>&emsp;&emsp;有时候我都在想，这么抽象的方法真的有人用吗？？</p><h2 id="什么是混合模型？"><a href="#什么是混合模型？" class="headerlink" title="什么是混合模型？"></a>什么是混合模型？</h2><p>&emsp;&emsp;混合模型指的是，你的数据的分布，可能是来源于其他的分布按照一定的比例混合而成的。说人话，就是数据分布是由一组分布混合而成。如果你从里面采样一个数据，实际操作是先按照$\pi_k$采样一个分布，再从这个分布中采样数据。(数据只能属于一组分布之一)<br>$$p(x) &#x3D; \sum_{k&#x3D;1}^{K} \pi_k p(x | z&#x3D;k)$$<br>$K$ 是混合分量的数量。$\pi_k$ 是混合权重（或先验概率），$\sum_{k&#x3D;1}^{K} \pi_k &#x3D; 1$ 且 $\pi_k \ge 0$。$p(x | z&#x3D;k)$ 是第 $k$ 个混合分量的条件概率密度函数。$z$ 是隐变量 (Latent Variable)，表示数据点 $x$ 来源于哪个分量 ($z \in {1, 2, \dots, K}$)。</p><h2 id="Gaussian-Mixture-Model-GMM"><a href="#Gaussian-Mixture-Model-GMM" class="headerlink" title="Gaussian Mixture Model (GMM)"></a>Gaussian Mixture Model (GMM)</h2><p>&emsp;&emsp;顾名思义，在 GMM 中，每个分量 $k$ 的条件概率密度函数 $p(x | z&#x3D;k)$ 都是一个正态分布（根据你的data dimension可能是多维正态分布）。<br>$$p(\mathbf{x}) &#x3D; \sum_{k&#x3D;1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$<br>$\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$: 第 $k$ 个高斯分量的概率密度函数，由其均值向量 $\boldsymbol{\mu}_k$ 和协方差矩阵 $\boldsymbol{\Sigma}_k$ 决定。</p><p>这东西有什么用？GMM 能够近似任何形状的概率分布，学习数据的概率分布。比如我们可以做聚类，下面是一个例子。</p><p>假设我们从一个班级中随机抽取了 $N&#x3D;5$ 个人的身高数据（单位：厘米），我们认为这些数据可能来源于 $K&#x3D;2$ 个高斯分布的混合（例如，男生群体的身高分布和女生群体的身高分布）。</p><h4 id="1-数据集-一维"><a href="#1-数据集-一维" class="headerlink" title="1. 数据集 (一维)"></a>1. 数据集 (一维)</h4><table><thead><tr><th align="center">样本 $i$</th><th align="center">身高 $x_i$ (cm)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">165</td></tr><tr><td align="center">2</td><td align="center">170</td></tr><tr><td align="center">3</td><td align="center">175</td></tr><tr><td align="center">4</td><td align="center">185</td></tr><tr><td align="center">5</td><td align="center">190</td></tr></tbody></table><h4 id="2-模型设置与初始化"><a href="#2-模型设置与初始化" class="headerlink" title="2. 模型设置与初始化"></a>2. 模型设置与初始化</h4><ul><li><strong>模型:</strong> $K&#x3D;2$ 个高斯分量 (簇 1 和 簇 2)。</li><li><strong>模型参数:</strong> $\Theta &#x3D; { \pi_1, \mu_1, \sigma_1^2, \pi_2, \mu_2, \sigma_2^2 }$。</li></ul><p>我们进行<strong>随机初始化</strong>（第 0 步迭代）：</p><table><thead><tr><th align="center">参数</th><th align="center">值</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">$\pi_1$</td><td align="center">0.5</td><td align="center">属于簇 1 的先验概率</td></tr><tr><td align="center">$\pi_2$</td><td align="center">0.5</td><td align="center">属于簇 2 的先验概率</td></tr><tr><td align="center">$\mu_1$</td><td align="center">170</td><td align="center">簇 1 的初始均值</td></tr><tr><td align="center">$\sigma_1^2$</td><td align="center">25</td><td align="center">簇 1 的初始方差 ( $\sigma_1&#x3D;5$ )</td></tr><tr><td align="center">$\mu_2$</td><td align="center">185</td><td align="center">簇 2 的初始均值</td></tr><tr><td align="center">$\sigma_2^2$</td><td align="center">25</td><td align="center">簇 2 的初始方差 ( $\sigma_2&#x3D;5$ )</td></tr></tbody></table><h4 id="3-EM-算法迭代-第一轮"><a href="#3-EM-算法迭代-第一轮" class="headerlink" title="3. EM 算法迭代 (第一轮)"></a>3. EM 算法迭代 (第一轮)</h4><p>我们使用一维高斯分布的概率密度函数：<br>$$\mathcal{N}(x | \mu, \sigma^2) &#x3D; \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$</p><hr><p>我们要计算每个数据点 $x_i$ 属于每个簇 $k$ 的后验概率 $\gamma(z_{ik}) &#x3D; p(z&#x3D;k | x_i, \Theta^{\text{old}})$。</p><p><strong>以 $x_1 &#x3D; 165$ 为例计算：</strong></p><p><strong>1. 计算似然 $p(x_1 | z&#x3D;k)$：</strong></p><ul><li><p><strong>簇 1 似然 ($\mu_1&#x3D;170, \sigma_1^2&#x3D;25$):</strong><br>  $$p(165|z&#x3D;1) \propto e^{-\frac{(165-170)^2}{2 \times 25}} &#x3D; e^{-\frac{25}{50}} &#x3D; e^{-0.5} \approx \mathbf{0.6065}$$<br>  (注：为简化计算，我们忽略了常数项 $\frac{1}{\sqrt{2\pi\sigma^2}}$，因为它在下一步的比例计算中会抵消，或者您认为 $\sigma_k$ 相同，常数项也相同。)</p></li><li><p><strong>簇 2 似然 ($\mu_2&#x3D;185, \sigma_2^2&#x3D;25$):</strong><br>  $$p(165|z&#x3D;2) \propto e^{-\frac{(165-185)^2}{2 \times 25}} &#x3D; e^{-\frac{400}{50}} &#x3D; e^{-8} \approx \mathbf{0.000335}$$</p></li></ul><p><strong>2. 计算责任 $\gamma(z_{1k})$：</strong></p><ul><li><p><strong>分母:</strong> $M &#x3D; \pi_1 p(x_1|z&#x3D;1) + \pi_2 p(x_1|z&#x3D;2)$<br>  $$M &#x3D; 0.5 \times 0.6065 + 0.5 \times 0.000335 \approx 0.30325 + 0.0001675 &#x3D; \mathbf{0.3034}$$</p></li><li><p><strong>$\gamma(z_{11})$ (属于簇 1):</strong><br>  $$\frac{0.5 \times 0.6065}{0.3034} \approx \mathbf{0.9995}$$</p></li><li><p><strong>$\gamma(z_{12})$ (属于簇 2):</strong><br>  $$\frac{0.5 \times 0.000335}{0.3034} \approx \mathbf{0.0005}$$</p></li></ul><p><strong>责任矩阵 $\gamma$ (所有样本的计算结果):</strong></p><table><thead><tr><th align="center">样本 $i$</th><th align="center">$x_i$</th><th align="center">$\gamma(z_{i1})$ (簇 1 责任)</th><th align="center">$\gamma(z_{i2})$ (簇 2 责任)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">165</td><td align="center">0.9995</td><td align="center">0.0005</td></tr><tr><td align="center">2</td><td align="center">170</td><td align="center">0.9999</td><td align="center">0.0001</td></tr><tr><td align="center">3</td><td align="center">175</td><td align="center">0.9995</td><td align="center">0.0005</td></tr><tr><td align="center">4</td><td align="center">185</td><td align="center">0.0005</td><td align="center">0.9995</td></tr><tr><td align="center">5</td><td align="center">190</td><td align="center">0.0001</td><td align="center">0.9999</td></tr><tr><td align="center"><strong>总和 $N_k$</strong></td><td align="center"></td><td align="center"><strong>3.0095</strong></td><td align="center"><strong>1.9905</strong></td></tr></tbody></table><hr><h4 id="4-更新参数"><a href="#4-更新参数" class="headerlink" title="4.更新参数"></a>4.更新参数</h4><p>我们使用责任总和 $N_k$ 和加权数据来更新参数。</p><p><strong>1. 更新混合权重 $\pi_k$</strong></p><p>$$N_k &#x3D; \sum_{i&#x3D;1}^{5} \gamma(z_{ik})$$<br>$$\pi_k^{\text{new}} &#x3D; N_k &#x2F; N$$</p><ul><li>$\pi_1^{\text{new}} &#x3D; 3.0095 &#x2F; 5 \approx \mathbf{0.6019}$</li><li>$\pi_2^{\text{new}} &#x3D; 1.9905 &#x2F; 5 \approx \mathbf{0.3981}$</li></ul><p><strong>2. 更新均值 $\mu_k$</strong></p><p>$$\mu_k^{\text{new}} &#x3D; \frac{\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) x_i}{N_k}$$</p><ul><li><p><strong>簇 1 均值 $\mu_1^{\text{new}}$:</strong><br>  $$\frac{0.9995 \times 165 + 0.9999 \times 170 + 0.9995 \times 175 + \dots}{3.0095} \approx \frac{510.015}{3.0095} \approx \mathbf{169.49}$$</p></li><li><p><strong>簇 2 均值 $\mu_2^{\text{new}}$:</strong><br>  $$\frac{\dots + 0.0005 \times 185 + 0.9999 \times 190}{1.9905} \approx \frac{375.025}{1.9905} \approx \mathbf{188.47}$$</p></li></ul><p><strong>3. 更新方差 $\sigma_k^2$</strong></p><p>$$\sigma_k^{2, \text{new}} &#x3D; \frac{\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) (x_i - \mu_k^{\text{new}})^2}{N_k}$$</p><ul><li><p><strong>簇 1 方差 $\sigma_1^{2, \text{new}}$:</strong><br>  $$\frac{0.9995 \times (165 - 169.49)^2 + 0.9999 \times (170 - 169.49)^2 + \dots}{3.0095} \approx \mathbf{25.0}$$ (方差变化很小)</p></li><li><p><strong>簇 2 方差 $\sigma_2^{2, \text{new}}$:</strong><br>  $$\frac{\dots + 0.9999 \times (190 - 188.47)^2}{1.9905} \approx \mathbf{12.5}$$ (方差显著减小)</p></li></ul><h4 id="5-结论-一轮迭代后的结果"><a href="#5-结论-一轮迭代后的结果" class="headerlink" title="5. 结论 (一轮迭代后的结果)"></a>5. 结论 (一轮迭代后的结果)</h4><table><thead><tr><th align="center">参数</th><th align="center">迭代前 (初始)</th><th align="center">迭代后 (新值)</th><th align="center">变化</th></tr></thead><tbody><tr><td align="center">$\pi_1$</td><td align="center">0.5</td><td align="center"><strong>0.6019</strong></td><td align="center">$\uparrow$</td></tr><tr><td align="center">$\mu_1$</td><td align="center">170</td><td align="center"><strong>169.49</strong></td><td align="center">$\downarrow$</td></tr><tr><td align="center">$\sigma_1^2$</td><td align="center">25</td><td align="center"><strong>25.0</strong></td><td align="center">$\approx$</td></tr><tr><td align="center">$\pi_2$</td><td align="center">0.5</td><td align="center"><strong>0.3981</strong></td><td align="center">$\downarrow$</td></tr><tr><td align="center">$\mu_2$</td><td align="center">185</td><td align="center"><strong>188.47</strong></td><td align="center">$\uparrow$</td></tr><tr><td align="center">$\sigma_2^2$</td><td align="center">25</td><td align="center"><strong>12.5</strong></td><td align="center">$\downarrow$</td></tr></tbody></table><p><strong>解释：</strong></p><ol><li><strong>聚类效果明显:</strong> 在第一轮迭代中，模型已经将 ${165, 170, 175}$ 这三个点坚定地分配给了<strong>簇 1</strong> (责任接近 1)，将 ${185, 190}$ 分配给了<strong>簇 2</strong> (责任接近 1)。</li><li><strong>参数更新合理:</strong><ul><li>簇 1 的均值从 170 调整到 169.49，更贴近它所负责的三个点 (165, 170, 175) 的平均值。</li><li>簇 2 的均值从 185 调整到 188.47，更贴近它所负责的两个点 (185, 190) 的平均值。</li></ul></li><li><strong>方差收缩:</strong> 簇 2 的方差 $\sigma_2^2$ 从 25 减小到 12.5，反映了点 185 和 190 之间的距离比初始设定的要紧凑。<br>如果继续迭代，参数将逐渐收敛到这两个“自然簇”的统计估计值。</li></ol><p>(这是真的麻烦 想不明白谁发明的..)</p><h2 id="Multinomial-Mixture-Model"><a href="#Multinomial-Mixture-Model" class="headerlink" title="Multinomial Mixture Model"></a>Multinomial Mixture Model</h2><p>&emsp;&emsp;多项混合分布，其实就是把分量的概率分布变为多项分布。什么是多项分布？就是用于描述有限次独立试验中，各种可能结果发生次数的一种离散概率分布。说人话：进行 $N$ 次独立的试验。每次试验的结果只能属于 $D$ 个互斥的类别 $C_1, C_2, \dots, C_D$ 中的一个。多项分布给出了在 $N$ 次试验中，类别 $C_1$ 发生 $x_1$ 次、类别 $C_2$ 发生 $x_2$ 次、…、类别 $C_D$ 发生 $x_D$ 次的概率 $P(\mathbf{x})$。$$P(\mathbf{x} | N, \boldsymbol{\theta}) &#x3D; \frac{N!}{x_1! x_2! \dots x_D!} \prod_{j&#x3D;1}^{D} \theta_j^{x_j}$$</p><p>$\frac{N!}{x_1! x_2! \dots x_D!}$其实就是组合数，总的N次实验方案（不考虑顺序）是${N!}$，但是我们不需要顺序，所以还要除一下消除。而$\theta_j$是单次实验$C_j$发生的概率。</p><p>假设我们有 $N&#x3D;4$ 篇短文档，只关注 $D&#x3D;3$ 个词汇（A, B, C）。我们希望将文档聚类成 $K&#x3D;2$ 个主题（主题 1 和 主题 2）。</p><h4 id="1-数据集-词频计数"><a href="#1-数据集-词频计数" class="headerlink" title="1. 数据集 (词频计数)"></a>1. 数据集 (词频计数)</h4><table><thead><tr><th align="center">文档 $i$</th><th align="center">$x_{i, \text{A}}$</th><th align="center">$x_{i, \text{B}}$</th><th align="center">$x_{i, \text{C}}$</th><th align="center">总词数 $N_i$</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">4</td><td align="center">1</td><td align="center">0</td><td align="center">5</td></tr><tr><td align="center">2</td><td align="center">1</td><td align="center">5</td><td align="center">0</td><td align="center">6</td></tr><tr><td align="center">3</td><td align="center">0</td><td align="center">2</td><td align="center">3</td><td align="center">5</td></tr><tr><td align="center">4</td><td align="center">0</td><td align="center">1</td><td align="center">4</td><td align="center">5</td></tr></tbody></table><h4 id="2-模型设置和初始化"><a href="#2-模型设置和初始化" class="headerlink" title="2. 模型设置和初始化"></a>2. 模型设置和初始化</h4><table><thead><tr><th align="center">参数</th><th align="center">值</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">$\pi_1$</td><td align="center">0.5</td><td align="center">文档属于主题 1 的先验概率</td></tr><tr><td align="center">$\pi_2$</td><td align="center">0.5</td><td align="center">文档属于主题 2 的先验概率</td></tr><tr><td align="center">$\theta_{1}$</td><td align="center">(0.8, 0.1, 0.1)</td><td align="center">主题 1 下 (A, B, C) 的生成概率</td></tr><tr><td align="center">$\theta_{2}$</td><td align="center">(0.1, 0.4, 0.5)</td><td align="center">主题 2 下 (A, B, C) 的生成概率</td></tr></tbody></table><hr><h4 id="3-EM-算法第一轮迭代"><a href="#3-EM-算法第一轮迭代" class="headerlink" title="3. EM 算法第一轮迭代"></a>3. EM 算法第一轮迭代</h4><h4 id="计算责任-gamma-z-ik"><a href="#计算责任-gamma-z-ik" class="headerlink" title="计算责任 $\gamma(z_{ik})$"></a>计算责任 $\gamma(z_{ik})$</h4><p>责任 $\gamma(z_{ik})$ 是文档 $i$ 属于主题 $k$ 的后验概率。</p><p>$$\gamma(z_{ik}) &#x3D; \frac{\pi_k L_k}{\sum_{l&#x3D;1}^{2} \pi_l L_l}$$</p><p>其中 $L_k &#x3D; \prod_{j&#x3D;\text{A}}^{\text{C}} \theta_{kj}^{x_{ij}}$ 是忽略多项系数的似然比例。</p><p><strong>以文档 2 ($\mathbf{x}_2&#x3D;(1, 5, 0)$, $N_2&#x3D;6$) 为例计算：</strong></p><ol><li><strong>主题 1 的似然比例 $L_1$:</strong><br>$$L_1 &#x3D; 0.8^1 \times 0.1^5 \times 0.1^0 &#x3D; 0.8 \times 0.00001 \times 1 &#x3D; \mathbf{0.000008}$$</li><li><strong>主题 2 的似然比例 $L_2$:</strong><br>$$L_2 &#x3D; 0.1^1 \times 0.4^5 \times 0.5^0 &#x3D; 0.1 \times 0.01024 \times 1 &#x3D; \mathbf{0.001024}$$</li><li><strong>计算 $\gamma(z_{21})$:</strong><br>$$\gamma(z_{21}) &#x3D; \frac{0.5 \times 0.000008}{0.5 \times 0.000008 + 0.5 \times 0.001024} &#x3D; \frac{0.000004}{0.000004 + 0.000512} \approx \mathbf{0.0078}$$</li><li><strong>计算 $\gamma(z_{22})$:</strong><br>$$\gamma(z_{22}) &#x3D; 1 - 0.0078 \approx \mathbf{0.9922}$$</li></ol><p><strong>完整的责任矩阵 $\gamma$</strong></p><table><thead><tr><th align="center">文档 $i$</th><th align="center">$N_i$</th><th align="center">$\gamma(z_{i1})$ (主题 1)</th><th align="center">$\gamma(z_{i2})$ (主题 2)</th></tr></thead><tbody><tr><td align="center">1 (4, 1, 0)</td><td align="center">5</td><td align="center">0.9999</td><td align="center">0.0001</td></tr><tr><td align="center">2 (1, 5, 0)</td><td align="center">6</td><td align="center">0.0078</td><td align="center">0.9922</td></tr><tr><td align="center">3 (0, 2, 3)</td><td align="center">5</td><td align="center">0.0000</td><td align="center">1.0000</td></tr><tr><td align="center">4 (0, 1, 4)</td><td align="center">5</td><td align="center">0.0000</td><td align="center">1.0000</td></tr><tr><td align="center"><strong>总和 $N_k$</strong></td><td align="center"></td><td align="center"><strong>1.0077</strong></td><td align="center"><strong>2.9923</strong></td></tr></tbody></table><h4 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h4><p><strong>1. 更新混合权重 $\pi_k$</strong></p><p>$$N_k &#x3D; \sum_{i&#x3D;1}^{N} \gamma(z_{ik}) \quad \pi_k^{\text{new}} &#x3D; N_k &#x2F; N$$</p><ul><li>$\pi_1^{\text{new}} &#x3D; 1.0077 &#x2F; 4 \approx \mathbf{0.2519}$</li><li>$\pi_2^{\text{new}} &#x3D; 2.9923 &#x2F; 4 \approx \mathbf{0.7481}$</li></ul><p><strong>2. 更新多项概率 $\theta_{kj}$</strong></p><p>$$\theta_{kj}^{\text{new}} &#x3D; \frac{\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) x_{ij}}{\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) N_i} &#x3D; \frac{\text{词 } j \text{ 的总加权计数}}{T_k (\text{总加权词数})}$$</p><p>首先计算<strong>总加权词数</strong> $T_k$:<br>$$T_k &#x3D; \sum_{i&#x3D;1}^{4} \gamma(z_{ik}) N_i$$</p><ul><li>$T_1 &#x3D; 0.9999 \times 5 + 0.0078 \times 6 + 0.0000 \times 5 + 0.0000 \times 5 \approx \mathbf{5.0468}$</li><li>$T_2 &#x3D; 0.0001 \times 5 + 0.9922 \times 6 + 1.0000 \times 5 + 1.0000 \times 5 \approx \mathbf{15.9538}$</li></ul><p><strong>计算词 A 的加权计数 $C_{k, \text{A}}$:</strong><br>$$C_{k, \text{A}} &#x3D; \sum_{i&#x3D;1}^{4} \gamma(z_{ik}) x_{i, \text{A}}$$</p><ul><li>$C_{1, \text{A}} &#x3D; 0.9999 \times 4 + 0.0078 \times 1 + 0 + 0 \approx \mathbf{4.0074}$</li><li>$C_{2, \text{A}} &#x3D; 0.0001 \times 4 + 0.9922 \times 1 + 0 + 0 \approx \mathbf{0.9926}$</li></ul><p><strong>更新 $\theta_{kj}$:</strong></p><table><thead><tr><th align="center">参数</th><th align="center">词 A ($j&#x3D;\text{A}$)</th><th align="center">词 B ($j&#x3D;\text{B}$)</th><th align="center">词 C ($j&#x3D;\text{C}$)</th></tr></thead><tbody><tr><td align="center"><strong>主题 1</strong></td><td align="center">$\theta_{1, \text{A}}^{\text{new}} &#x3D; 4.0074 &#x2F; 5.0468 \approx \mathbf{0.794}$</td><td align="center">$\theta_{1, \text{B}}^{\text{new}} &#x3D; 1.0078 &#x2F; 5.0468 \approx \mathbf{0.200}$</td><td align="center">$\theta_{1, \text{C}}^{\text{new}} &#x3D; 0 &#x2F; 5.0468 \approx \mathbf{0.000}$</td></tr><tr><td align="center"><strong>主题 2</strong></td><td align="center">$\theta_{2, \text{A}}^{\text{new}} &#x3D; 0.9926 &#x2F; 15.9538 \approx \mathbf{0.062}$</td><td align="center">$\theta_{2, \text{B}}^{\text{new}} &#x3D; 8.0156 &#x2F; 15.9538 \approx \mathbf{0.502}$</td><td align="center">$\theta_{2, \text{C}}^{\text{new}} &#x3D; 6.9456 &#x2F; 15.9538 \approx \mathbf{0.436}$</td></tr></tbody></table><h3 id="4-结论-第一轮迭代后的结果"><a href="#4-结论-第一轮迭代后的结果" class="headerlink" title="4. 结论 (第一轮迭代后的结果)"></a>4. 结论 (第一轮迭代后的结果)</h3><p>经过一轮 EM 迭代，模型参数发生了显著变化：</p><ol><li><strong>主题 1 ( $\pi_1&#x3D;0.25$ ):</strong> 权重下降。它的核心是<strong>词 A (0.794)</strong>。</li><li><strong>主题 2 ( $\pi_2&#x3D;0.75$ ):</strong> 权重上升。它的核心是<strong>词 B (0.502) 和 词 C (0.436)</strong>。</li><li><strong>聚类强化:</strong> E 步的责任显示，模型已经明确地将<strong>文档 1</strong> 分配给主题 1，将<strong>文档 2, 3, 4</strong> 分配给主题 2。</li></ol><p>这个例子清晰地展示了 EM 算法如何通过<strong>责任</strong> ($\gamma$) 来软分配数据点，并使用这些分配的权重来重新计算每个主题的<strong>生成概率</strong> ($\theta$)，从而迭代优化主题模型。</p><h2 id="Bernoulli-Mixture-Model"><a href="#Bernoulli-Mixture-Model" class="headerlink" title="Bernoulli Mixture Model"></a>Bernoulli Mixture Model</h2><p>&emsp;&emsp;BMM 适用于建模 $D$ 维的二元特征向量 $\mathbf{x} &#x3D; (x_1, x_2, \dots, x_D)$，其中每个特征 $x_j \in {0, 1}$。应用场景: 用户是否点击了某个广告 (1&#x2F;0)、一个基因是否表达 (1&#x2F;0)、文档中某个词汇是否出现 (1&#x2F;0)。</p><p>每个分量 $k$ 的条件概率 $p(\mathbf{x} | z&#x3D;k)$ 是 $D$ 个伯努利分布的乘积。说人话就是：每个分量都是D维数据，而且每一个feature都是伯努利分布。<br>$$p(\mathbf{x} | z&#x3D;k) &#x3D; \prod_{j&#x3D;1}^{D} p(x_j | z&#x3D;k) &#x3D; \prod_{j&#x3D;1}^{D} \theta_{kj}^{x_j} (1 - \theta_{kj})^{1 - x_j}$$<br>整体的分布就是要乘上一个$\pi_k$ 求和<br>$$p(\mathbf{x}) &#x3D; \sum_{k&#x3D;1}^{K} \pi_k p(\mathbf{x} | z&#x3D;k) &#x3D; \sum_{k&#x3D;1}^{K} \pi_k \left[ \prod_{j&#x3D;1}^{D} \theta_{kj}^{x_j} (1 - \theta_{kj})^{1 - x_j} \right]$$</p><p>$$\gamma(z_{ik}) &#x3D; p(z_i&#x3D;k | \mathbf{x}_i, \Theta^{\text{old}}) &#x3D; \frac{p(\mathbf{x}<em>i | z_i&#x3D;k, \Theta^{\text{old}}) p(z_i&#x3D;k)}{\sum</em>{l&#x3D;1}^{K} p(\mathbf{x}_i | z_i&#x3D;l, \Theta^{\text{old}}) p(z_i&#x3D;l)}$$</p><p>$$\gamma(z_{ik}) &#x3D; \frac{\pi_k^{\text{old}} \left[ \prod_{j&#x3D;1}^{D} (\theta_{kj}^{\text{old}})^{x_{ij}} (1 - \theta_{kj}^{\text{old}})^{1 - x_{ij}} \right]}{\sum_{l&#x3D;1}^{K} \pi_l^{\text{old}} \left[ \prod_{j&#x3D;1}^{D} (\theta_{lj}^{\text{old}})^{x_{ij}} (1 - \theta_{lj}^{\text{old}})^{1 - x_{ij}} \right]}$$</p><p>$$N_k &#x3D; \sum_{i&#x3D;1}^{N} \gamma(z_{ik})$$</p><p>$$\pi_k^{\text{new}} &#x3D; \frac{N_k}{N} &#x3D; \frac{\sum_{i&#x3D;1}^{N} \gamma(z_{ik})}{N}$$</p><p>$$\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) x_{ij}$$</p><p>$$\theta_{kj}^{\text{new}} &#x3D; \frac{\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) x_{ij}}{N_k}$$</p><p>假设我们有 $N&#x3D;5$ 位客户，记录了他们在 $D&#x3D;3$ 种行为上的二元（$0&#x2F;1$）数据。目标是将他们聚类成 $K&#x3D;2$ 个客户群体（簇 1 和 簇 2）。</p><h4 id="1-数据集-二元数据-mathbf-x-i"><a href="#1-数据集-二元数据-mathbf-x-i" class="headerlink" title="1. 数据集 (二元数据 $\mathbf{x}_i$)"></a>1. 数据集 (二元数据 $\mathbf{x}_i$)</h4><table><thead><tr><th align="center">客户 $i$</th><th align="center">$x_{i1}$ (浏览商品)</th><th align="center">$x_{i2}$ (加入购物车)</th><th align="center">$x_{i3}$ (最终购买)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">1</td><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="center">4</td><td align="center">0</td><td align="center">0</td><td align="center">1</td></tr><tr><td align="center">5</td><td align="center">1</td><td align="center">1</td><td align="center">0</td></tr></tbody></table><h4 id="2-模型设置和初始化-第-0-轮迭代"><a href="#2-模型设置和初始化-第-0-轮迭代" class="headerlink" title="2. 模型设置和初始化 (第 0 轮迭代)"></a>2. 模型设置和初始化 (第 0 轮迭代)</h4><p>我们初始化 $K&#x3D;2$ 个簇的参数 $\Theta &#x3D; { \pi_k, \theta_k }_{k&#x3D;1}^{2}$：</p><table><thead><tr><th align="center">参数</th><th align="center">值</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">$\pi_1$</td><td align="center">0.5</td><td align="center">客户属于簇 1 的先验概率</td></tr><tr><td align="center">$\pi_2$</td><td align="center">0.5</td><td align="center">客户属于簇 2 的先验概率</td></tr><tr><td align="center">$\theta_{1}$</td><td align="center">(0.8, 0.4, 0.1)</td><td align="center">簇 1 中 3 种行为发生的概率</td></tr><tr><td align="center">$\theta_{2}$</td><td align="center">(0.2, 0.6, 0.9)</td><td align="center">簇 2 中 3 种行为发生的概率</td></tr></tbody></table><hr><h4 id="3-EM-算法第一轮迭代-1"><a href="#3-EM-算法第一轮迭代-1" class="headerlink" title="3. EM 算法第一轮迭代"></a>3. EM 算法第一轮迭代</h4><h4 id="A-E-步-期望步骤：计算责任-gamma-z-ik"><a href="#A-E-步-期望步骤：计算责任-gamma-z-ik" class="headerlink" title="A. E 步 (期望步骤：计算责任 $\gamma(z_{ik})$)"></a>A. E 步 (期望步骤：计算责任 $\gamma(z_{ik})$)</h4><p>责任是 $\mathbf{x}<em>i$ 属于 $k$ 簇的后验概率 $\gamma(z</em>{ik}) &#x3D; p(z&#x3D;k | \mathbf{x}_i)$。</p><p><strong>以客户 1 ($\mathbf{x}_1&#x3D;(1, 1, 1)$) 为例计算：</strong></p><ol><li><p><strong>簇 1 的似然 $p(\mathbf{x}_1 | z&#x3D;1)$:</strong><br>$$0.8^{1} (1-0.8)^{0} \times 0.4^{1} (1-0.4)^{0} \times 0.1^{1} (1-0.1)^{0} &#x3D; 0.8 \times 0.4 \times 0.1 &#x3D; \mathbf{0.032}$$</p></li><li><p><strong>簇 2 的似然 $p(\mathbf{x}_1 | z&#x3D;2)$:</strong><br>$$0.2^{1} (1-0.2)^{0} \times 0.6^{1} (1-0.6)^{0} \times 0.9^{1} (1-0.9)^{0} &#x3D; 0.2 \times 0.6 \times 0.9 &#x3D; \mathbf{0.108}$$</p></li><li><p><strong>计算责任 $\gamma(z_{11})$ (客户 1 属于簇 1 的概率):</strong><br>$$\gamma(z_{11}) &#x3D; \frac{0.5 \times 0.032}{0.5 \times 0.032 + 0.5 \times 0.108} &#x3D; \frac{0.016}{0.070} \approx \mathbf{0.2286}$$</p></li><li><p><strong>计算责任 $\gamma(z_{12})$ (客户 1 属于簇 2 的概率):</strong><br>$$\gamma(z_{12}) &#x3D; 1 - 0.2286 \approx \mathbf{0.7714}$$</p></li></ol><p><strong>完整的责任矩阵 $\gamma$ (所有客户的计算结果):</strong></p><table><thead><tr><th align="center">客户 $i$</th><th align="center">$\mathbf{x}_i$</th><th align="center">$\gamma(z_{i1})$ (属于簇 1)</th><th align="center">$\gamma(z_{i2})$ (属于簇 2)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">(1, 1, 1)</td><td align="center">0.2286</td><td align="center">0.7714</td></tr><tr><td align="center">2</td><td align="center">(1, 0, 0)</td><td align="center">0.9855</td><td align="center">0.0145</td></tr><tr><td align="center">3</td><td align="center">(0, 1, 0)</td><td align="center">0.1379</td><td align="center">0.8621</td></tr><tr><td align="center">4</td><td align="center">(0, 0, 1)</td><td align="center">0.0033</td><td align="center">0.9967</td></tr><tr><td align="center">5</td><td align="center">(1, 1, 0)</td><td align="center">0.8351</td><td align="center">0.1649</td></tr><tr><td align="center"><strong>总和 $N_k$</strong></td><td align="center"></td><td align="center"><strong>2.1904</strong></td><td align="center"><strong>2.8096</strong></td></tr></tbody></table><hr><h4 id="更新参数-1"><a href="#更新参数-1" class="headerlink" title="更新参数"></a>更新参数</h4><p><strong>1. 更新混合权重 $\pi_k$</strong></p><p>$$\pi_k^{\text{new}} &#x3D; N_k &#x2F; N$$</p><ul><li>$\pi_1^{\text{new}} &#x3D; 2.1904 &#x2F; 5 \approx \mathbf{0.4381}$</li><li>$\pi_2^{\text{new}} &#x3D; 2.8096 &#x2F; 5 \approx \mathbf{0.5619}$</li></ul><p><strong>2. 更新伯努利参数 $\theta_{kj}$</strong></p><p>$$\theta_{kj}^{\text{new}} &#x3D; \frac{\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) x_{ij}}{N_k}$$</p><p>计算分子 $\sum_{i&#x3D;1}^{N} \gamma(z_{ik}) x_{ij}$ (特征 $j$ 的加权计数，只需对 $x_{ij}&#x3D;1$ 的客户进行求和)：</p><table><thead><tr><th align="center">特征 $j$</th><th align="center"><strong>分子 $C_{1j}$ (簇 1 加权计数)</strong></th><th align="center"><strong>分子 $C_{2j}$ (簇 2 加权计数)</strong></th></tr></thead><tbody><tr><td align="center"><strong>$x_{j&#x3D;1}$ (浏览)</strong></td><td align="center">$\gamma(z_{11})+\gamma(z_{21})+\gamma(z_{51}) \approx \mathbf{2.0492}$</td><td align="center">$\gamma(z_{12})+\gamma(z_{22})+\gamma(z_{52}) \approx \mathbf{0.9508}$</td></tr><tr><td align="center"><strong>$x_{j&#x3D;2}$ (购物车)</strong></td><td align="center">$\gamma(z_{11})+\gamma(z_{31})+\gamma(z_{51}) \approx \mathbf{1.2016}$</td><td align="center">$\gamma(z_{12})+\gamma(z_{32})+\gamma(z_{52}) \approx \mathbf{1.7984}$</td></tr><tr><td align="center"><strong>$x_{j&#x3D;3}$ (购买)</strong></td><td align="center">$\gamma(z_{11})+\gamma(z_{41}) \approx \mathbf{0.2319}$</td><td align="center">$\gamma(z_{12})+\gamma(z_{42}) \approx \mathbf{1.7681}$</td></tr></tbody></table><p>最后，除以各自的 $N_k$：</p><table><thead><tr><th align="center">簇 $k$</th><th align="center">$x_1$ (浏览)</th><th align="center">$x_2$ (购物车)</th><th align="center">$x_3$ (购买)</th></tr></thead><tbody><tr><td align="center"><strong>$\theta_{1}^{\text{new}}$</strong></td><td align="center">$2.0492 &#x2F; 2.1904 \approx \mathbf{0.936}$</td><td align="center">$1.2016 &#x2F; 2.1904 \approx \mathbf{0.548}$</td><td align="center">$0.2319 &#x2F; 2.1904 \approx \mathbf{0.106}$</td></tr><tr><td align="center"><strong>$\theta_{2}^{\text{new}}$</strong></td><td align="center">$0.9508 &#x2F; 2.8096 \approx \mathbf{0.339}$</td><td align="center">$1.7984 &#x2F; 2.8096 \approx \mathbf{0.640}$</td><td align="center">$1.7681 &#x2F; 2.8096 \approx \mathbf{0.629}$</td></tr></tbody></table><h4 id="4-结论-一轮迭代后的特征解读"><a href="#4-结论-一轮迭代后的特征解读" class="headerlink" title="4. 结论 (一轮迭代后的特征解读)"></a>4. 结论 (一轮迭代后的特征解读)</h4><p>迭代后的参数清晰地定义了两个不同的客户群体：</p><table><thead><tr><th align="center">簇 $k$</th><th align="center">$\pi_k$</th><th align="center">$\theta_{\text{浏览}}$</th><th align="center">$\theta_{\text{购物车}}$</th><th align="center">$\theta_{\text{购买}}$</th><th align="center"><strong>客户群体特征</strong></th></tr></thead><tbody><tr><td align="center"><strong>簇 1</strong></td><td align="center">$\approx 44%$</td><td align="center"><strong>高 (0.936)</strong></td><td align="center">中 (0.548)</td><td align="center">低 (0.106)</td><td align="center"><strong>“浏览型客户”</strong>: 非常活跃地浏览商品，但购买意愿&#x2F;转化率较低。</td></tr><tr><td align="center"><strong>簇 2</strong></td><td align="center">$\approx 56%$</td><td align="center">低 (0.339)</td><td align="center">中高 (0.640)</td><td align="center"><strong>高 (0.629)</strong></td><td align="center"><strong>“高转化客户”</strong>: 可能直接搜索目标商品，购买转化率极高。</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>course</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>DS杂谈25346期</title>
    <link href="/2025/12/15/DS%E6%9D%82%E8%B0%8825346%E6%9C%9F/"/>
    <url>/2025/12/15/DS%E6%9D%82%E8%B0%8825346%E6%9C%9F/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=ACT6oaPyO7o">link</a></p><p>（25346）林彪的头号爱将刘亚楼专门干丈母娘的事被周恩来知道了！柯庆施和刘亚楼的地位和作用是无可替代的！</p>]]></content>
    
    
    <categories>
      
      <category>history</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>图像/音频生成的套路概述</title>
    <link href="/2025/12/15/%E5%9B%BE%E5%83%8F-%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E7%9A%84%E7%AD%96%E7%95%A5%E6%A6%82%E8%BF%B0/"/>
    <url>/2025/12/15/%E5%9B%BE%E5%83%8F-%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E7%9A%84%E7%AD%96%E7%95%A5%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p><a href="https://speech.ee.ntu.edu.tw/~hylee/GenAI-ML/2025-fall-course-data/Generation.pdf">PPT link</a></p><h2 id="自回归的生成套路"><a href="#自回归的生成套路" class="headerlink" title="自回归的生成套路"></a>自回归的生成套路</h2><p>&emsp;&emsp;Image&#x2F;Sound的生成可以借鉴LLM的套路，变为Next token prediction。我们只需要预测下一个token是什么，然后拼在一起就构成了输出。这里有一个问题：LLM的token是一个个的词组，那Image&#x2F;Sound生成的token单元对应什么？</p><p>在实操中，我们可以认为Image就是由很多的token构成的一个矩阵。我们要训练的Image generation model的工作很简单：把Image tokenized &#x2F; 从token还原Image。<br><img src="./2025-12-15-21-17-32.png"><br><img src="./2025-12-15-21-19-49.png"></p><p>上面举得例子token呈现二维排列，其实我们可以直接把image token当成one dimension。这样的话就和LLM预测下一个token一样了。<br><img src="./2025-12-15-21-20-59.png"></p><p>继续深入，LLM生成文字天然有顺序，从左到右。但图像生成我们可不可以扩展一下，乱序生成呢？MaskGIT就是这样做的，每次随机mask一些token，让model尝试预测这些token。<br><img src="./2025-12-15-21-32-51.png"><br>在Inference的时候，如果token同时产生，因为不同token没有协调，可能会生成乱的图片（第一个token想生成动物的head 第3个也想 这样就乱了）。因此可以每次只保留K个token，运行多轮直到完全生成。<br><img src="./2025-12-15-21-34-20.png"><br><img src="./2025-12-15-21-36-09.png"><br>如下图，由于随机mask，生成的顺序是不固定的。<br><img src="./2025-12-15-21-36-23.png"></p><h2 id="Training中怎么评估图片的close"><a href="#Training中怎么评估图片的close" class="headerlink" title="Training中怎么评估图片的close?"></a>Training中怎么评估图片的close?</h2><p>&emsp;&emsp;上述的训练由tokenizer和detokenizer构成，需要一起训练让图片尽可能接近。那么如何评估图片&#x2F;音频之间的接近呢？</p><ol><li>绝对数值的接近<br>对音频来说，每个采样的音频点的绝对数值要接近，那就可以算一个Regression Loss。直接相减平方和；对图像来说，每个pixel要接近，也可以直接算平方和。<br><img src="./2025-12-15-21-26-33.png"></li><li>perceptual loss<br>可以让一个能够理解Image&#x2F;Sound的模型，输入两个数据，把模型的中间向量抽出来，然后比较两个向量的距离。(理论上来说，两个近似的图片，含义类似，在特征空间里的距离也应该比较接近  这种做法和Text Embedding差不多)<br><img src="./2025-12-15-21-28-40.png"></li><li>借鉴GAN的思路，让判别器判定<br><img src="./2025-12-15-21-30-50.png"></li></ol><h2 id="Token的极限"><a href="#Token的极限" class="headerlink" title="Token的极限"></a>Token的极限</h2><p>&emsp;&emsp;采用自回归的方式，predict next token是有一些极限的。token对信息的高度压缩会导致我们损失很多信息。（<strong>为什么语言模型就可以呢？我觉得language本身就是对信息的高度压缩，天然是一个个token。而且实操中，LLM也是先把token映射到Embedding dim然后再predict next token。显然这也是因为token损失了太多的信息，导致反而要先扩展再predict</strong>）</p><p>举个例子，16*16的pixel用8192个token代替，这里还原之后的效果很差。token限制了我们的信息压缩和解压。<br><img src="./2025-12-15-21-41-59.png"></p><p>我们可以考虑用continuous token(其实就是vector)来做图像生成。<br>但是这里会遇到一些问题，对于图像生成，我们不需要vector和目标vector完全一样，而是只要在一个分布就可以（奔跑的狗 可以在街上也可以在草地，都不是错误的target）。<br><img src="./2025-12-15-23-04-51.png"><br><img src="./2025-12-15-23-05-33.png"><br>这个问题在Language model里面有没有？没有。因为LLM采用的是离散的token，我们会根据概率分布采样一个token。而连续的token会产生两个target的mixture。</p><p>正因如此，产生了新的图像生成范式（VAE GAN Diffusion…）请看next charpter</p><h2 id="新的图像生成思路"><a href="#新的图像生成思路" class="headerlink" title="新的图像生成思路"></a>新的图像生成思路</h2><p>&emsp;&emsp;我们想要产生连续的token(vector)，可以考虑生成一个概率分布，然后再从这个概率分布中采样token。<br><img src="./2025-12-15-23-11-33.png"><br>一开始的研究认为，可以生成概率分布相关的参数，比如假设是一个多变量的Gauss distribution，我们可以预测均值和协方差。但是有个问题，就是没办法拟合一些复杂的分布（Gauss distribution怎么样都是椭圆）<br><img src="./2025-12-15-23-13-58.png"><br>后面的研究统称为”Generative Model”。他们的考虑是，既然没办法直接拟合分布，我可以拟合从一个分布到另一个分布的Transform Method。也就是说，我先从一个已知的分布中采样，然后想办法把他变换成目标分布。Model的作用就是实现这种变换。<br><img src="./2025-12-15-23-17-00.png"></p><h2 id="一些可以继续深入学习的参考资料"><a href="#一些可以继续深入学习的参考资料" class="headerlink" title="一些可以继续深入学习的参考资料"></a>一些可以继续深入学习的参考资料</h2><table><thead><tr><th align="left">主题</th><th align="left">标题</th><th align="left">URL</th><th align="left">课程&#x2F;年份</th></tr></thead><tbody><tr><td align="left"><strong>VAE (变分自编码器)</strong></td><td align="left">VAE &#x2F; Unsupervised Deep Learning - Deep Generative Models (Part I)</td><td align="left"><code>https://youtu.be/8zomhgKrsMQ</code></td><td align="left">2016 机器学习</td></tr><tr><td align="left"><strong>GAN (生成对抗网络)</strong></td><td align="left">Introduction of Generative Adversarial Network (GAN) &#x2F; 李宏毅</td><td align="left"><code>https://www.youtube.com/watch?v=DQNNMIAp5lw&amp;list=PLJV_eL3UvTsMq6JEFPW35BCiOQTsoQwNw</code></td><td align="left">2018 机器学习及其深层与结构化</td></tr><tr><td align="left"><strong>Normalizing Flow (归一化流)</strong></td><td align="left">Normalizing Flow &#x2F; Flow-based Generation Model &#x2F; Coupling Layer</td><td align="left"><code>https://youtu.be/uXY18nzdSsM</code></td><td align="left">2019 机器学习</td></tr><tr><td align="left"><strong>Diffusion Model (扩散模型)</strong></td><td align="left">Diffusion Model &#x2F; Denoising Diffusion Probabilistic Models (DDPM)</td><td align="left"><code>https://www.youtube.com/watch?v=a2BugJzmz-o&amp;list=PLJV_eL3UvTsNi7PgeKEUFSYVJIAJXRsP-</code></td><td align="left">2023 机器学习</td></tr><tr><td align="left"><strong>Flow matching</strong></td><td align="left"></td><td align="left"><a href="https://www.youtube.com/watch?v=ccqCDD9LqCA">https://www.youtube.com/watch?v=ccqCDD9LqCA</a></td><td align="left">生成式人工智慧與機器學習導論2025</td></tr></tbody></table>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>云服务器租赁和配置</title>
    <link href="/2025/12/15/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A7%9F%E8%B5%81%E5%92%8C%E9%85%8D%E7%BD%AE/"/>
    <url>/2025/12/15/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A7%9F%E8%B5%81%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.autodl.com/console/instance/list?tag_id=">link</a></p><h2 id="aliyun服务器定价">aliyun服务器定价</h2><p>一方面，卡的定价比较贵，24GB的A10要8.5元/h 而且还没有计算服务器的网费。另一方面显卡型号不全，便宜的卡如5090这种都没有。 <img src="./2025-12-17-10-45-30.png" /> <img src="./2025-12-17-10-46-02.png" /></p><h2 id="autodl服务器定价">AutoDL服务器定价</h2><p>价格便宜得多，而且支持0.1元/h的无GPU模式启动，这个模式下可以编写一些代码逻辑，很便宜。 <img src="./2025-12-17-10-51-11.png" /></p><h2 id="购买服务器并进行一些配置">购买服务器，并进行一些配置</h2><p>这里选择32GB的5090服务器，并进行一些LLM相关配置： - 选好cuda版本 12.8 - 下载对应版本的vllm 用于测试GPU - 配置镜像的链接（AutoDL是国内的服务器 需要配置一下镜像） - vllm下载Qwen3-8B 测试GPU</p><p><img src="./2025-12-15-13-35-31.png" /> <img src="./2025-12-15-13-37-26.png" /></p><p>配置好vllm (中间发现了一个<a href="https://github.com/vllm-project/vllm/issues/23517">bug</a>) 下载了Qwen3-8B 运行做测试，模型跑起来的GPU使用情况25GB/32GB <img src="./2025-12-15-13-39-39.png" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Tülu 3: Pushing Frontiers in Open Language Model Post-Training</title>
    <link href="/2025/12/14/Tulu-3-Pushing-Frontiers-in-Open-Language-Model-Post-Training/"/>
    <url>/2025/12/14/Tulu-3-Pushing-Frontiers-in-Open-Language-Model-Post-Training/</url>
    
    <content type="html"><![CDATA[<h2 id="我的评价"><a href="#我的评价" class="headerlink" title="我的评价"></a>我的评价</h2><p>&emsp;&emsp;</p><h2 id="当前存在什么问题"><a href="#当前存在什么问题" class="headerlink" title="当前存在什么问题"></a>当前存在什么问题</h2><p>&emsp;&emsp;</p><h2 id="本文打算通过什么思路解决"><a href="#本文打算通过什么思路解决" class="headerlink" title="本文打算通过什么思路解决"></a>本文打算通过什么思路解决</h2><p>&emsp;&emsp;</p><h2 id="该思路会遇到什么挑战"><a href="#该思路会遇到什么挑战" class="headerlink" title="该思路会遇到什么挑战"></a>该思路会遇到什么挑战</h2><p>&emsp;&emsp;</p><h2 id="本文通过什么手段克服该挑战"><a href="#本文通过什么手段克服该挑战" class="headerlink" title="本文通过什么手段克服该挑战"></a>本文通过什么手段克服该挑战</h2><p>&emsp;&emsp;</p><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><p>&emsp;&emsp;</p><h2 id="本文的写作思路"><a href="#本文的写作思路" class="headerlink" title="本文的写作思路"></a>本文的写作思路</h2><p>&emsp;&emsp;</p><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><p>&emsp;&emsp;</p>]]></content>
    
    
    <categories>
      
      <category>COLM2025</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Evals for Data Diversity</title>
    <link href="/2025/12/13/Evals-for-Data-Diversity/"/>
    <url>/2025/12/13/Evals-for-Data-Diversity/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM生成数据的多样性评估"><a href="#LLM生成数据的多样性评估" class="headerlink" title="LLM生成数据的多样性评估"></a>LLM生成数据的多样性评估</h1><p><a href="https://amitness.com/posts/diversity-evals">link</a></p><h2 id="引言：为什么合成数据需要多样性？"><a href="#引言：为什么合成数据需要多样性？" class="headerlink" title="引言：为什么合成数据需要多样性？"></a>引言：为什么合成数据需要多样性？</h2><p>使用大型语言模型（LLMs）生成合成数据已成为一种流行的方法。然而，一个常见的问题是，LLM直接生成的输出往往具有<strong>重复性</strong>。</p><p>为了提高生成数据的多样性，我们通常会采用一些技术，例如：</p><ul><li><strong>采样参数：</strong> 提高温度（temperature）、使用核采样（nucleus-sampling）或 Top-K 采样。</li><li><strong>属性生成：</strong> 预先生成各种属性（如主题、风格、长度、角色等），并将其随机插入到提示中。</li><li><strong>解码后聚类：</strong> 过量生成大量文本，然后通过聚类中心或语义哈希进行去重。</li></ul><p>这就引出了一个关键问题：<strong>我们如何系统地评估这些技术对多样性的影响</strong></p><p>本文将概述学术界已有的、用于衡量LLM生成文本<strong>Diversity</strong>的各种自动评估指标，涵盖词汇、语义和句法三个维度。</p><hr><h2 id="一、词汇多样性指标-Lexical-Diversity-Metrics"><a href="#一、词汇多样性指标-Lexical-Diversity-Metrics" class="headerlink" title="一、词汇多样性指标 (Lexical Diversity Metrics)"></a>一、词汇多样性指标 (Lexical Diversity Metrics)</h2><p>这类指标用于捕捉文本中词语、短语、主题和 N-gram 在表面上的重复程度。</p><h3 id="1-独特-N-gram-Distinct-k"><a href="#1-独特-N-gram-Distinct-k" class="headerlink" title="1. 独特 N-gram (Distinct-k)"></a>1. 独特 N-gram (Distinct-k)</h3><ul><li><strong>基本概念：</strong> 源自语言学中的“型符比”（type-token ratio）。</li><li><strong>计算方式：</strong> 计算生成的整个数据集中<strong>独特 N-gram 的数量</strong>与<strong>总 N-gram 的数量</strong>之比。</li></ul><p>例如，对于文本 “As an AI language model” 和 “As an AI model”：</p><ul><li>总 Unigram (1-gram) 数量：9</li><li>独特 Unigram 数量：5 (As, an, AI, language, model)</li><li><strong>多样性得分：</strong> $5 &#x2F; 9 \approx 0.55$</li></ul><p><img src="./2025-12-13-19-46-14.png"><br><img src="./2025-12-13-19-46-20.png"></p><p>这个概念可以扩展到 Bigram (k&#x3D;2)、Trigram (k&#x3D;3) 等，并可以分别组合成一个分数。</p><h3 id="2-N-gram-熵-Ent-n"><a href="#2-N-gram-熵-Ent-n" class="headerlink" title="2. N-gram 熵 (Ent-n)"></a>2. N-gram 熵 (Ent-n)</h3><ul><li><strong>基本概念：</strong> 在理想情况下，LLM 生成的所有文本都应该是独特的，任何 N-gram 都不会重复超过一次。</li><li><strong>计算方式：</strong> 通过收集文本中所有独特的 N-gram，计算它们的频率，从而得到 N-gram 的概率分布。然后，计算该概率分布的<strong>信息熵</strong>。</li><li><strong>判断标准：</strong> 分布越均匀（重复越少），熵值越高，多样性也越高。</li></ul><p><img src="./2025-12-13-19-46-58.png"></p><h3 id="3-压缩比-Compression-Ratio"><a href="#3-压缩比-Compression-Ratio" class="headerlink" title="3. 压缩比 (Compression Ratio)"></a>3. 压缩比 (Compression Ratio)</h3><ul><li><strong>基本概念：</strong> 借鉴用于评估压缩算法的压缩比概念。</li><li><strong>计算方式：</strong> 使用如 Gzip 等算法压缩文本，计算<strong>压缩文件大小</strong>与<strong>原始大小</strong>之比。</li><li><strong>判断标准：</strong><ul><li><strong>比率越高</strong>（如 16.258），表明文本<strong>可压缩性越高</strong>，冗余度高，因此<strong>多样性越低</strong>。</li><li>多样性可以计算为压缩比的<strong>倒数</strong>，从而得到一个 0 到 1 之间的分数。</li></ul></li></ul><h2 id=""><a href="#" class="headerlink" title=""></a><img src="./2025-12-13-19-48-01.png"></h2><h2 id="二、语义多样性指标-Semantic-Diversity-Metrics"><a href="#二、语义多样性指标-Semantic-Diversity-Metrics" class="headerlink" title="二、语义多样性指标 (Semantic Diversity Metrics)"></a>二、语义多样性指标 (Semantic Diversity Metrics)</h2><p>这类指标关注文本在<strong>语义</strong>上的多样性，并依赖于embeddings。它们可以处理词汇重叠为零但意义相似（例如，“Play the music” 和 “Start a song”）的情况。</p><h3 id="1-嵌入多样性-Embedding-Diversity"><a href="#1-嵌入多样性-Embedding-Diversity" class="headerlink" title="1. 嵌入多样性 (Embedding Diversity)"></a>1. 嵌入多样性 (Embedding Diversity)</h3><ul><li><p><strong>计算方式：</strong></p><ol><li>使用编码器（如 Sentence-Transformers）计算所有生成文本的<strong>Embedding vector</strong>。</li><li>计算所有独特文本对之间的<strong>余弦相似度（Cosine Similarity）</strong>。</li><li>取这些相似度的<strong>平均值</strong>。</li></ol></li><li><p><strong>判断标准：</strong> 将平均相似度转换为多样性得分（如：$1 - \text{平均余弦相似度}$），得分越高表示多样性越高。</p></li></ul><p><img src="./2025-12-13-19-48-18.png"><br><img src="./2025-12-13-19-48-24.png"></p><h3 id="2-DCScore"><a href="#2-DCScore" class="headerlink" title="2. DCScore"></a>2. DCScore</h3><ul><li><strong>计算方式：</strong><ol><li>计算文本嵌入的<strong>两两相似度矩阵</strong>(做法和1相同)。</li><li>对该矩阵应用 <strong>Softmax</strong> 函数。</li><li>计算 Softmax 矩阵<strong>对角线元素的平均值</strong>。</li></ol></li></ul><ul><li><strong>判断标准：</strong> 对角线元素表示文本相对于所有其他文本（包括自身）与自身的相似度“概率”。平均值越接近 1，表示文本与其自身的相似度远高于与其他任何文本的相似度，从而意味着整体数据集的多样性高。</li></ul><p><img src="./2025-12-13-19-48-34.png"><br><img src="./2025-12-13-19-48-39.png"></p><h3 id="3-聚类惯性-Cluster-Inertia"><a href="#3-聚类惯性-Cluster-Inertia" class="headerlink" title="3. 聚类惯性 (Cluster Inertia)"></a>3. 聚类惯性 (Cluster Inertia)</h3><ul><li><strong>基本概念：</strong> 重用聚类算法中用于衡量聚类质量的“惯性”指标。</li><li><strong>计算方式：</strong><ol><li>将文本嵌入聚类到 $K$ 个簇（如 $K&#x3D;10$）。</li><li>计算<strong>惯性</strong> (Inertia)：即簇内所有点到其**质心（centroid）**的平方距离之和。</li></ol></li><li><strong>判断标准：</strong> 如果文本多样，它们会离质心更远，导致<strong>惯性更大</strong>，因此惯性被视为多样性的一个代理指标。</li></ul><p><img src="./2025-12-13-19-48-47.png"></p><hr><h2 id="三、句法多样性指标-Syntactic-Diversity-Metrics"><a href="#三、句法多样性指标-Syntactic-Diversity-Metrics" class="headerlink" title="三、句法多样性指标 (Syntactic Diversity Metrics)"></a>三、句法多样性指标 (Syntactic Diversity Metrics)</h2><p>这类指标捕捉文本在<strong>底层语法结构</strong>上的多样性。</p><h3 id="1-压缩比-词性-CR-POS"><a href="#1-压缩比-词性-CR-POS" class="headerlink" title="1. 压缩比 - 词性 (CR-POS)"></a>1. 压缩比 - 词性 (CR-POS)</h3><ul><li><strong>基本概念：</strong> 重用“压缩比”的概念，但应用于文本的<strong>句法表示</strong>。</li><li><strong>计算方式：</strong><ol><li>使用词性标注器（POS tagger）将所有生成文本转换为其<strong>词性标签序列</strong>（即句法表示）。</li><li>将所有词性标签序列拼接成一个长字符串。</li><li>计算这个长字符串的<strong>压缩比</strong>。</li></ol></li><li><strong>判断标准：</strong> 压缩比越高，表示句法模板重复越多，<strong>多样性越低</strong>。多样性得分取压缩比的倒数。</li></ul><p><img src="./2025-12-13-19-48-57.png"><br><img src="./2025-12-13-19-49-04.png"><br><img src="./2025-12-13-19-49-10.png"></p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们探索了衡量LLM生成数据的语言多样性的三大类指标：</p><table><thead><tr><th align="left">类别</th><th align="left">关注点</th><th align="left">关键指标</th></tr></thead><tbody><tr><td align="left"><strong>词汇</strong></td><td align="left">表面词语、N-gram 的重复</td><td align="left">N-gram (Distinct-k), N-gram 熵, 压缩比</td></tr><tr><td align="left"><strong>语义</strong></td><td align="left">文本在意义上的不同</td><td align="left">嵌入多样性, DCScore, 聚类惯性</td></tr><tr><td align="left"><strong>句法</strong></td><td align="left">底层语法结构的重复</td><td align="left">压缩比 - 词性 (CR-POS)</td></tr></tbody></table><p>在实际应用中，这些自动指标快速且易于计算，可以作为评估LLM生成数据多样性的指标。</p>]]></content>
    
    
    <categories>
      
      <category>eval</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>VERBALIZED SAMPLING: HOW TO MITIGATE MODE COLLAPSE AND UNLOCK LLM DIVERSITY</title>
    <link href="/2025/12/11/VERBALIZED-SAMPLING-HOW-TO-MITIGATE-MODE-COLLAPSE-AND-UNLOCK-LLM-DIVERSITY/"/>
    <url>/2025/12/11/VERBALIZED-SAMPLING-HOW-TO-MITIGATE-MODE-COLLAPSE-AND-UNLOCK-LLM-DIVERSITY/</url>
    
    <content type="html"><![CDATA[<p><img src="./2025-12-11-22-43-23.png"></p><h2 id="我的评价"><a href="#我的评价" class="headerlink" title="我的评价"></a>我的评价</h2><p>&emsp;&emsp; 作者说LLM生成内容的多样性不好，是因为数据本身就带有人类的偏好。数据本身的偏好是模型多样性缺失的原因，是本文要证明的内容。按照这个分析，作者大概会构造一些偏好数据，然后训练模型，证明这种多样性确实源于偏好数据。</p><p>&emsp;&emsp; 作者改进模型数据多样性的方法，不是通过output的采样，而是使用Prompt，让模型输出多个回答并带有概率分布，这种方式很难理解为什么这么做。明明使用llm的参数temperature改变采样策略就可以做到，为什么还要这种Prompt呢？(作者标榜Prompt可以做到Training-Free 但是采样策略也是啊 而且更可控…)</p><p>&emsp;&emsp;总结来说，这篇文章有点像是独立的两篇文章。前半段对Training Data的bias的分析不够深入，只是针对一个数据集做了分析，通过拟合，证明了数据确实有bias。在我看来，还应该对bias data对模型的影响进行更加细致的分析（比如通过消除bias data中的bias，重新训练，看能否消除这种diversity的缺失）。后半段则是对VERBALIZED SAMPLING（一种带有概率的Prompt）来缓解模型diversity的降低。在我看来，后半段完全可以单独弄一个文章，分析temperature &#x2F; reward model &#x2F; training algforithm &#x2F; prompt等对模型输出diversity的影响，看看哪个更有效。</p><p>&emsp;&emsp;作为一篇ICLR 2026在投的文章，从<a href="https://openreview.net/forum?id=9jQkmGunGo">OpenReview</a>上面的评论，也可以看出和评价类似的Reviewer comment。不过这确实是一篇实验非常详实，消融实验设计也很合理的好文章。</p><h2 id="当前存在什么问题"><a href="#当前存在什么问题" class="headerlink" title="当前存在什么问题"></a>当前存在什么问题</h2><p>&emsp;&emsp;在创意性的问题，比如”tell me a joke” “write a story about bear”… 场景下，我们需要 LLM 给出尽可能 diverse 的 answer。但是前人研究发现，LLM 在训练的过程中，生成内容的 diversity 会降低。过去的研究集中在算法层面，认为 trainging algorithm 存在天然的缺陷，比如单一的奖励信号，RLHF 中的 KL 正则化项等等，他们天然的就会在训练的过程中降低 model output diversity。因此，解决的方案就是从算法角度下手，尽可能优化 sft rl 的过程。</p><p>本文想要从 Data 的角度解释这种 diversity 的缺失，同时提出一些提高 diversity 的方法。</p><h2 id="本文打算通过什么思路解决"><a href="#本文打算通过什么思路解决" class="headerlink" title="本文打算通过什么思路解决"></a>本文打算通过什么思路解决</h2><p>&emsp;&emsp;本文做出假设：模型的训练数据本身是带有 bias 的。人类在标注 RLHF 数据的时候，做排序天然的就有 bias。比如更喜欢比较熟悉的语法，熟悉的结构，简单易懂的呈现方式等等。本文通过分析一个 Nvidia 团队的数据集HELPSTEER，证明这种数据bias 的存在。这里通过理论推导和拟合，发现确实存在。</p><p>$$r(x, y) &#x3D; r_{\text{true}}(x, y) + \alpha \log \pi_{\text{ref}}(y | x) + \epsilon(x) \tag{1}$$</p><p>&emsp;&emsp;研究人员使用了 HELPSTEER 数据集。这个数据集非常特殊，因为它对同一个回答给了两个不同的打分：</p><ul><li>Correctness（正确性）： 对应公式中的 $r_{\text{true}}$（真实任务效用）。</li><li>Helpfulness（有用性）： 对应公式中的 $r$（最终奖励，即人类给出的总分）</li></ul><p>研究人员从数据集中筛选出了 6,874 对回答。这些配对有一个关键特征：“same correctness ratings” (拥有相同的正确性评分)这意味着对于每一对回答 $(y_1, y_2)$，它们的真实效用是相等的：$$r_{\text{true}}(y_1) &#x3D; r_{\text{true}}(y_2)$$</p><p>假设有两个回答 $A$ 和 $B$，它们的正确性评分一样，但人类给的总分（有用性）不一样。根据公式 (1)：回答 A 的得分： $r_A &#x3D; r_{\text{true}} + \alpha \cdot \text{Typicality}<em>A$回答 B 的得分： $r_B &#x3D; r</em>{\text{true}} + \alpha \cdot \text{Typicality}<em>B$当我们把这两个式子相减（比较 $A$ 和 $B$）：$$r_A - r_B &#x3D; (r</em>{\text{true}} - r_{\text{true}}) + \alpha (\text{Typicality}_A - \text{Typicality}<em>B)$$由于正确性相同，$r</em>{\text{true}}$ 互相抵消为 0，剩下的就是：$$\Delta \text{Reward} &#x3D; \alpha \times \Delta \text{Typicality}$$</p><h2 id="该思路会遇到什么挑战"><a href="#该思路会遇到什么挑战" class="headerlink" title="该思路会遇到什么挑战"></a>该思路会遇到什么挑战</h2><ul><li>如何证明 data bias 的存在。</li><li>找到一个新的方法，能够提高模型的 diversity</li></ul><h2 id="本文通过什么手段克服该挑战"><a href="#本文通过什么手段克服该挑战" class="headerlink" title="本文通过什么手段克服该挑战"></a>本文通过什么手段克服该挑战</h2><ul><li>本文通过理论推导 bias 公式，然后使用HELPSTEER 数据集做数据拟合的验证，表明这种数据的 bias 的存在。</li><li>前人的解决方案大多集中在 algorithm 或者 inference setting (sampling 策略)。本文提出带有概率输出的 prompt 来解决。如:“Generate 5 responses with their corresponding probabilities. Tell me a joke about coffee.”</li></ul><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><ul><li><p>实验目标：证明该 prompt 能够提高 LLM 输出的 diversity。</p></li><li><p>实验的 benchmark：诗歌续写，故事生成，讲笑话。</p></li><li><p>怎么 evaluate 模型的输出呢：<br>多样性：把模型的输出做 Embedding，通过相似度计算来反映输出的 diversity<br>输出质量：只输出多样的内容是不够的，我们还需要保证 quality。本文把 claude 模型 LLM as judge，从几个语言学上的维度，评估输出的质量。</p></li><li><p>实验设置：在下列模型上，不同的 method （看Prompt Example column）跑 benchmark，使用 evaluate 的方法，计算输出的多样性和质量，绘制图像。<br>Gemini-2.5-Flash, Gemini-2.5-Pro<br>Claude Series (Claude-3.7-Sonnet, Claude-4-Sonnet)<br>open ones like (Llama-3.1-70B-Instruct and Qwen3-235B-A22B-2507-Instruct-2507)<br>reasoning models like (OpenAI o3 and DeepSeek R1)<br><img src="./2025-12-12-22-38-08.png"><br>实验结果：VS（也就是本文提出的方法）在各个 benchmark 上的表现都更优<br><img src="./2025-12-12-22-38-22.png"><br><img src="./2025-12-12-22-38-27.png"></p></li><li><p>消融实验：</p></li></ul><ol><li>llm params: temparature<br>LLM 的 api 中可以调节 temperature 参数来调整模型输出的 diversity。因此需要消融 temperature。本文通过设置不同的 temperature 进行相同实验，消除该参数对方案有效性的影响。可以看到，VS 方案在各个 temperature 下的 diversity + quality 的表现依然是最佳的。<br><img src="./2025-12-12-22-39-21.png"></li><li>llm 不同的 traing stage<br>为了证明这种 VS 方法的有效性，本文在 LLM 不同的训练阶段 （SFT RLHF RLVR）都进行了 diversity 的测试，发现该方法在不同阶段都有效果。如图，随着训练的进行，Base Model 的 diversity 下降，但是 VS 方法的下降是最慢的，缓解了模型 diversity 的下降。<br><img src="./2025-12-12-22-40-09.png"></li></ol><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><p>&emsp;&emsp;我在gemini-2.5-flash模型上试了一下，“Write a story about a bear.” 会得到如下结果 确实会有可能得到相似的回答：<br><img src="./2025-12-11-22-52-38.png"><br><img src="./2025-12-11-22-52-49.png"></p><p>不过这里只是粗略的看，<strong>如何评估两段文字的语义相似性 也是一个值得研究的问题</strong></p><p>研究RLHF阶段的training data，分析到底什么样的数据会更优先，有没有搞头？可能需要分析排在前面的answer的通用范式（可能有总结 结构比较清晰 有各种tag分点之类的），也需要评估排在前面的answer是不是真的质量高于后面的，还是说因为human preference打分比较靠前。（这里你可能就要找一个方法 评估回答的质量–》 如果让LLM judge，那是不是就会出问题–》本身就是偏好数据RLHF训练出来的 会不会就给这里的排在前面的answer打高分呢 ）</p><p>对比现有的提升模型输出多样性的方法，找到最有效的那个，不知道算不算创新，能不能发文章</p><ul><li>本文提及的论文中，可以继续深入了解的内容：</li></ul><ol><li>了解 RLHF 相关数据集的标注格式： HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM <a href="https://arxiv.org/abs/2311.09528">link</a></li><li>了解模型训练的不同阶段： Tulu 3: Pushing Frontiers in Open Language Model Post-Training  <a href="https://arxiv.org/abs/2411.15124">link</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>iclr</category>
      
    </categories>
    
    
    <tags>
      
      <tag>iclr2026在投</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>COZE项目概述</title>
    <link href="/2025/12/11/COZE%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/"/>
    <url>/2025/12/11/COZE%E9%A1%B9%E7%9B%AE%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="我的评价"><a href="#我的评价" class="headerlink" title="我的评价"></a>我的评价</h2><p>&emsp;&emsp;</p><h2 id="当前存在什么问题"><a href="#当前存在什么问题" class="headerlink" title="当前存在什么问题"></a>当前存在什么问题</h2><p>&emsp;&emsp;</p><h2 id="本文打算通过什么思路解决"><a href="#本文打算通过什么思路解决" class="headerlink" title="本文打算通过什么思路解决"></a>本文打算通过什么思路解决</h2><p>&emsp;&emsp;</p><h2 id="该思路会遇到什么挑战"><a href="#该思路会遇到什么挑战" class="headerlink" title="该思路会遇到什么挑战"></a>该思路会遇到什么挑战</h2><p>&emsp;&emsp;</p><h2 id="本文通过什么手段克服该挑战"><a href="#本文通过什么手段克服该挑战" class="headerlink" title="本文通过什么手段克服该挑战"></a>本文通过什么手段克服该挑战</h2><p>&emsp;&emsp;</p><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><p>&emsp;&emsp;</p><h2 id="本文的写作思路"><a href="#本文的写作思路" class="headerlink" title="本文的写作思路"></a>本文的写作思路</h2><p>&emsp;&emsp;</p><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><p>&emsp;&emsp;</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?</title>
    <link href="/2025/12/10/StressPrompt-Does-Stress-Impact-Large-Language-Models-and-Human-Performance-Similarly/"/>
    <url>/2025/12/10/StressPrompt-Does-Stress-Impact-Large-Language-Models-and-Human-Performance-Similarly/</url>
    
    <content type="html"><![CDATA[<p><img src="./2025-12-10-23-32-16.png"></p><h2 id="我的评价"><a href="#我的评价" class="headerlink" title="我的评价"></a>我的评价</h2><p>写作逻辑还可以，实验部分写的很差</p><ol><li>实验数据不完整<br>&emsp;&emsp;The models tested included Llama-3-8B-Instruct, Llama-3.1-8B-Instruct, Llama-3-70B-Instruct(AI@Meta 2024), Phi-3-mini-4k-Instruct (Abdin et al.2024), Qwen2-72B-Instruct, Qwen2-7B-Instruct (Yang et al.2024), and Mistral-7B-Instruct-v0.3 (Jiang et al. 2023). The generation temperature was set to 0, and specific dialogue tokens were used to ensure consistency.</li></ol><p>&emsp;&emsp;The datasets employed in these evaluations included IFEval (Zhou et al.2023), BBH (Suzgun et al. 2022), MATH (Hendrycks et al. 2021b), GPQA (Rein et al. 2023), MuSR (Sprague et al. 2023), MMLU-P (Wang et al. 2024b), EQBench (Paech 2023), MMLU (Hendrycks et al. 2021a),TruthfulQA  </p><p>&emsp;&emsp;然而实际上你只能看到这些Llama-3-8B-Instruct, Phi-3-mini-4k-Instruct 以及一些意义不明 明显是凑数用的数据分析(PCA  T-SNE) 对于StressPrompt的作用原理也是点到为止。</p><ol start="2"><li><p>如何构造的prompt也是意义不明<br>只是说自己套用了很多什么心理学准则提示词生成的原理也没讲，AI生成吗？</p></li><li><p>喜欢蹭心理科学，标榜自己证实了耶克斯-多德森定律。纯纯唐文</p></li></ol><h2 id="当前存在什么问题"><a href="#当前存在什么问题" class="headerlink" title="当前存在什么问题"></a>当前存在什么问题</h2><p>&emsp;&emsp;现在没有人做模型的压力测试。就像人在不同压力等级下的表现不同，模型会不会也有这种现象。此外分析压力对模型内部的影响，对模型的训练过程也有一定的启发。</p><h2 id="本文打算通过什么思路解决"><a href="#本文打算通过什么思路解决" class="headerlink" title="本文打算通过什么思路解决"></a>本文打算通过什么思路解决</h2><p>&emsp;&emsp;设计100个StressPrompt， 对prompt进行人工压力评级(0 - 10)。把StressPrompt作为System Prompt输到模型里，再让模型进行MMLU MATH GPQA等主流测试。看看模型的表现有没有变化。  </p><p>&emsp;&emsp;分析StressPrompt对模型内部的影响，可以把hidden state的向量拿到，然后做一个PCA，找到方差最大的那个vector。对于某个prompt，可以拿到不同layer的不同token的hidden state然后和vector相乘得到一个标量，用于表示影响。</p><p><img src="./2025-12-10-23-40-02.png"></p><h2 id="该思路会遇到什么挑战"><a href="#该思路会遇到什么挑战" class="headerlink" title="该思路会遇到什么挑战"></a>该思路会遇到什么挑战</h2><p>&emsp;&emsp; 没什么挑战。构建prompt(我猜用的是ai)；PCA和分析不同layer的hidden state的方法感觉像是ai想出来的数据分析方法。</p><h2 id="本文通过什么手段克服该挑战"><a href="#本文通过什么手段克服该挑战" class="headerlink" title="本文通过什么手段克服该挑战"></a>本文通过什么手段克服该挑战</h2><p>&emsp;&emsp; 。。。</p><h2 id="本文如何通过实验论证了该方案的优越性"><a href="#本文如何通过实验论证了该方案的优越性" class="headerlink" title="本文如何通过实验论证了该方案的优越性"></a>本文如何通过实验论证了该方案的优越性</h2><p>&emsp;&emsp; 跑各种主流评测数据集。但是有个问题，本文根本就是挂羊头卖狗肉，没有展示出来所有的模型的表现。而且表格画的很烂。有提升的为什么不把这里加粗（把最高的标粗），还有1-10都没有提升的也不讨论。<br><img src="./2025-12-10-23-43-33.png"></p><h2 id="本文的写作思路"><a href="#本文的写作思路" class="headerlink" title="本文的写作思路"></a>本文的写作思路</h2><p>&emsp;&emsp;引言 (Introduction)： 提出LLMs性能受压力影响的问题尚未被探索，阐述研究的必要性（理解LLMs与人类智能的相似性、AI的鲁棒性）。  </p><p>&emsp;&emsp;相关工作 (Related Works)： 回顾LLMs的进步、情感智能评估以及压力对人类的影响等研究，指出现有研究缺乏对LLMs内部状态的定量分析，引出本文的研究切入点 。  </p><p>&emsp;&emsp;方法 (Method)： 详细介绍核心创新点：StressPrompt 的构建：基于四大心理学框架，并通过人类评分校准 。StressPrompt 的评估框架：如何在不同压力水平下系统地评估LLMs的性能 。压力扫描器 (Stress Scanner) 的分析：如何利用表征工程（RepE）来量化压力对LLMs内部状态的影响 。  </p><p>&emsp;&emsp;实验 (Experiments)： 描述实验设置、测试的模型和基准任务，并对在不同压力水平下的任务性能进行详细分析，重点论证耶克斯-多德森定律的发现和对不同任务（如复杂推理、情绪智能）的影响 。  </p><p>&emsp;&emsp;结论与贡献 (Conclusion&#x2F;Contributions)： 总结研究发现，强调StressPrompt的贡献（创新数据集、压力扫描器），并提出对未来开发更具弹性和适应性的AI系统的指导意义 。</p><h2 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h2><p>这种垃圾还是少看为佳</p>]]></content>
    
    
    <categories>
      
      <category>AAAI2025</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>概率算法ppt总结</title>
    <link href="/2025/12/10/%E6%A6%82%E7%8E%87%E7%AE%97%E6%B3%95ppt%E6%80%BB%E7%BB%93/"/>
    <url>/2025/12/10/%E6%A6%82%E7%8E%87%E7%AE%97%E6%B3%95ppt%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>计算 $a &#x3D; g^{x} mod \ p$<br>离散对数计算的问题 最坏情况下是$O(p)$<br>这里可以使用Sherwood算法引入随机性，降低进入最坏情况的概率。</p><p><img src="./2025-12-10-14-09-30.png"></p><p>这个改写利用了随机预处理来改变求解的实例，使得原本可能在特定输入下性能不佳的确定性离散对数算法 $log_{g,p}$，能够以接近平均性能的效率求解。</p><h2 id="🔬-dlog-RH-算法工作原理分析"><a href="#🔬-dlog-RH-算法工作原理分析" class="headerlink" title="🔬 $dlog_{RH}$ 算法工作原理分析"></a>🔬 $dlog_{RH}$ 算法工作原理分析</h2><h3 id="1-算法目标"><a href="#1-算法目标" class="headerlink" title="1. 算法目标"></a>1. 算法目标</h3><ul><li><strong>问题</strong>: 离散对数问题。</li><li><strong>输入</strong>: 基数 $g$，目标值 $a$，模数 $p$。</li><li><strong>求解</strong>: 找到指数 $x$，使得 $g^x \equiv a \pmod{p}$。</li><li><strong>模</strong>: 运算发生在 $\mathbb{Z}<em>p^*$ (模 $p$ 乘法群) 中。指数 $x$ 在 $\mathbb{Z}</em>{p-1}$ (模 $p-1$ 加法群) 中，即 $x \in {0, 1, \dots, p-2}$。</li></ul><h3 id="2-dlog-RH-步骤解析"><a href="#2-dlog-RH-步骤解析" class="headerlink" title="2. $dlog_{RH}$ 步骤解析"></a>2. $dlog_{RH}$ 步骤解析</h3><table><thead><tr><th align="left">步骤</th><th align="left">代码</th><th align="left">解释</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left"><strong>I. 随机数</strong></td><td align="left"><code>r ← uniform(0..p-2);</code></td><td align="left">随机选择一个指数 $r \in {0, 1, \dots, p-2}$。<strong>这是随机性的来源。</strong></td><td align="left">引入随机变量 $r$。</td></tr><tr><td align="left"><strong>II. 预处理 (实例 $y$)</strong></td><td align="left"><code>b ← ModularExponent(g, r, p);</code></td><td align="left">计算 $b &#x3D; g^r \pmod{p}$。</td><td align="left"></td></tr><tr><td align="left"></td><td align="left"><code>c ← b \cdot a \pmod{p};</code></td><td align="left">计算 $c &#x3D; b \cdot a \pmod{p} &#x3D; g^r \cdot g^x \pmod{p} &#x3D; g^{r+x} \pmod{p}$。</td><td align="left"><strong>原实例 $a$ 变换为随机实例 $c$。</strong></td></tr><tr><td align="left"><strong>III. 确定求解 ($f$)</strong></td><td align="left"><code>y ← logg,pc;</code></td><td align="left">使用<strong>确定性算法</strong> $log_{g,p}$ 求解实例 $c$。<strong>目标是找到 $y$，使得 $g^y \equiv c \pmod{p}$。</strong></td><td align="left">$y$ 是 $r+x$ 的解。</td></tr><tr><td align="left"><strong>IV. 后处理 ($v$)</strong></td><td align="left"><code>return (y-r) mod (p-1);</code></td><td align="left">$y$ 是 $r+x$ 的解，所以 $y \equiv r+x \pmod{p-1}$。因此，$x \equiv y-r \pmod{p-1}$。<strong>这是将解 $y$ 变回 $x$ 的步骤。</strong></td><td align="left">恢复原实例 $a$ 的解 $x$。</td></tr></tbody></table><h3 id="3-u-和-v-函数的确定"><a href="#3-u-和-v-函数的确定" class="headerlink" title="3. $u$ 和 $v$ 函数的确定"></a>3. $u$ 和 $v$ 函数的确定</h3><p>根据 Sherwood 算法的<strong>随机预处理一般方法</strong>：</p><ul><li><strong>原实例 $x$</strong>: 在离散对数问题中，原实例是<strong>目标值 $a$</strong>。</li><li><strong>随机变量 $r$</strong>: 在 ${0, 1, \dots, p-2}$ 中随机抽取。</li><li><strong>确定性算法 $f$</strong>: 是 $log_{g,p}$，即 $f(a) &#x3D; x$。</li></ul><h4 id="1-预处理函数-u"><a href="#1-预处理函数-u" class="headerlink" title="1. 预处理函数 $u$"></a>1. 预处理函数 $u$</h4><p>$u(a, r)$ 的作用是将原实例 $a$ 变换为随机实例 $c$。</p><p>$$\text{原实例 } a \xrightarrow{u(a, r)} \text{随机实例 } c$$</p><p>根据代码 $c \leftarrow (g^r \pmod{p}) \cdot a \pmod{p}$：</p><p>$$u(a, r) &#x3D; (g^r \cdot a) \pmod{p}$$</p><h4 id="2-后处理函数-v"><a href="#2-后处理函数-v" class="headerlink" title="2. 后处理函数 $v$"></a>2. 后处理函数 $v$</h4><p>$v(r, s)$ 的作用是将确定性算法的解 $s$ (即代码中的 $y$) 结合随机变量 $r$，变换回原实例的解 $x$。</p><p>$$\text{随机变量 } r \text{ 和 解 } s \xrightarrow{v(r, s)} \text{原实例的解 } x$$</p><p>根据代码 $x \leftarrow (y-r) \pmod{p-1}$：</p><p>$$v(r, s) &#x3D; (s - r) \pmod{p-1}$$</p><hr><h3 id="5-2-素数判定问题-p124"><a href="#5-2-素数判定问题-p124" class="headerlink" title="5.2 素数判定问题 p124"></a>5.2 素数判定问题 p124</h3><ul><li>方法1：随机因子</li></ul><p><img src="./2025-12-10-15-52-43.png"><br>在 n &#x3D; 2623 的情况下 $2 - \sqrt{2623}$随机选择有98%的概率正确</p><ul><li>方法2：费马小定理的逆否命题</li></ul><p><img src="./2025-12-10-15-55-14.png"></p><p><img src="./2025-12-10-15-57-53.png"></p><p>显然这种判定比较不负责（只要有 mod n &#x3D;&#x3D; 1就是素数 不等于1就不是）<br>满足$a^{n - 1} mod \ n &#x3D; 1$的数我们称为伪素数</p><p><img src="./2025-12-10-16-00-10.png"></p><p>也就是说，仅仅使用费马小定理，会有一些合数比较特殊，他们有很多的a做了运算后<br>均为1。这就迎来了我们的方法3</p><ul><li>方法3： Miller-Rabin测试</li></ul><p><img src="./2025-12-10-16-18-12.png"></p><p><img src="./2025-12-10-16-21-17.png"></p><p>判断一个底数 $a$ 是否为强伪证据：<br>分解 $n-1$：<br>将 $n-1$ 分解为 $2^s t$（$t$ 为奇数）。<br>计算初始值：<br>计算 $x \leftarrow a^t \pmod{n}$。<br>检查条件 ① 和 $i&#x3D;0$ 的条件 ②：<br>如果 $x&#x3D;1$ 或 $x&#x3D;n-1$（即 $a^t \equiv 1 \pmod{n}$ 或 $a^t \equiv -1 \pmod{n}$），则 $a$ 满足条件，返回 $true$（$a$ 是强伪证据）。<br>循环检查其余的条件 ②：<br>执行 $s-1$ 次循环，在每次循环中计算 $x \leftarrow x^2 \pmod{n}$（相当于检查 $a^{2^i t} \pmod{n}$）。如果在循环中发现 $x&#x3D;n-1$，则满足条件②，返回 $true$（$a$ 是强伪证据）。</p><p>返回 $false$：如果所有检查都未通过，则 $a$ 不是强伪证据。此时 $a$ 是 $n$ 是合数的强证据，所以 $n$ 必定是合数，返回 $false$（测试成功）。</p><p>这种方法大大缩减了伪证据数目</p>]]></content>
    
    
    <categories>
      
      <category>course</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>GenAL: Generative Agent for Adaptive Learning</title>
    <link href="/2025/12/10/GenAL-Generative-Agent-for-Adaptive-Learning/"/>
    <url>/2025/12/10/GenAL-Generative-Agent-for-Adaptive-Learning/</url>
    
    <content type="html"><![CDATA[<hr><p>title: ‘GenAL: Generative Agent for Adaptive Learning’<br>date: 2025-12-09 15:21:28<br>categories:</p><ul><li>aaai2025<br>tags:</li></ul><hr><p><img src="./image.png"></p><p><img src="./2025-12-10-10-26-20.png"></p><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/32038">论文链接</a></p><p>写在前面：</p><p>简单来说，它的核心思想是：<strong>把“推荐题目”这件事变成两个智能体（Agent）之间的配合</strong>。一个负责“宏观思考”（了解学生整体情况），另一个负责“微观教学”（具体挑哪道题）。而且，它不像以前的推荐系统只看题目 ID，它是真的去<strong>读题目的文本内容</strong>，理解题意。</p><h4 id="第一步：宏观把脉-——-全局思考智能体-GTA"><a href="#第一步：宏观把脉-——-全局思考智能体-GTA" class="headerlink" title="第一步：宏观把脉 —— 全局思考智能体 (GTA)"></a>第一步：宏观把脉 —— 全局思考智能体 (GTA)</h4><p>在推荐新题目之前，系统首先要搞清楚“这个学生现在是什么水平？”。</p><ol><li><strong>看病历 (Log Memory)</strong>：系统会读取学生过去的答题记录 $\mathcal{H}_t$，不仅看对错，还要看之前做过的题目的<strong>文本内容</strong>。</li><li><strong>做化验 (Educational Tools)</strong>：使用传统的<strong>知识追踪模型 (如 DKT)</strong> 来计算学生对各个知识点的掌握概率（比如“函数”掌握度 60%，“几何”掌握度 80%）[cite: 110, 112]。</li><li><strong>写诊断书 (Reflector)</strong>：这是关键。LLM（大模型）会结合上面的答题记录和掌握概率，生成一份<strong>学生画像 ($L_t$)</strong>。<ul><li>画像里包含：学生的能力评估、学习偏好（比如喜欢抽象题还是应用题）。<br>*同时，它会反思上一轮推荐得好不好，生成一个新的<strong>推荐策略 ($R_t$)</strong>（比如：“学生基础不牢，下一题应该降低难度，复习前置知识”）。</li></ul></li></ol><h4 id="第二步：缩小范围-——-教学工具-Teaching-Tools"><a href="#第二步：缩小范围-——-教学工具-Teaching-Tools" class="headerlink" title="第二步：缩小范围 —— 教学工具 (Teaching Tools)"></a>第二步：缩小范围 —— 教学工具 (Teaching Tools)</h4><p>现在有了策略，但题库里有成千上万道题，不能瞎选。这就需要 <strong>Local Teaching Agent (LTA)</strong> 里的工具来帮忙。</p><ol><li><strong>查地图 (Knowledge Graph)</strong>：利用<strong>层级知识图谱 ($G$)</strong>。</li><li><strong>圈范围</strong>：根据当前要学的知识点，找出它自己以及它的<strong>直接前驱节点</strong>（即学这个知识点之前必须会的那些基础）。</li><li><strong>定候选集</strong>：系统只把属于这些知识点的题目拿出来，形成一个<strong>候选练习集 ($S_t$)</strong>。这就大大缩小了搜索范围，避免大模型产生幻觉或跑题。</li></ol><h4 id="第三步：精准决策-——-推荐器-Recommender"><a href="#第三步：精准决策-——-推荐器-Recommender" class="headerlink" title="第三步：精准决策 —— 推荐器 (Recommender)"></a>第三步：精准决策 —— 推荐器 (Recommender)</h4><p>这是最终做决定的环节。</p><ol><li><strong>输入信息</strong>：推荐器（也是一个 LLM）接收两个东西：<ul><li>来自第一步的<strong>推荐策略 ($R_t$)</strong>（“要降低难度”）。</li><li>来自第二步的<strong>候选练习集 ($S_t$)</strong> [cite: 169]。</li></ul></li><li><strong>预测下一题 ($e_{t+1}$)</strong>：LLM 结合策略，从候选集里挑出最合适的一道题 [cite: 172]。</li><li><strong>给出理由与预测 ($F_{t+1}, A_{t+1}$)</strong>：<ul><li>这步很有意思，LLM 不光甩出一道题，还得<strong>解释为什么选这道题</strong>（Reasoning），并且<strong>预测学生能不能做对</strong>。这为下一步的“自我反思”留下了依据 [cite: 175]。</li></ul></li></ol><h4 id="第四步：闭环反馈"><a href="#第四步：闭环反馈" class="headerlink" title="第四步：闭环反馈"></a>第四步：闭环反馈</h4><p>学生做完题后，真实结果会和 LLM 的预测结果（$A_{t+1}$）进行对比。如果 LLM 预测学生会做对，结果学生做错了，GTA（第一步那个智能体）就会在下一轮更新策略：“我高估了他，即使是基础题他也做错了，需要更基础的练习”。</p><hr><h3 id="📝-举个栗子：小明的“一次函数”学习之旅"><a href="#📝-举个栗子：小明的“一次函数”学习之旅" class="headerlink" title="📝 举个栗子：小明的“一次函数”学习之旅"></a>📝 举个栗子：小明的“一次函数”学习之旅</h3><p>假设现在 <strong>小明</strong> 正在学习初中数学的 <strong>“一次函数 ($y&#x3D;kx+b$)”</strong>。</p><p><strong>1. 初始状态 (GTA 工作):</strong></p><ul><li><strong>记录：</strong> 小明刚做完一道关于“斜率 $k$”的题，做错了。</li><li><strong>工具分析：</strong> 知识追踪模型显示，他对“斜率”的掌握度只有 30%，对“坐标系”掌握度 80%。</li><li><strong>GTA (大模型) 思考：</strong> “小明虽然懂坐标系，但对斜率的概念很模糊。现在的策略 $R_t$ 应该是：<strong>暂停新课，回退一步，先巩固斜率的几何定义。</strong>”</li></ul><p><strong>2. 筛选题目 (Teaching Tools 工作):</strong></p><ul><li><strong>目标：</strong> 巩固“斜率”。</li><li><strong>知识图谱：</strong> 查到“斜率”的前置知识是“两点间距离”和“坐标点”。</li><li><strong>候选集 $S_t$：</strong> 系统从题库里捞出了 50 道相关的题。</li></ul><p><strong>3. 最终推荐 (Recommender 工作):</strong></p><ul><li><strong>输入：</strong> 策略是“巩固定义，不要太难”，候选集有 50 道题。</li><li><strong>大模型选择：</strong> 既然不能太难，它排除了那些“给定图像求解析式”的复杂题，选中了一道文字题 <strong>$e_{t+1}$</strong>：<blockquote><p><em>题目：已知点 A(1, 2) 和点 B(3, 4)，请计算直线 AB 的斜率。</em></p></blockquote></li><li><strong>大模型解释 ($F_{t+1}$):</strong> “推荐这道题是因为它是斜率的最基本计算，直接应用公式，没有复杂的变形，适合当前基础薄弱的小明。”</li><li><strong>预测 ($A_{t+1}$):</strong> “预测他能做对，因为这是基础计算。”</li></ul><p><strong>4. 结果反馈:</strong></p><ul><li><strong>实际情况：</strong> 小明做对了。</li><li><strong>下一轮循环：</strong> GTA 看到“预测正确，小明做对了”，于是更新策略：“基础已巩固，下一轮可以尝试稍微难一点的题目，比如结合截距 $b$ 的考察。”</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>GenAL 相比传统推荐系统，最大的亮点在于它<strong>像个真人老师</strong>：</p><ol><li>它<strong>读得懂题</strong>（利用 LLM 的语义理解能力）[cite: 14, 26]。</li><li>它<strong>有教学逻辑</strong>（先看前置知识，再定难度）。</li><li>它<strong>会反思</strong>（每次推荐都要写理由，预测不对就自我修正）。</li></ol><hr><h1 id="📚-GenAL-基于生成式智能体的自适应学习框架"><a href="#📚-GenAL-基于生成式智能体的自适应学习框架" class="headerlink" title="📚 GenAL: 基于生成式智能体的自适应学习框架"></a>📚 GenAL: 基于生成式智能体的自适应学习框架</h1><h2 id="一、-当前教育推荐系统存在的问题-🚧"><a href="#一、-当前教育推荐系统存在的问题-🚧" class="headerlink" title="一、 当前教育推荐系统存在的问题 🚧"></a>一、 当前教育推荐系统存在的问题 🚧</h2><p>论文指出，尽管自适应学习（Adaptive Learning）研究广泛，但现有模型主要依赖于对学习者和练习题的<strong>简单 ID 索引</strong>，导致以下三个核心问题：</p><table><thead><tr><th align="left">问题点</th><th align="left">详细描述</th></tr></thead><tbody><tr><td align="left"><strong>信息利用率不足</strong></td><td align="left">传统模型（如基于序列或强化学习的模型）<strong>无法有效利用</strong>练习题的<strong>文本内容</strong>等丰富的语义信息，导致对题目理解肤浅，推荐缺乏深度。</td></tr><tr><td align="left"><strong>泛化性差，难以适应扩展</strong></td><td align="left">模型需要针对不同的数据集或不断<strong>扩张的题库</strong>进行重新训练。这使其难以适应在线教育场景中资源快速更新和增长的需求。</td></tr><tr><td align="left"><strong>稀疏环境下的不稳定</strong></td><td align="left">基于训练的强化学习（RL）推荐范式，在学生学习记录<strong>稀疏</strong>或数据量较小时，其推荐性能往往<strong>不稳定</strong>且难以保证效果。</td></tr></tbody></table><hr><h2 id="二、-本文的解决思路与核心方案-💡"><a href="#二、-本文的解决思路与核心方案-💡" class="headerlink" title="二、 本文的解决思路与核心方案 💡"></a>二、 本文的解决思路与核心方案 💡</h2><p>本文提出的 <strong>GenAL</strong> 框架旨在通过引入 <strong>大型语言模型 (LLM)</strong>，模仿人类教师的认知与教学过程，从而实现语义驱动、可泛化的自适应学习推荐。</p><table><thead><tr><th align="left">解决思路</th><th align="left">具体机制</th><th align="left">目标</th></tr></thead><tbody><tr><td align="left"><strong>分层决策架构</strong></td><td align="left">采用<strong>双智能体（Agent）结构，将复杂的教学任务分解：<br>1. 全局思考智能体 (GTA)：制定宏观</strong>教学策略 $R_t$。<br>2. <strong>局部教学智能体 (LTA)</strong>：执行<strong>微观</strong>推荐决策 $e_{t+1}$。</td><td align="left">实现教学策略与题目选择的有效解耦和协同。</td></tr><tr><td align="left"><strong>语义驱动推荐</strong></td><td align="left">LTA 中的<strong>推荐器（Recommender）是一个 LLM，它直接读取题目内容</strong>的文本信息，并根据 GTA 的策略进行语义级别的推理和选择。</td><td align="left">克服传统模型只看 ID 的局限性，实现基于内容的理解推荐。</td></tr><tr><td align="left"><strong>引入外部知识</strong></td><td align="left">LTA 中包含<strong>教学工具（Teaching Tools）</strong>，用于接入教育领域的<strong>先验知识</strong>。</td><td align="left">解决 LLM 在特定教育领域的知识短板，避免“幻觉”。</td></tr></tbody></table><hr><h2 id="三、-该思路可能遇到的挑战-⚔️"><a href="#三、-该思路可能遇到的挑战-⚔️" class="headerlink" title="三、 该思路可能遇到的挑战 ⚔️"></a>三、 该思路可能遇到的挑战 ⚔️</h2><table><thead><tr><th align="left">挑战点</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><strong>LLM 的领域幻觉</strong></td><td align="left">LLM 在通用领域强大，但在专业的教育知识点上可能缺乏精确性，容易产生不准确的**“幻觉”决策<strong>或</strong>不专业理由**。</td></tr><tr><td align="left"><strong>巨大的搜索空间</strong></td><td align="left">如果让 LLM 直接从整个题库中挑选 $e_{t+1}$，其<strong>计算和时间成本不可接受</strong>，效率极低。</td></tr><tr><td align="left"><strong>缺乏教学闭环与自我修正</strong></td><td align="left">缺乏将 LLM 的推荐决策与学生的<strong>真实反馈</strong>进行对比、反思和迭代修正的机制，难以保证教学策略的<strong>长期稳定性</strong>和<strong>有效性</strong>。</td></tr></tbody></table><hr><h2 id="四、-本文克服挑战的手段-🛠️"><a href="#四、-本文克服挑战的手段-🛠️" class="headerlink" title="四、 本文克服挑战的手段 🛠️"></a>四、 本文克服挑战的手段 🛠️</h2><p>GenAL 框架通过设计精巧的工具和反思机制，系统性地解决了上述挑战：</p><table><thead><tr><th align="left">挑战</th><th align="left">克服手段</th><th align="left">实现细节</th></tr></thead><tbody><tr><td align="left"><strong>领域幻觉与搜索空间</strong></td><td align="left"><strong>知识图谱指导与搜索空间限制</strong></td><td align="left"><strong>教学工具（Teaching Tools）</strong>：利用<strong>分层知识图谱</strong> $G$ 来<strong>限制</strong>推荐范围。候选集 $S_t$ 只包括当前目标知识点及其<strong>直接前驱节点</strong>相关的练习题。这既缩小了搜索空间，又用结构化知识<strong>校准了 LLM</strong>。</td></tr><tr><td align="left"><strong>缺乏教学闭环</strong></td><td align="left"><strong>反思与预测机制</strong></td><td align="left"><strong>推荐器（Recommender）</strong>：强制要求 LLM 在推荐 $e_{t+1}$ 的同时，生成推荐<strong>理由</strong> ($F_{t+1}$) 和对学生作答的<strong>预测</strong> ($A_{t+1}$)。<br><strong>全局思考智能体（GTA）</strong>：负责将 $A_{t+1}$ 与学生<strong>实际作答</strong>进行对比，通过**反思器（Reflector）<strong>生成新的反思总结 $L_t$，并更新下一轮的教学策略 $R_{t+1}$，形成</strong>“观察-反思-规划-行动”**的闭环。</td></tr></tbody></table><hr><h2 id="五、-实验论证优越性-🏆"><a href="#五、-实验论证优越性-🏆" class="headerlink" title="五、 实验论证优越性 🏆"></a>五、 实验论证优越性 🏆</h2><p>本文通过在实际教育数据集上的对比实验和消融实验，全面论证了 GenAL 框架的优越性：</p><ol><li><p><strong>推荐有效性 (Recommendation Effectiveness)</strong>：</p><ul><li><strong>结果：</strong> GenAL 在评估自适应推荐效率的指标（如通过更少的问题达成目标知识掌握度）上，<strong>显著优于</strong>传统的序列推荐模型和基于 RL 的推荐模型。</li><li><strong>论证：</strong> 证明了其策略制定和题目选择更高效，能加速学生的学习路径。</li></ul></li><li><p><strong>知识追踪准确性 (Knowledge Tracing Accuracy)</strong>：</p><ul><li><strong>指标：</strong> 使用 $\text{AUC}$ 和 $\text{ACC}$ 等指标评估模型对学生<strong>下一个问题作答对错的预测能力</strong>。</li><li><strong>结果：</strong> GenAL 达到了<strong>最高的预测准确率</strong>，表明其能更精确地理解学生的实时学习状态。</li></ul></li><li><p><strong>消融实验 (Ablation Study)</strong>：</p><ul><li>通过移除关键组件（如知识图谱、反思机制），实验结果显示性能大幅下降。</li><li><strong>论证：</strong> 证明了<strong>知识图谱</strong>是克服 LLM 幻觉的关键，而<strong>反思机制</strong>则是保障 GenAL 长期、稳定、高效推荐的根本保障。</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>AAAI2025</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
